{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian sentences lemmatization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Bidirectional, TimeDistributed, RepeatVector, Activation, Dot, Lambda, GRU\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import gensim\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# set all random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "dataset_path = \"./dev.csv\"\n",
    "df_dev = pd.read_csv(dataset_path, sep=\"\\t\", header=None,names=[\"word\", \"tag\", \"lemm\"])\n",
    "\n",
    "dataset_path = \"./test.csv\"\n",
    "df_test = pd.read_csv(dataset_path, sep=\"\\t\",header=None, names=[\"word\", \"tag\", \"lemm\"])\n",
    "\n",
    "df_dev[\"word\"] = df_dev[\"word\"].astype(str)\n",
    "df_dev[\"tag\"] = df_dev[\"tag\"].astype(str)\n",
    "df_dev[\"lemm\"] = df_dev[\"lemm\"].astype(str)\n",
    "\n",
    "df_test[\"word\"] = df_test[\"word\"].astype(str)\n",
    "df_test[\"tag\"] = df_test[\"tag\"].astype(str)\n",
    "df_test[\"lemm\"] = df_test[\"lemm\"].astype(str)\n",
    "\n",
    "# remove head\n",
    "df_dev = df_dev.iloc[1:]\n",
    "df_test = df_test.iloc[1:]\n",
    "\n",
    "# removing rows where tag is nan\n",
    "df_dev = df_dev.dropna(subset=[\"tag\"])\n",
    "df_dev = df_dev[df_dev[\"tag\"] != \"nan\"]\n",
    "df_test = df_test.dropna(subset=[\"tag\"])\n",
    "df_test = df_test[df_test[\"tag\"] != \"nan\"]\n",
    "\n",
    "# lower case all words\n",
    "df_test[\"word\"] = df_test[\"word\"].str.lower()\n",
    "df_dev[\"word\"] = df_dev[\"word\"].str.lower()\n",
    "\n",
    "\n",
    "def get_sentences(df):\n",
    "    words = []\n",
    "    tags = []\n",
    "    lemmas = []\n",
    "    sentence = []\n",
    "    max_s = 0\n",
    "    for index, row in df.iterrows():\n",
    "        word = row[\"word\"]\n",
    "        tag = row[\"tag\"]\n",
    "        lemm = row[\"lemm\"]\n",
    "        sentence.append([word, tag, lemm])\n",
    "\n",
    "        if row[\"word\"] in [\".\", \"?\", \"!\", \";\"]:\n",
    "            words.append([word for word, tag, lemm in sentence])\n",
    "            tags.append([tag for word, tag, lemm in sentence])\n",
    "            lemmas.append([lemm for word, tag, lemm in sentence])\n",
    "            max_s = max(max_s, len(sentence))\n",
    "            sentence = []\n",
    "\n",
    "    print(\"Max sentence length: \", max_s)\n",
    "    return words, tags, lemmas\n",
    "\n",
    "s_dev_words, s_dev_tags, s_dev_lemmas = get_sentences(df_dev)\n",
    "s_test_words, s_test_tags, s_test_lemmas = get_sentences(df_test)\n",
    "\n",
    "print(\"Number of sentences in dev set: \", len(s_dev_words))\n",
    "print(\"Number of sentences in test set: \", len(s_test_words))\n",
    "\n",
    "for i in range(len(s_dev_words)):\n",
    "    if len(s_dev_words[i]) != len(s_dev_tags[i]) or len(s_dev_words[i]) != len(s_dev_lemmas[i]):\n",
    "        print(\"Dimension mismatch in sentence: \", i)\n",
    "        print(\"Words: \", s_dev_words[i])\n",
    "        print(\"Tags: \", s_dev_tags[i])\n",
    "        print(\"Lemmas: \", s_dev_lemmas[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  ['mi', 'riferisco', 'al', 'lavoro', 'dove', 'non', \"c'\", '&egrave;', ',', 'innanzitutto', 'nel', 'mezzogiorno', ',', 'e', 'al', 'lavoro', 'che', 'cambia', '.']\n",
      "Tag:  ['pron_per', 'v_gvrb', 'prep_a', 'nn', 'conj_s', 'adv', 'adv', 'v_essere', 'p_oth', 'adv', 'prep_a', 'nn_p', 'p_oth', 'conj_c', 'prep_a', 'nn', 'pron_rel', 'v_gvrb', 'p_eos']\n",
      "Lemma:  ['mi', 'riferire', 'al', 'lavoro', 'dove', 'non', 'ci', 'essere', ',', 'innanzitutto', 'nel', 'mezzogiorno', ',', 'e', 'al', 'lavoro', 'che', 'cambiare', '.']\n",
      "Encoded word:  [153, 8977, 31, 162, 99, 13, 75, 11, 1, 4242, 35, 1277, 1, 4, 31, 162, 5, 3359, 2]\n",
      "Encoded tag:  [14, 5, 7, 1, 16, 8, 8, 13, 4, 8, 7, 9, 4, 11, 7, 1, 15, 5, 10]\n",
      "Encoded lemma:  [167, 2613, 32, 147, 104, 14, 41, 4, 1, 3755, 39, 1380, 1, 5, 32, 147, 6, 642, 2]\n"
     ]
    }
   ],
   "source": [
    "# encode words\n",
    "word_tokenizer = Tokenizer(filters=\"\")\n",
    "word_tokenizer.fit_on_texts(s_dev_words + s_test_words)\n",
    "s_dev_words_e = word_tokenizer.texts_to_sequences(s_dev_words)\n",
    "s_test_words_e = word_tokenizer.texts_to_sequences(s_test_words)\n",
    "\n",
    "# encode tags\n",
    "tag_tokenizer = Tokenizer(filters=\"\")\n",
    "tag_tokenizer.fit_on_texts(s_dev_tags + s_test_tags)\n",
    "s_dev_tags_e = tag_tokenizer.texts_to_sequences(s_dev_tags)\n",
    "s_test_tags_e = tag_tokenizer.texts_to_sequences(s_test_tags)\n",
    "\n",
    "# encode lemmas\n",
    "lemma_tokenizer = Tokenizer(filters=\"\")\n",
    "lemma_tokenizer.fit_on_texts(s_dev_lemmas + s_test_lemmas)\n",
    "s_dev_lemmas_e = lemma_tokenizer.texts_to_sequences(s_dev_lemmas)\n",
    "s_test_lemmas_e = lemma_tokenizer.texts_to_sequences(s_test_lemmas)\n",
    "\n",
    "# look at first encoded data point\n",
    "print(\"Word: \", s_dev_words[0])\n",
    "print(\"Tag: \", s_dev_tags[0])\n",
    "print(\"Lemma: \", s_dev_lemmas[0])\n",
    "print(\"Encoded word: \", s_dev_words_e[0])\n",
    "print(\"Encoded tag: \", s_dev_tags_e[0])\n",
    "print(\"Encoded lemma: \", s_dev_lemmas_e[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max len:  107\n",
      "Encoded words:  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0  153 8977   31  162   99   13   75   11    1 4242\n",
      "   35 1277    1    4   31  162    5 3359    2]\n",
      "Encoded tags:  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14  5  7  1 16  8  8 13\n",
      "  4  8  7  9  4 11  7  1 15  5 10]\n",
      "Encoded lemmas:  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0  167 2613   32  147  104   14   41    4    1 3755\n",
      "   39 1380    1    5   32  147    6  642    2]\n"
     ]
    }
   ],
   "source": [
    "# find max len\n",
    "max_len = 0\n",
    "for i in range(len(s_dev_words_e)):\n",
    "    max_len = max(max_len, len(s_dev_words_e[i]))\n",
    "\n",
    "for i in range(len(s_test_words_e)):\n",
    "    max_len = max(max_len, len(s_test_words_e[i]))\n",
    "    \n",
    "print(\"Max len: \", max_len)\n",
    "padding_type = \"pre\"\n",
    "\n",
    "s_dev_words_e = tf.keras.preprocessing.sequence.pad_sequences(s_dev_words_e, maxlen=max_len, padding=padding_type)\n",
    "s_test_words_e = tf.keras.preprocessing.sequence.pad_sequences(s_test_words_e, maxlen=max_len, padding=padding_type)\n",
    "s_dev_tags_e = tf.keras.preprocessing.sequence.pad_sequences(s_dev_tags_e, maxlen=max_len, padding=padding_type)\n",
    "s_test_tags_e = tf.keras.preprocessing.sequence.pad_sequences(s_test_tags_e, maxlen=max_len, padding=padding_type)\n",
    "s_dev_lemmas_e = tf.keras.preprocessing.sequence.pad_sequences(s_dev_lemmas_e, maxlen=max_len, padding=padding_type)\n",
    "s_test_lemmas_e = tf.keras.preprocessing.sequence.pad_sequences(s_test_lemmas_e, maxlen=max_len, padding=padding_type)\n",
    "\n",
    "# print first encoded data point\n",
    "print(\"Encoded words: \", s_dev_words_e[0])\n",
    "print(\"Encoded tags: \", s_dev_tags_e[0])\n",
    "print(\"Encoded lemmas: \", s_dev_lemmas_e[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 300\n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "# train word2vec model\n",
    "word2vec = gensim.models.Word2Vec(s_dev_words + s_test_words, vector_size=EMBEDDING_SIZE, window=7, min_count=1, workers=4)\n",
    "\n",
    "# create an empty embedding matix\n",
    "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "\n",
    "# create a word to index dictionary mapping\n",
    "word2id = word_tokenizer.word_index\n",
    "\n",
    "# copy vectors from word2vec model to the words present in corpus\n",
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec.wv[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_enc = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# get all tags\n",
    "tags = list(tag_tokenizer.word_index.values())\n",
    "tags = [0] + tags\n",
    "tags = np.array(tags).reshape(-1, 1)\n",
    "\n",
    "dev_tags_1he = np.empty((len(s_dev_tags_e), max_len, len(tags)))\n",
    "test_tags_1he = np.empty((len(s_test_tags_e), max_len, len(tags)))\n",
    "\n",
    "# fit and transform all tags\n",
    "tag_enc.fit(tags)\n",
    "for i in range(len(s_dev_tags_e)):\n",
    "    for j in range(len(s_dev_tags_e[i])):\n",
    "        dev_tags_1he[i][j] = tag_enc.transform([[s_dev_tags_e[i][j]]])[0]\n",
    "\n",
    "for i in range(len(s_test_tags_e)):\n",
    "    for j in range(len(s_test_tags_e[i])):\n",
    "        test_tags_1he[i][j] = tag_enc.transform([[s_test_tags_e[i][j]]])[0]\n",
    "\n",
    "mask_tag = np.zeros((len(tags)))\n",
    "mask_tag[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_enc = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# get all lemmas\n",
    "lemmas = list(lemma_tokenizer.word_index.values())\n",
    "lemmas = [0] + lemmas\n",
    "lemmas = np.array(lemmas).reshape(-1, 1)\n",
    "\n",
    "dev_lemmas_1he = np.empty((len(s_dev_lemmas_e), max_len, len(lemmas)))\n",
    "test_lemmas_1he = np.empty((len(s_test_lemmas_e), max_len, len(lemmas)))\n",
    "\n",
    "# fit and transform all lemmas\n",
    "lemmas_enc.fit(lemmas)\n",
    "for i in range(len(s_dev_lemmas_e)):\n",
    "    for j in range(len(s_dev_lemmas_e[i])):\n",
    "        dev_lemmas_1he[i][j] = lemmas_enc.transform([[s_dev_lemmas_e[i][j]]])[0]\n",
    "\n",
    "for i in range(len(s_test_lemmas_e)):\n",
    "    for j in range(len(s_test_lemmas_e[i])):\n",
    "        test_lemmas_1he[i][j] = lemmas_enc.transform([[s_test_lemmas_e[i][j]]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded words:  (107, 60)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# get all unique letter in words\n",
    "characters = set()\n",
    "\n",
    "for lemma in df_test[\"lemm\"]:\n",
    "    for letter in lemma:\n",
    "        characters.add(letter)\n",
    "\n",
    "for word in df_dev[\"word\"]:\n",
    "    for letter in word:\n",
    "        characters.add(letter)\n",
    "\n",
    "# add padding and unknown to characters\n",
    "characters.add(\" \")\n",
    "\n",
    "# the length of the vocab for one-hot encoded char\n",
    "vocab_size = len(characters)\n",
    "\n",
    "#print(\"\\n### Number of unique characters:\", vocab_size)\n",
    "\n",
    "max_word_length = max(df_dev[\"word\"].str.len().max(), df_test[\"lemm\"].str.len().max())\n",
    "#print(\"\\n### Max word length:\", max_word_length)\n",
    "max_word_length += 1\n",
    "max_word_length = int(max_word_length)\n",
    "\n",
    "# Each word is encoded as a list of one-hot encoded characters\n",
    "char_enc = OneHotEncoder(sparse_output=False)\n",
    "char_enc.fit([[char] for char in characters])\n",
    "\n",
    "def pad_word(word, max_word_length):\n",
    "    return word + \" \" * (max_word_length - len(word))\n",
    "\n",
    "def encode_word(word):\n",
    "    return char_enc.transform([[char] for char in word])\n",
    "\n",
    "def encode_words(s, lemma = True):\n",
    "    words = []\n",
    "    for w in s:\n",
    "        if w == 0:\n",
    "            w = \"\"\n",
    "        else:\n",
    "            if lemma:\n",
    "                w = lemma_tokenizer.index_word[w]\n",
    "            else:\n",
    "                w = word_tokenizer.index_word[w]\n",
    "\n",
    "        w = pad_word(w, max_word_length)\n",
    "        words.append(encode_word(w)[0])\n",
    "    return words\n",
    "\n",
    "mask_value = char_enc.transform([[\" \"]])[0]\n",
    "\n",
    "dev_lemmas_c = np.array([encode_words(s) for s in s_dev_lemmas_e])\n",
    "test_lemmas_c = np.array([encode_words(s) for s in s_test_lemmas_e])\n",
    "\n",
    "dev_words_c = np.array([encode_words(s, lemma=False) for s in s_dev_words_e])\n",
    "test_words_c = np.array([encode_words(s, lemma=False) for s in s_test_words_e])\n",
    "\n",
    "mask_value_c = char_enc.transform([[\" \"]])[0]\n",
    "\n",
    "print(\"Encoded words: \", dev_words_c[0].shape)\n",
    "print(dev_words_c[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 107, 26, 60)]     0         \n",
      "                                                                 \n",
      " word_c_reshape (Reshape)    (None, 107, 1560)         0         \n",
      "                                                                 \n",
      " lstm (Bidirectional)        (None, 107, 600)          3351600   \n",
      "                                                                 \n",
      " lstm2 (Bidirectional)       (None, 107, 600)          1623600   \n",
      "                                                                 \n",
      " dense (TimeDistributed)     (None, 107, 300)          180300    \n",
      "                                                                 \n",
      " output1 (TimeDistributed)   (None, 107, 1560)         469560    \n",
      "                                                                 \n",
      " output_reshape (Reshape)    (None, 107, 26, 60)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,625,060\n",
      "Trainable params: 5,625,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# bidirectional LSTM\n",
    "def get_model():\n",
    "    # input layers\n",
    "    # word_input = Input(shape=(max_len,), name=\"word_input\")\n",
    "    word_input_c = Input(shape=(max_len, max_word_length, vocab_size), name=\"word_input_c\")\n",
    "    # tag_input = Input(shape=(max_len, len(tags)), name=\"tag_input\")\n",
    "\n",
    "    # word_input = tf.keras.layers.Masking(mask_value=0)(word_input)\n",
    "    # tag_input = tf.keras.layers.Masking(mask_value=mask_tag)(tag_input)\n",
    "    word_input_c = tf.keras.layers.Masking(mask_value=mask_value_c)(word_input_c)\n",
    "\n",
    "    # # embedding layers\n",
    "    # word_embedding = Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, weights=[embedding_weights], trainable=True, name=\"word_embedding\")(word_input)\n",
    "    # tag_embedding = TimeDistributed(Dense(EMBEDDING_SIZE, activation=\"swish\"), name=\"tag_embedding\")(tag_input)  # (None, 107, 300)\n",
    "\n",
    "\n",
    "    # word_embedding = RepeatVector(max_word_length, name=\"word_embedding_repeat\")(word_embedding)\n",
    "    # word_embedding = tf.keras.layers.Reshape((max_len, max_word_length, vocab_size), name=\"word_embedding_reshape\")(word_embedding)\n",
    "\n",
    "    word_c_reshape = tf.keras.layers.Reshape((max_len, max_word_length * vocab_size), name=\"word_c_reshape\")(word_input_c)\n",
    "    word_embedding_c = TimeDistributed(Dense(EMBEDDING_SIZE, activation=\"swish\"), name=\"word_embedding_c\")(word_c_reshape)\n",
    "    \n",
    "   \n",
    "\n",
    "    # bidirectional LSTM\n",
    "    lstm = Bidirectional(GRU(EMBEDDING_SIZE, return_sequences=True), name=\"lstm\")(word_c_reshape)\n",
    "    lstm = Bidirectional(GRU(EMBEDDING_SIZE, return_sequences=True), name=\"lstm2\")(lstm)\n",
    "\n",
    "    dense = TimeDistributed(Dense(EMBEDDING_SIZE, activation=\"swish\"), name=\"dense\")(lstm)\n",
    "    \n",
    "    # output layer\n",
    "    output = TimeDistributed(Dense(1560, activation=\"softmax\"), name=\"output1\")(dense)\n",
    "    output = tf.keras.layers.Reshape((107,26,60), name=\"output_reshape\")(output)\n",
    "    \n",
    "\n",
    "    model = Model(inputs=[ word_input_c], outputs=output)\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom accuracy metric\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.argmax(y_true, axis=-1)\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    correct_predictions = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(703, 107)\n",
      "(703, 107, 60)\n",
      "(703, 107, 32)\n",
      "(703, 107, 60)\n"
     ]
    }
   ],
   "source": [
    "print(s_dev_words_e.shape)\n",
    "print(dev_words_c.shape)\n",
    "print(dev_tags_1he.shape)\n",
    "print(dev_lemmas_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 107, 26, 60), found shape=(None, 107, 60)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39madam\u001b[39m\u001b[39m\"\u001b[39m, metrics\u001b[39m=\u001b[39m[accuracy])\n\u001b[0;32m----> 3\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit([ dev_words_c], dev_lemmas_c, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/b3/wwdp9x6d2_3fvz33_v4d_k_80000gn/T/__autograph_generated_filervrczkiz.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 107, 26, 60), found shape=(None, 107, 60)\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[accuracy])\n",
    "history = model.fit([ dev_words_c], dev_lemmas_c, epochs=10, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 21s 117ms/step - loss: 0.1350 - accuracy: 0.7854\n",
      "Test loss: 0.1350139081478119\n",
      "Test accuracy: 0.7853794693946838\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "result = model.evaluate([s_test_words_e, test_words_c, test_tags_1he], test_lemmas_c)\n",
    "\n",
    "print(\"Test loss:\", result[0])\n",
    "print(\"Test accuracy:\", result[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization Accuracy\n",
    "\n",
    "Lemmatisation accuracy is defined as the number of correct lemma assignment divided by the total number of tokens in the test set belonging to the considered lexical classes (ADJ_, ADV,NN, V_). \n",
    "\n",
    "(Evalita2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 26s 140ms/step\n",
      "Test accuracy: 3.740806966878895e-05\n"
     ]
    }
   ],
   "source": [
    "# predict lemmas for test data\n",
    "pred_lemmas = model.predict([s_test_words_e, test_words_c, test_tags_1he])\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for i in range(len(pred_lemmas)):\n",
    "    for j in range(len(pred_lemmas[i])):\n",
    "        if np.argmax(test_lemmas_c[i][j]) != 0:\n",
    "            total += 1\n",
    "            if np.argmax(pred_lemmas[i][j]) == np.argmax(test_lemmas_c[i][j]):\n",
    "                correct += 1\n",
    "\n",
    "print(\"Test accuracy:\", correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: \"\n",
      "Predicted lemma: \n",
      "Actual lemma: ['\"']\n",
      "\n",
      "Word: craxi\n",
      "Predicted lemma: ['die']\n",
      "Actual lemma: ['craxi']\n",
      "\n",
      "Word: :\n",
      "Predicted lemma: \n",
      "Actual lemma: [':']\n",
      "\n",
      "Word: non\n",
      "Predicted lemma: ['d']\n",
      "Actual lemma: ['non']\n",
      "\n",
      "Word: ci\n",
      "Predicted lemma: ['d']\n",
      "Actual lemma: ['ci']\n",
      "\n",
      "Word: sar&agrave;\n",
      "Predicted lemma: ['cosere']\n",
      "Actual lemma: ['essere']\n",
      "\n",
      "Word: amnistia\n",
      "Predicted lemma: ['coser']\n",
      "Actual lemma: ['amnistia']\n",
      "\n",
      "Word: finch&eacute;\n",
      "Predicted lemma: ['coseree']\n",
      "Actual lemma: ['finch&eacute;']\n",
      "\n",
      "Word: dura\n",
      "Predicted lemma: ['die']\n",
      "Actual lemma: ['durare']\n",
      "\n",
      "Word: il\n",
      "Predicted lemma: ['d']\n",
      "Actual lemma: ['il']\n",
      "\n",
      "Word: cavaliere\n",
      "Predicted lemma: ['cosere']\n",
      "Actual lemma: ['cavaliere']\n",
      "\n",
      "Word: \"\n",
      "Predicted lemma: ['.']\n",
      "Actual lemma: ['\"']\n",
      "\n",
      "Word: .\n",
      "Predicted lemma: ['.']\n",
      "Actual lemma: ['.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# demo \n",
    "def decode_word(word):\n",
    "    d_word = \"\"\n",
    "    for char in word:\n",
    "        if np.argmax(char) != 0:\n",
    "            d_word += (char_enc.inverse_transform([char])[0])\n",
    "    return d_word\n",
    "\n",
    "for i in range(len(pred_lemmas[0])):\n",
    "    if s_test_words_e[0][i] != 0:\n",
    "        print(\"Word:\", word_tokenizer.index_word[s_test_words_e[0][i]])\n",
    "        print(\"Predicted lemma:\",decode_word(pred_lemmas[0][i]))\n",
    "        print(\"Actual lemma:\",decode_word(test_lemmas_c[0][i]))\n",
    "        print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
