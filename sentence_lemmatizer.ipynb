{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian sentences lemmatization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  95\n",
      "Max sentence length:  107\n",
      "Number of sentences in dev set:  703\n",
      "Number of sentences in test set:  5596\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Bidirectional, TimeDistributed, RepeatVector, Activation, Dot, Lambda\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import gensim\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# set all random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "dataset_path = \"./dev.csv\"\n",
    "df_dev = pd.read_csv(dataset_path, sep=\"\\t\", header=None,names=[\"word\", \"tag\", \"lemm\"])\n",
    "\n",
    "dataset_path = \"./test.csv\"\n",
    "df_test = pd.read_csv(dataset_path, sep=\"\\t\",header=None, names=[\"word\", \"tag\", \"lemm\"])\n",
    "\n",
    "df_dev[\"word\"] = df_dev[\"word\"].astype(str)\n",
    "df_dev[\"tag\"] = df_dev[\"tag\"].astype(str)\n",
    "df_dev[\"lemm\"] = df_dev[\"lemm\"].astype(str)\n",
    "\n",
    "df_test[\"word\"] = df_test[\"word\"].astype(str)\n",
    "df_test[\"tag\"] = df_test[\"tag\"].astype(str)\n",
    "df_test[\"lemm\"] = df_test[\"lemm\"].astype(str)\n",
    "\n",
    "# remove head\n",
    "df_dev = df_dev.iloc[1:]\n",
    "df_test = df_test.iloc[1:]\n",
    "\n",
    "# removing rows where tag is nan\n",
    "df_dev = df_dev.dropna(subset=[\"tag\"])\n",
    "df_dev = df_dev[df_dev[\"tag\"] != \"nan\"]\n",
    "df_test = df_test.dropna(subset=[\"tag\"])\n",
    "df_test = df_test[df_test[\"tag\"] != \"nan\"]\n",
    "\n",
    "# lower case all words\n",
    "df_test[\"word\"] = df_test[\"word\"].str.lower()\n",
    "df_dev[\"word\"] = df_dev[\"word\"].str.lower()\n",
    "\n",
    "\n",
    "def get_sentences(df):\n",
    "    words = []\n",
    "    tags = []\n",
    "    lemmas = []\n",
    "    sentence = []\n",
    "    max_s = 0\n",
    "    for index, row in df.iterrows():\n",
    "        word = row[\"word\"]\n",
    "        tag = row[\"tag\"]\n",
    "        lemm = row[\"lemm\"]\n",
    "        sentence.append([word, tag, lemm])\n",
    "\n",
    "        if row[\"word\"] in [\".\", \"?\", \"!\", \";\"]:\n",
    "            words.append([word for word, tag, lemm in sentence])\n",
    "            tags.append([tag for word, tag, lemm in sentence])\n",
    "            lemmas.append([lemm for word, tag, lemm in sentence])\n",
    "            max_s = max(max_s, len(sentence))\n",
    "            sentence = []\n",
    "\n",
    "    print(\"Max sentence length: \", max_s)\n",
    "    return words, tags, lemmas\n",
    "\n",
    "s_dev_words, s_dev_tags, s_dev_lemmas = get_sentences(df_dev)\n",
    "s_test_words, s_test_tags, s_test_lemmas = get_sentences(df_test)\n",
    "\n",
    "print(\"Number of sentences in dev set: \", len(s_dev_words))\n",
    "print(\"Number of sentences in test set: \", len(s_test_words))\n",
    "\n",
    "for i in range(len(s_dev_words)):\n",
    "    if len(s_dev_words[i]) != len(s_dev_tags[i]) or len(s_dev_words[i]) != len(s_dev_lemmas[i]):\n",
    "        print(\"Dimension mismatch in sentence: \", i)\n",
    "        print(\"Words: \", s_dev_words[i])\n",
    "        print(\"Tags: \", s_dev_tags[i])\n",
    "        print(\"Lemmas: \", s_dev_lemmas[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  ['mi', 'riferisco', 'al', 'lavoro', 'dove', 'non', \"c'\", '&egrave;', ',', 'innanzitutto', 'nel', 'mezzogiorno', ',', 'e', 'al', 'lavoro', 'che', 'cambia', '.']\n",
      "Tag:  ['pron_per', 'v_gvrb', 'prep_a', 'nn', 'conj_s', 'adv', 'adv', 'v_essere', 'p_oth', 'adv', 'prep_a', 'nn_p', 'p_oth', 'conj_c', 'prep_a', 'nn', 'pron_rel', 'v_gvrb', 'p_eos']\n",
      "Lemma:  ['mi', 'riferire', 'al', 'lavoro', 'dove', 'non', 'ci', 'essere', ',', 'innanzitutto', 'nel', 'mezzogiorno', ',', 'e', 'al', 'lavoro', 'che', 'cambiare', '.']\n",
      "Encoded word:  [153, 8977, 31, 162, 99, 13, 75, 11, 1, 4242, 35, 1277, 1, 4, 31, 162, 5, 3359, 2]\n",
      "Encoded tag:  [14, 5, 7, 1, 16, 8, 8, 13, 4, 8, 7, 9, 4, 11, 7, 1, 15, 5, 10]\n",
      "Encoded lemma:  [167, 2613, 32, 147, 104, 14, 41, 4, 1, 3755, 39, 1380, 1, 5, 32, 147, 6, 642, 2]\n"
     ]
    }
   ],
   "source": [
    "# encode words\n",
    "word_tokenizer = Tokenizer(filters=\"\")\n",
    "word_tokenizer.fit_on_texts(s_dev_words + s_test_words)\n",
    "s_dev_words_e = word_tokenizer.texts_to_sequences(s_dev_words)\n",
    "s_test_words_e = word_tokenizer.texts_to_sequences(s_test_words)\n",
    "\n",
    "# encode tags\n",
    "tag_tokenizer = Tokenizer(filters=\"\")\n",
    "tag_tokenizer.fit_on_texts(s_dev_tags + s_test_tags)\n",
    "s_dev_tags_e = tag_tokenizer.texts_to_sequences(s_dev_tags)\n",
    "s_test_tags_e = tag_tokenizer.texts_to_sequences(s_test_tags)\n",
    "\n",
    "# encode lemmas\n",
    "lemma_tokenizer = Tokenizer(filters=\"\")\n",
    "lemma_tokenizer.fit_on_texts(s_dev_lemmas + s_test_lemmas)\n",
    "s_dev_lemmas_e = lemma_tokenizer.texts_to_sequences(s_dev_lemmas)\n",
    "s_test_lemmas_e = lemma_tokenizer.texts_to_sequences(s_test_lemmas)\n",
    "\n",
    "# look at first encoded data point\n",
    "print(\"Word: \", s_dev_words[0])\n",
    "print(\"Tag: \", s_dev_tags[0])\n",
    "print(\"Lemma: \", s_dev_lemmas[0])\n",
    "print(\"Encoded word: \", s_dev_words_e[0])\n",
    "print(\"Encoded tag: \", s_dev_tags_e[0])\n",
    "print(\"Encoded lemma: \", s_dev_lemmas_e[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max len:  107\n",
      "Encoded words:  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0  153 8977   31  162   99   13   75   11    1 4242\n",
      "   35 1277    1    4   31  162    5 3359    2]\n",
      "Encoded tags:  [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14  5  7  1 16  8  8 13\n",
      "  4  8  7  9  4 11  7  1 15  5 10]\n",
      "Encoded lemmas:  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0  167 2613   32  147  104   14   41    4    1 3755\n",
      "   39 1380    1    5   32  147    6  642    2]\n"
     ]
    }
   ],
   "source": [
    "# find max len\n",
    "max_len = 0\n",
    "for i in range(len(s_dev_words_e)):\n",
    "    max_len = max(max_len, len(s_dev_words_e[i]))\n",
    "\n",
    "for i in range(len(s_test_words_e)):\n",
    "    max_len = max(max_len, len(s_test_words_e[i]))\n",
    "    \n",
    "print(\"Max len: \", max_len)\n",
    "padding_type = \"pre\"\n",
    "\n",
    "s_dev_words_e = tf.keras.preprocessing.sequence.pad_sequences(s_dev_words_e, maxlen=max_len, padding=padding_type)\n",
    "s_test_words_e = tf.keras.preprocessing.sequence.pad_sequences(s_test_words_e, maxlen=max_len, padding=padding_type)\n",
    "s_dev_tags_e = tf.keras.preprocessing.sequence.pad_sequences(s_dev_tags_e, maxlen=max_len, padding=padding_type)\n",
    "s_test_tags_e = tf.keras.preprocessing.sequence.pad_sequences(s_test_tags_e, maxlen=max_len, padding=padding_type)\n",
    "s_dev_lemmas_e = tf.keras.preprocessing.sequence.pad_sequences(s_dev_lemmas_e, maxlen=max_len, padding=padding_type)\n",
    "s_test_lemmas_e = tf.keras.preprocessing.sequence.pad_sequences(s_test_lemmas_e, maxlen=max_len, padding=padding_type)\n",
    "\n",
    "# print first encoded data point\n",
    "print(\"Encoded words: \", s_dev_words_e[0])\n",
    "print(\"Encoded tags: \", s_dev_tags_e[0])\n",
    "print(\"Encoded lemmas: \", s_dev_lemmas_e[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 300\n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "# train word2vec model\n",
    "word2vec = gensim.models.Word2Vec(s_dev_words + s_test_words, vector_size=EMBEDDING_SIZE, window=7, min_count=1, workers=4)\n",
    "\n",
    "# create an empty embedding matix\n",
    "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "\n",
    "# create a word to index dictionary mapping\n",
    "word2id = word_tokenizer.word_index\n",
    "\n",
    "# copy vectors from word2vec model to the words present in corpus\n",
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec.wv[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_enc = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# get all tags\n",
    "tags = list(tag_tokenizer.word_index.values())\n",
    "tags = [0] + tags\n",
    "tags = np.array(tags).reshape(-1, 1)\n",
    "\n",
    "dev_tags_1he = np.empty((len(s_dev_tags_e), max_len, len(tags)))\n",
    "test_tags_1he = np.empty((len(s_test_tags_e), max_len, len(tags)))\n",
    "\n",
    "# fit and transform all tags\n",
    "tag_enc.fit(tags)\n",
    "for i in range(len(s_dev_tags_e)):\n",
    "    for j in range(len(s_dev_tags_e[i])):\n",
    "        dev_tags_1he[i][j] = tag_enc.transform([[s_dev_tags_e[i][j]]])[0]\n",
    "\n",
    "\n",
    "for i in range(len(s_test_tags_e)):\n",
    "    for j in range(len(s_test_tags_e[i])):\n",
    "        test_tags_1he[i][j] = tag_enc.transform([[s_test_tags_e[i][j]]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_enc = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# get all lemmas\n",
    "lemmas = list(lemma_tokenizer.word_index.values())\n",
    "lemmas = [0] + lemmas\n",
    "lemmas = np.array(lemmas).reshape(-1, 1)\n",
    "\n",
    "dev_lemmas_1he = np.empty((len(s_dev_lemmas_e), max_len, len(lemmas)))\n",
    "test_lemmas_1he = np.empty((len(s_test_lemmas_e), max_len, len(lemmas)))\n",
    "\n",
    "# fit and transform all lemmas\n",
    "lemmas_enc.fit(lemmas)\n",
    "for i in range(len(s_dev_lemmas_e)):\n",
    "    for j in range(len(s_dev_lemmas_e[i])):\n",
    "        dev_lemmas_1he[i][j] = lemmas_enc.transform([[s_dev_lemmas_e[i][j]]])[0]\n",
    "\n",
    "for i in range(len(s_test_lemmas_e)):\n",
    "    for j in range(len(s_test_lemmas_e[i])):\n",
    "        test_lemmas_1he[i][j] = lemmas_enc.transform([[s_test_lemmas_e[i][j]]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoded words: (703, 107, 26, 60)\n"
     ]
    }
   ],
   "source": [
    "# get all unique letter in words\n",
    "characters = set()\n",
    "\n",
    "for lemma in df_test[\"lemm\"]:\n",
    "    for letter in lemma:\n",
    "        characters.add(letter)\n",
    "\n",
    "for word in df_dev[\"word\"]:\n",
    "    for letter in word:\n",
    "        characters.add(letter)\n",
    "\n",
    "# add padding and unknown to characters\n",
    "characters.add(\" \")\n",
    "\n",
    "# the length of the vocab for one-hot encoded char\n",
    "vocab_size = len(characters)\n",
    "\n",
    "#print(\"\\n### Number of unique characters:\", vocab_size)\n",
    "\n",
    "max_word_length = max(df_dev[\"word\"].str.len().max(), df_test[\"lemm\"].str.len().max())\n",
    "#print(\"\\n### Max word length:\", max_word_length)\n",
    "max_word_length += 1\n",
    "max_word_length = int(max_word_length)\n",
    "\n",
    "\n",
    "# Each word is encoded as a list of one-hot encoded characters\n",
    "char_enc = OneHotEncoder(sparse_output=False)\n",
    "char_enc.fit([[char] for char in characters])\n",
    "\n",
    "def pad_word(word, max_word_length):\n",
    "    return word + \" \" * (max_word_length - len(word))\n",
    "\n",
    "def encode_word(word):\n",
    "    return char_enc.transform([[char] for char in word])\n",
    "\n",
    "def encode_words(s):\n",
    "    words = []\n",
    "    for w in s:\n",
    "        # transform w from int to string\n",
    "        if w == 0:\n",
    "            w = \"\"\n",
    "        else:\n",
    "            w = lemma_tokenizer.index_word[w]\n",
    "\n",
    "        w = pad_word(w, max_word_length)\n",
    "        words.append(encode_word(w))\n",
    "    return words\n",
    "\n",
    "mask_value = char_enc.transform([[\" \"]])[0]\n",
    "\n",
    "d_lemmas_e = np.array([encode_words(s) for s in s_dev_lemmas_e])\n",
    "t_leamms_e = np.array([encode_words(s) for s in s_test_lemmas_e])\n",
    "print(\"Shape of encoded words:\", d_lemmas_e.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " word_input (InputLayer)        [(None, 107)]        0           []                               \n",
      "                                                                                                  \n",
      " tag_input (InputLayer)         [(None, 107, 32)]    0           []                               \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)     (None, 107, 300)     5919000     ['word_input[0][0]']             \n",
      "                                                                                                  \n",
      " tag_embedding (TimeDistributed  (None, 107, 300)    9900        ['tag_input[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concat (Concatenate)           (None, 107, 600)     0           ['word_embedding[0][0]',         \n",
      "                                                                  'tag_embedding[0][0]']          \n",
      "                                                                                                  \n",
      " lstm (Bidirectional)           (None, 107, 600)     2162400     ['concat[0][0]']                 \n",
      "                                                                                                  \n",
      " lstm2 (Bidirectional)          (None, 107, 600)     2162400     ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense (TimeDistributed)        (None, 107, 300)     180300      ['lstm2[0][0]']                  \n",
      "                                                                                                  \n",
      " dense2 (TimeDistributed)       (None, 107, 300)     90300       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " output (TimeDistributed)       (None, 107, 12858)   3870258     ['dense2[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,394,558\n",
      "Trainable params: 14,394,558\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# bidirectional LSTM\n",
    "def get_model():\n",
    "    # input layers\n",
    "    word_input = Input(shape=(max_len,), name=\"word_input\")\n",
    "    tag_input = Input(shape=(max_len, len(tags)), name=\"tag_input\")\n",
    "\n",
    "    # embedding layers\n",
    "    word_embedding = Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, weights=[embedding_weights], trainable=True, name=\"word_embedding\")(word_input)\n",
    "    tag_embedding = TimeDistributed(Dense(EMBEDDING_SIZE, activation=\"swish\"), name=\"tag_embedding\")(tag_input)\n",
    "\n",
    "    # concatenate embeddings\n",
    "    concat = Concatenate(axis=-1, name=\"concat\")([word_embedding, tag_embedding])\n",
    "\n",
    "    # bidirectional LSTM\n",
    "    lstm = Bidirectional(LSTM(EMBEDDING_SIZE, return_sequences=True), name=\"lstm\")(concat)\n",
    "    lstm = Bidirectional(LSTM(EMBEDDING_SIZE, return_sequences=True), name=\"lstm2\")(lstm)\n",
    "\n",
    "\n",
    "    dense = TimeDistributed(Dense(EMBEDDING_SIZE, activation=\"swish\"), name=\"dense\")(lstm)\n",
    "    dense = TimeDistributed(Dense(EMBEDDING_SIZE, activation=\"swish\"), name=\"dense2\")(dense)\n",
    "    \n",
    "    # output layer\n",
    "    output = TimeDistributed(Dense(len(lemmas), activation=\"softmax\"), name=\"output\")(dense)\n",
    "\n",
    "    model = Model(inputs=[word_input, tag_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom accuracy metric\n",
    "# a word is correct if all letters are correct\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = K.argmax(y_true, axis=-1)\n",
    "    y_pred = K.argmax(y_pred, axis=-1)\n",
    "    correct = K.cast(K.equal(y_true, y_pred), K.floatx())\n",
    "    return K.mean(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 14s 394ms/step - loss: 4.2716 - accuracy: 0.7356\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 7s 302ms/step - loss: 2.0292 - accuracy: 0.7700\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 7s 295ms/step - loss: 1.5561 - accuracy: 0.7716\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 7s 297ms/step - loss: 1.4193 - accuracy: 0.7788\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 6s 292ms/step - loss: 1.3423 - accuracy: 0.7910\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 6s 294ms/step - loss: 1.2852 - accuracy: 0.7927\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 6s 293ms/step - loss: 1.2194 - accuracy: 0.7968\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 6s 291ms/step - loss: 1.1662 - accuracy: 0.8019\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 6s 288ms/step - loss: 1.1187 - accuracy: 0.8052\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 7s 296ms/step - loss: 1.0737 - accuracy: 0.8115\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit([s_dev_words_e, dev_tags_1he], dev_lemmas_1he, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# test model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mevaluate([s_test_words_e, test_tags_1he], test_lemmas_1he)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "# test model\n",
    "model.evaluate([s_test_words_e, test_tags_1he], test_lemmas_1he)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization Accuracy\n",
    "\n",
    "Lemmatisation accuracy is defined as the number of correct lemma assignment divided by the total number of tokens in the test set belonging to the considered lexical classes (ADJ_, ADV,NN, V_). \n",
    "\n",
    "(Evalita2011)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
