{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian Word Lemmatizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Bidirectional, TimeDistributed, RepeatVector, Activation, Dot, Lambda, Dropout, Add, Multiply, Masking\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import gensim\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# set all random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  95\n",
      "Max sentence length:  107\n",
      "Number of sentences in dev set:  703\n",
      "Number of sentences in test set:  5596\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_path = \"./dev.csv\"\n",
    "df_dev = pd.read_csv(dataset_path, sep=\"\\t\", header=None,\n",
    "                     names=[\"word\", \"tag\", \"lemm\"])\n",
    "\n",
    "dataset_path = \"./test.csv\"\n",
    "df_test = pd.read_csv(dataset_path, sep=\"\\t\", header=None,\n",
    "                      names=[\"word\", \"tag\", \"lemm\"])\n",
    "\n",
    "df_dev[\"word\"] = df_dev[\"word\"].astype(str)\n",
    "df_dev[\"tag\"] = df_dev[\"tag\"].astype(str)\n",
    "df_dev[\"lemm\"] = df_dev[\"lemm\"].astype(str)\n",
    "\n",
    "df_test[\"word\"] = df_test[\"word\"].astype(str)\n",
    "df_test[\"tag\"] = df_test[\"tag\"].astype(str)\n",
    "df_test[\"lemm\"] = df_test[\"lemm\"].astype(str)\n",
    "\n",
    "# remove head\n",
    "df_dev = df_dev.iloc[1:]\n",
    "df_test = df_test.iloc[1:]\n",
    "\n",
    "# removing rows where tag is nan\n",
    "df_dev = df_dev.dropna(subset=[\"tag\"])\n",
    "df_dev = df_dev[df_dev[\"tag\"] != \"nan\"]\n",
    "df_test = df_test.dropna(subset=[\"tag\"])\n",
    "df_test = df_test[df_test[\"tag\"] != \"nan\"]\n",
    "\n",
    "# lower case all words\n",
    "df_test[\"word\"] = df_test[\"word\"].str.lower()\n",
    "df_dev[\"word\"] = df_dev[\"word\"].str.lower()\n",
    "\n",
    "\n",
    "def get_sentences(df):\n",
    "    words = []\n",
    "    tags = []\n",
    "    lemmas = []\n",
    "    sentence = []\n",
    "    max_s = 0\n",
    "    for index, row in df.iterrows():\n",
    "        word = row[\"word\"]\n",
    "        tag = row[\"tag\"]\n",
    "        lemm = row[\"lemm\"]\n",
    "        sentence.append([word, tag, lemm])\n",
    "\n",
    "        if row[\"word\"] in [\".\", \"?\", \"!\", \";\"]:\n",
    "            words.append([word for word, tag, lemm in sentence])\n",
    "            tags.append([tag for word, tag, lemm in sentence])\n",
    "            lemmas.append([lemm for word, tag, lemm in sentence])\n",
    "            max_s = max(max_s, len(sentence))\n",
    "            sentence = []\n",
    "\n",
    "    print(\"Max sentence length: \", max_s)\n",
    "    return words, tags, lemmas\n",
    "\n",
    "# _s is for string\n",
    "dev_words_s, dev_tags_s, dev_lemmas_s = get_sentences(df_dev)\n",
    "test_words_s, test_tags_s, test_lemmas_s = get_sentences(df_test)\n",
    "print(\"Number of sentences in dev set: \", len(dev_words_s))\n",
    "print(\"Number of sentences in test set: \", len(test_words_s))\n",
    "\n",
    "for i in range(len(dev_words_s)):\n",
    "    if len(dev_words_s[i]) != len(dev_tags_s[i]) or len(dev_words_s[i]) != len(dev_lemmas_s[i]):\n",
    "        print(\"Dimension mismatch in sentence: \", i)\n",
    "        print(\"Words: \", dev_words_s[i])\n",
    "        print(\"Tags: \", dev_tags_s[i])\n",
    "        print(\"Lemmas: \", dev_lemmas_s[i])\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTX_DIM = 13\n",
    "PRE_VALUE = \"<PRE>\"\n",
    "POST_VALUE = \"<POST>\"\n",
    "\n",
    "def get_context(words, tags, lemmas):\n",
    "    ctx = []\n",
    "    w = []\n",
    "    tag = []\n",
    "    lemma = []\n",
    "\n",
    "    for s_index in range(len(words)):\n",
    "        s = words[s_index]\n",
    "        sentence = \" \".join(s)\n",
    "        s = [PRE_VALUE] * CTX_DIM + s + [POST_VALUE] * CTX_DIM\n",
    "\n",
    "        for w_index in range(len(s)):\n",
    "            if w_index < CTX_DIM or w_index >= len(s) - CTX_DIM:\n",
    "                continue\n",
    "\n",
    "            context = s[w_index - CTX_DIM:w_index] + s[w_index + 1:w_index  + CTX_DIM + 1]\n",
    "            context = \" \".join(context)\n",
    "            ctx.append(context)\n",
    "            w.append(words[s_index][w_index-CTX_DIM])\n",
    "            tag.append(tags[s_index][w_index-CTX_DIM])\n",
    "            lemma.append(lemmas[s_index][w_index-CTX_DIM])\n",
    "\n",
    "    return ctx, w, tag, lemma\n",
    "\n",
    "dev_ctx, dev_words, dev_tags, dev_lemmas = get_context(dev_words_s, dev_tags_s, dev_lemmas_s)\n",
    "test_ctx, test_words, test_tags, test_lemmas = get_context(test_words_s, test_tags_s, test_lemmas_s)\n",
    "\n",
    "dev_ctx, val_ctx, dev_words, val_words, dev_tags, val_tags, dev_lemmas, val_lemmas = train_test_split(dev_ctx, dev_words, dev_tags, dev_lemmas, test_size=0.01, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTX Dim: 13 \n",
      "\n",
      "Context:  con anni o mesi di anticipo , per gli asteroidi vaganti il rischio collisione inaspettata &egrave; molto pi&ugrave; grande e il fenomeno deve essere preso sul\n",
      "Word:  di\n",
      "Tag:  prep\n",
      "Lemma:  di\n",
      "\n",
      "Context:  <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> perfino negli usa che pure sono all' avanguardia e vantano una diffusione capillare del telefono (\n",
      "Word:  ,\n",
      "Tag:  p_oth\n",
      "Lemma:  ,\n",
      "\n",
      "Context:  <PRE> <PRE> con il risultato che i loro figli si trovano avvantaggiati , hanno pi&ugrave; tempo per crescere prima_che giunga la cattiva stagione e pi&ugrave; tempo\n",
      "Word:  in_quanto\n",
      "Tag:  conj_s\n",
      "Lemma:  in_quanto\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CTX Dim:\", CTX_DIM, \"\\n\")\n",
    "for i in range(3):\n",
    "    index = np.random.randint(0, len(dev_ctx))\n",
    "    print(\"Context: \", dev_ctx[index])\n",
    "    print(\"Word: \", dev_words[index])\n",
    "    print(\"Tag: \", dev_tags[index])\n",
    "    print(\"Lemma: \", dev_lemmas[index])\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Class Words\n",
    "The evaluation is done only on open-class words and not to functional words: only the tokens having a PoS-tag comprised in the set ADJ *, ADV, NN, V * had to be lemmatised, in all the other cases the token could be copied unchanged into the lemma column as they were not considered for the evaluation (the asterisk indicates all PoS-tag possibilities beginning with that prefix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_open_class_words(ctx, words, tags, lemmas):\n",
    "    open_class_words = []\n",
    "    open_class_ctx = []\n",
    "    open_class_tags = []\n",
    "    open_class_lemmas = []\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if \"adj_\" in tags[i] or \"adv\" in tags[i] or \"nn\" in tags[i] or \"v_\" in tags[i]:\n",
    "            open_class_words.append(words[i])\n",
    "            open_class_ctx.append(ctx[i])\n",
    "            open_class_tags.append(tags[i])\n",
    "            open_class_lemmas.append(lemmas[i])\n",
    "\n",
    "    return open_class_ctx, open_class_words, open_class_tags, open_class_lemmas\n",
    "\n",
    "\n",
    "test_ctx, test_words, test_tags, test_lemmas = get_open_class_words(test_ctx, test_words, test_tags, test_lemmas)\n",
    "dev_ctx, dev_words, dev_tags, dev_lemmas = get_open_class_words(dev_ctx, dev_words, dev_tags, dev_lemmas)\n",
    "val_ctx, val_words, val_tags, val_lemmas = get_open_class_words(val_ctx, val_words, val_tags, val_lemmas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  60\n",
      "Max word length:  26\n",
      "Context:  <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> ha una grossa con denti bianchi e forti , tranne_che per un canino che si  ->  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 26, 22, 1964, 23, 1757, 3474, 5, 1116, 3, 14624, 13, 11, 6239, 7, 17]\n",
      "Words:  bocca  ->  [35 48 36 36 34  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n",
      "Tag:  nn  ->  [1]\n",
      "Lemma:  bocca  ->  [35 48 36 36 34  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n"
     ]
    }
   ],
   "source": [
    "# word encoder\n",
    "word_tokenizer = Tokenizer(filters=\"\")\n",
    "word_tokenizer.fit_on_texts(dev_ctx + test_ctx + val_ctx)\n",
    "\n",
    "# tag encoder\n",
    "tag_tokenizer = Tokenizer(filters=\"\")\n",
    "tag_tokenizer.fit_on_texts(dev_tags_s + test_tags_s)\n",
    "\n",
    "# lemma encoder\n",
    "lemma_tokenizer = Tokenizer(filters=\"\")\n",
    "lemma_tokenizer.fit_on_texts(dev_lemmas_s + test_lemmas_s)\n",
    "\n",
    "dev_ctx_e = word_tokenizer.texts_to_sequences(dev_ctx)\n",
    "val_ctx_e = word_tokenizer.texts_to_sequences(val_ctx)\n",
    "test_ctx_e = word_tokenizer.texts_to_sequences(test_ctx)\n",
    "\n",
    "PRE_E = word_tokenizer.texts_to_sequences([PRE_VALUE])[0][0]\n",
    "POST_E = word_tokenizer.texts_to_sequences([POST_VALUE])[0][0]\n",
    "\n",
    "\n",
    "dev_tags_e = tag_tokenizer.texts_to_sequences(dev_tags)\n",
    "val_tags_e = tag_tokenizer.texts_to_sequences(val_tags)\n",
    "test_tags_e = tag_tokenizer.texts_to_sequences(test_tags)\n",
    "\n",
    "\n",
    "# get all unique letter in words\n",
    "characters = set()\n",
    "\n",
    "for lemma in df_dev[\"lemm\"].unique():\n",
    "    for letter in lemma:\n",
    "        characters.add(letter)\n",
    "\n",
    "for lemma in df_test[\"lemm\"].unique():\n",
    "    for letter in lemma:\n",
    "        characters.add(letter)\n",
    "\n",
    "# add padding and unknown to characters\n",
    "characters.add(\" \")\n",
    "\n",
    "# the length of the vocab for one-hot encoded char\n",
    "VOCAB_SIZE = len(characters)\n",
    "\n",
    "print (\"Vocab size: \", VOCAB_SIZE)\n",
    "# order characters\n",
    "characters = sorted(list(characters))\n",
    "\n",
    "\n",
    "char2idx = {char: idx for idx, char in enumerate(characters)}\n",
    "idx2char = {idx: char for idx, char in enumerate(characters)}\n",
    "\n",
    "MAX_WORD_LENGTH = 0\n",
    "for w in dev_words + test_words + dev_lemmas + test_lemmas:\n",
    "    MAX_WORD_LENGTH = max(MAX_WORD_LENGTH, len(w))\n",
    "MAX_WORD_LENGTH += 1\n",
    "print(\"Max word length: \", MAX_WORD_LENGTH)\n",
    "\n",
    "def encode_words(words):\n",
    "    encoded_words = []\n",
    "    for word in words:\n",
    "        word_e = []\n",
    "        for letter in word:\n",
    "            word_e.append(characters.index(letter))\n",
    "        encoded_words.append(word_e)\n",
    "    return encoded_words\n",
    "\n",
    "dev_words_e = encode_words(dev_words)\n",
    "test_words_e = encode_words(test_words)\n",
    "val_words_e = encode_words(val_words)\n",
    "\n",
    "dev_lemmas_e = encode_words(dev_lemmas)\n",
    "test_lemmas_e = encode_words(test_lemmas)\n",
    "val_lemmas_e = encode_words(val_lemmas)\n",
    "\n",
    "dev_words_e = tf.keras.preprocessing.sequence.pad_sequences(dev_words_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "test_words_e = tf.keras.preprocessing.sequence.pad_sequences(test_words_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "\n",
    "dev_lemmas_e = tf.keras.preprocessing.sequence.pad_sequences(dev_lemmas_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "test_lemmas_e = tf.keras.preprocessing.sequence.pad_sequences(test_lemmas_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "\n",
    "val_words_e = tf.keras.preprocessing.sequence.pad_sequences(val_words_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "val_lemmas_e = tf.keras.preprocessing.sequence.pad_sequences(val_lemmas_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "\n",
    "# show random data point\n",
    "index = np.random.randint(0, len(dev_ctx))\n",
    "print(\"Context: \", dev_ctx[index], \" -> \", dev_ctx_e[index])\n",
    "print(\"Words: \", dev_words[index], \" -> \", dev_words_e[index])\n",
    "print(\"Tag: \", dev_tags[index], \" -> \", dev_tags_e[index])\n",
    "print(\"Lemma: \", dev_lemmas[index], \" -> \", dev_lemmas_e[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the characters for the lemmas\n",
    "dev_lemmas_e = tf.one_hot(dev_lemmas_e, VOCAB_SIZE)\n",
    "test_lemmas_e = tf.one_hot(test_lemmas_e, VOCAB_SIZE)\n",
    "val_lemmas_e = tf.one_hot(val_lemmas_e, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context shape:  (8086, 26)\n",
      "Words shape:  (8086, 26)\n",
      "Tags shape:  (8086, 1)\n",
      "Lemmas shape:  (8086, 26, 60)\n"
     ]
    }
   ],
   "source": [
    "# trnasform to numpy array\n",
    "dev_ctx_e = np.array(dev_ctx_e)\n",
    "dev_words_e = np.array(dev_words_e)\n",
    "dev_tags_e = np.array(dev_tags_e)\n",
    "dev_lemmas_e = np.array(dev_lemmas_e)\n",
    "\n",
    "test_ctx_e = np.array(test_ctx_e)\n",
    "test_words_e = np.array(test_words_e)\n",
    "test_tags_e = np.array(test_tags_e)\n",
    "test_lemmas_e = np.array(test_lemmas_e)\n",
    "\n",
    "val_ctx_e = np.array(val_ctx_e)\n",
    "val_words_e = np.array(val_words_e)\n",
    "val_tags_e = np.array(val_tags_e)\n",
    "val_lemmas_e = np.array(val_lemmas_e)\n",
    "\n",
    "print(\"Context shape: \", dev_ctx_e.shape)\n",
    "print(\"Words shape: \", dev_words_e.shape)\n",
    "print(\"Tags shape: \", dev_tags_e.shape)\n",
    "print(\"Lemmas shape: \", dev_lemmas_e.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.argmax(y_true, axis=-1)\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    correct_predictions = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "    return accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 512\n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "def get_word2vec_weights(DIM):\n",
    "    # train word2vec model\n",
    "    word2vec = gensim.models.Word2Vec(dev_ctx, vector_size=DIM, window=10, min_count=1, workers=8)\n",
    "\n",
    "    # create an empty embedding matix\n",
    "    embedding_weights = np.zeros((VOCABULARY_SIZE, DIM))\n",
    "\n",
    "    # create a word to index dictionary mapping\n",
    "    word2id = word_tokenizer.word_index\n",
    "\n",
    "    # copy vectors from word2vec model to the words present in corpus\n",
    "    for word, index in word2id.items():\n",
    "        try:\n",
    "            embedding_weights[index, :] = word2vec.wv[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    return embedding_weights\n",
    "\n",
    "embedding_weights = get_word2vec_weights(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " context_input (InputLayer)     [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " tags_input (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " context_embedding (Embedding)  (None, 26, 512)      10096128    ['context_input[0][0]']          \n",
      "                                                                                                  \n",
      " repeat_vector_4 (RepeatVector)  (None, 26, 1)       0           ['tags_input[0][0]']             \n",
      "                                                                                                  \n",
      " words_embedding (Embedding)    (None, 26, 512)      30720       ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " context_embedding_dense (Dense  (None, 26, 512)     262656      ['context_embedding[1][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tags_embedding_dense2 (Dense)  (None, 26, 512)      1024        ['repeat_vector_4[1][0]']        \n",
      "                                                                                                  \n",
      " words_embedding_dense (Dense)  (None, 26, 512)      262656      ['words_embedding[1][0]']        \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 26, 512)      0           ['context_embedding_dense[1][0]',\n",
      "                                                                  'tags_embedding_dense2[1][0]',  \n",
      "                                                                  'words_embedding_dense[1][0]']  \n",
      "                                                                                                  \n",
      " combine_dense (Dense)          (None, 26, 512)      262656      ['add_4[1][0]']                  \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 26, 512)      0           ['combine_dense[1][0]']          \n",
      "                                                                                                  \n",
      " lstm (Bidirectional)           (None, 26, 1024)     4198400     ['dropout_8[1][0]']              \n",
      "                                                                                                  \n",
      " lstm2 (Bidirectional)          (None, 26, 1024)     6295552     ['lstm[1][0]']                   \n",
      "                                                                                                  \n",
      " dense1 (Dense)                 (None, 26, 512)      524800      ['lstm2[1][0]']                  \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 26, 512)      0           ['dense1[1][0]']                 \n",
      "                                                                                                  \n",
      " dense2 (Dense)                 (None, 26, 512)      262656      ['dropout_9[1][0]']              \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 26, 512)      0           ['dense2[1][0]']                 \n",
      "                                                                                                  \n",
      " dense3 (Dense)                 (None, 26, 512)      262656      ['dropout_10[1][0]']             \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 26, 60)       30780       ['dense3[1][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 22,490,684\n",
      "Trainable params: 12,394,556\n",
      "Non-trainable params: 10,096,128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Neural network model\n",
    "# inputs:\n",
    "#   - context: (batch_size, CTX_DIM * 2) \n",
    "#   - tags: encoded tags: (batch_size, 1)\n",
    "#   - words: encoded words: (batch_size, MAX_WORD_LENGTH)\n",
    "# outputs:\n",
    "#  - lemma: encoded lemma: (batch_size, MAX_WORD_LENGTH)\n",
    "\n",
    "def get_model():\n",
    "    # context\n",
    "    context_input = Input(shape=(CTX_DIM * 2,), name=\"context_input\")\n",
    "    \n",
    "    context_embedding = Embedding(len(word_tokenizer.word_index) + 1, EMBEDDING_DIM, input_length=CTX_DIM * 2, name=\"context_embedding\", trainable=False, weights=[embedding_weights])(context_input)\n",
    "    context_embedding = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"context_embedding_dense\")(context_embedding)\n",
    "\n",
    "    # tags\n",
    "    tags_input = Input(shape=(1,), name=\"tags_input\")\n",
    "    tags_embedding = RepeatVector(MAX_WORD_LENGTH)(tags_input)\n",
    "    tags_embedding = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"tags_embedding_dense2\")(tags_embedding)\n",
    "\n",
    "    # words\n",
    "    words_input = Input(shape=(MAX_WORD_LENGTH,), name=\"words_input\")\n",
    "    words_input = Masking(mask_value=0)(words_input)\n",
    "    \n",
    "    words_embedding = Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_WORD_LENGTH, name=\"words_embedding\", trainable=True)(words_input)\n",
    "    words_embedding = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"words_embedding_dense\")(words_embedding)\n",
    "\n",
    "    # combine context, tags and words\n",
    "    combine = Add()([context_embedding, tags_embedding, words_embedding])\n",
    "    combine = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"combine_dense\")(combine)\n",
    "\n",
    "    combine = Dropout(0.5)(combine)\n",
    "    lstm = Bidirectional(LSTM(EMBEDDING_DIM, return_sequences=True), name=\"lstm\")(combine)\n",
    "    lstm = Bidirectional(LSTM(EMBEDDING_DIM, return_sequences=True), name=\"lstm2\")(lstm)\n",
    "\n",
    "    # dense layers\n",
    "    dense1 = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"dense1\")(lstm)\n",
    "    dense1 = Dropout(0.4)(dense1)\n",
    "    dense2 = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"dense2\")(dense1)\n",
    "    dense2 = Dropout(0.4)(dense2)\n",
    "    dense3 = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"dense3\")(dense2)\n",
    "\n",
    "    # output\n",
    "    output = Dense(VOCAB_SIZE, activation=\"softmax\", name=\"output\")(dense3)\n",
    "\n",
    "    model = Model(inputs=[context_input, tags_input, words_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 20s 455ms/step - loss: 1.2687 - accuracy: 0.0000e+00 - val_loss: 0.8764 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 12s 353ms/step - loss: 0.8703 - accuracy: 0.0000e+00 - val_loss: 0.7438 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 11s 332ms/step - loss: 0.6942 - accuracy: 8.5449e-04 - val_loss: 0.5606 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 10s 328ms/step - loss: 0.4880 - accuracy: 0.0170 - val_loss: 0.3909 - val_accuracy: 0.0417 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 11s 341ms/step - loss: 0.3329 - accuracy: 0.0817 - val_loss: 0.2963 - val_accuracy: 0.1528 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 11s 358ms/step - loss: 0.2376 - accuracy: 0.2047 - val_loss: 0.2510 - val_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 10s 303ms/step - loss: 0.1900 - accuracy: 0.2941 - val_loss: 0.2110 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 10s 300ms/step - loss: 0.1464 - accuracy: 0.4026 - val_loss: 0.1631 - val_accuracy: 0.5417 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 10s 303ms/step - loss: 0.1259 - accuracy: 0.4750 - val_loss: 0.1579 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 10s 305ms/step - loss: 0.1012 - accuracy: 0.5808 - val_loss: 0.1310 - val_accuracy: 0.6944 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 10s 303ms/step - loss: 0.0855 - accuracy: 0.6452 - val_loss: 0.1179 - val_accuracy: 0.7083 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 10s 302ms/step - loss: 0.0732 - accuracy: 0.6936 - val_loss: 0.1081 - val_accuracy: 0.7222 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 11s 333ms/step - loss: 0.0636 - accuracy: 0.7200 - val_loss: 0.1091 - val_accuracy: 0.7361 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 10s 298ms/step - loss: 0.0736 - accuracy: 0.6423 - val_loss: 0.0948 - val_accuracy: 0.7222 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 11s 330ms/step - loss: 0.0525 - accuracy: 0.7487 - val_loss: 0.0771 - val_accuracy: 0.7639 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 10s 319ms/step - loss: 0.0427 - accuracy: 0.7927 - val_loss: 0.0767 - val_accuracy: 0.7639 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 9s 293ms/step - loss: 0.1138 - accuracy: 0.5748 - val_loss: 0.1135 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 10s 313ms/step - loss: 0.0589 - accuracy: 0.7294 - val_loss: 0.0870 - val_accuracy: 0.7639 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 10s 303ms/step - loss: 0.0387 - accuracy: 0.8131 - val_loss: 0.0724 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 10s 318ms/step - loss: 0.0338 - accuracy: 0.8271 - val_loss: 0.0514 - val_accuracy: 0.8472 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "14/32 [============>.................] - ETA: 6s - loss: 0.0346 - accuracy: 0.8376"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[accuracy])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_accuracy\", patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.5, patience=5, min_lr=0.00001)\n",
    "\n",
    "# train model\n",
    "history = model.fit([dev_ctx_e, dev_tags_e, dev_words_e], dev_lemmas_e, epochs=100, batch_size=256, validation_data=([val_ctx_e, val_tags_e, val_words_e], val_lemmas_e), callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1957/1957 [==============================] - 104s 53ms/step - loss: 0.0275 - accuracy: 0.9402\n",
      "Test loss:  0.02748919650912285\n",
      "Test accuracy:  0.940166711807251\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "result = model.evaluate([test_ctx_e, test_tags_e, test_words_e], test_lemmas_e)\n",
    "print(\"Test loss: \", result[0])\n",
    "print(\"Test accuracy: \", result[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
