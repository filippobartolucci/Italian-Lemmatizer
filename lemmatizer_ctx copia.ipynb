{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian Word Lemmatizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Bidirectional, TimeDistributed, RepeatVector, Activation, Dot, Lambda, Dropout, Add, Multiply, Masking, Attention\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import gensim\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# set all random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  95\n",
      "Max sentence length:  107\n",
      "Number of sentences in dev set:  703\n",
      "Number of sentences in test set:  5596\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_path = \"./dev.csv\"\n",
    "df_dev = pd.read_csv(dataset_path, sep=\"\\t\", header=None,\n",
    "                     names=[\"word\", \"tag\", \"lemm\"])\n",
    "\n",
    "dataset_path = \"./test.csv\"\n",
    "df_test = pd.read_csv(dataset_path, sep=\"\\t\", header=None,\n",
    "                      names=[\"word\", \"tag\", \"lemm\"])\n",
    "\n",
    "df_dev[\"word\"] = df_dev[\"word\"].astype(str)\n",
    "df_dev[\"tag\"] = df_dev[\"tag\"].astype(str)\n",
    "df_dev[\"lemm\"] = df_dev[\"lemm\"].astype(str)\n",
    "\n",
    "df_test[\"word\"] = df_test[\"word\"].astype(str)\n",
    "df_test[\"tag\"] = df_test[\"tag\"].astype(str)\n",
    "df_test[\"lemm\"] = df_test[\"lemm\"].astype(str)\n",
    "\n",
    "# remove head\n",
    "df_dev = df_dev.iloc[1:]\n",
    "df_test = df_test.iloc[1:]\n",
    "\n",
    "# removing rows where tag is nan\n",
    "df_dev = df_dev.dropna(subset=[\"tag\"])\n",
    "df_dev = df_dev[df_dev[\"tag\"] != \"nan\"]\n",
    "df_test = df_test.dropna(subset=[\"tag\"])\n",
    "df_test = df_test[df_test[\"tag\"] != \"nan\"]\n",
    "\n",
    "# lower case all words\n",
    "df_test[\"word\"] = df_test[\"word\"].str.lower()\n",
    "df_dev[\"word\"] = df_dev[\"word\"].str.lower()\n",
    "\n",
    "\n",
    "def get_sentences(df):\n",
    "    words = []\n",
    "    tags = []\n",
    "    lemmas = []\n",
    "    sentence = []\n",
    "    max_s = 0\n",
    "    for index, row in df.iterrows():\n",
    "        word = row[\"word\"]\n",
    "        tag = row[\"tag\"]\n",
    "        lemm = row[\"lemm\"]\n",
    "        sentence.append([word, tag, lemm])\n",
    "\n",
    "        if row[\"word\"] in [\".\", \"?\", \"!\", \";\"]:\n",
    "            words.append([word for word, tag, lemm in sentence])\n",
    "            tags.append([tag for word, tag, lemm in sentence])\n",
    "            lemmas.append([lemm for word, tag, lemm in sentence])\n",
    "            max_s = max(max_s, len(sentence))\n",
    "            sentence = []\n",
    "\n",
    "    print(\"Max sentence length: \", max_s)\n",
    "    return words, tags, lemmas\n",
    "\n",
    "# _s is for string\n",
    "dev_words_s, dev_tags_s, dev_lemmas_s = get_sentences(df_dev)\n",
    "test_words_s, test_tags_s, test_lemmas_s = get_sentences(df_test)\n",
    "print(\"Number of sentences in dev set: \", len(dev_words_s))\n",
    "print(\"Number of sentences in test set: \", len(test_words_s))\n",
    "\n",
    "for i in range(len(dev_words_s)):\n",
    "    if len(dev_words_s[i]) != len(dev_tags_s[i]) or len(dev_words_s[i]) != len(dev_lemmas_s[i]):\n",
    "        print(\"Dimension mismatch in sentence: \", i)\n",
    "        print(\"Words: \", dev_words_s[i])\n",
    "        print(\"Tags: \", dev_tags_s[i])\n",
    "        print(\"Lemmas: \", dev_lemmas_s[i])\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTX_DIM = 13\n",
    "PRE_VALUE = \"<PRE>\"\n",
    "POST_VALUE = \"<POST>\"\n",
    "\n",
    "def get_context(words, tags, lemmas):\n",
    "    ctx = []\n",
    "    w = []\n",
    "    tag = []\n",
    "    lemma = []\n",
    "\n",
    "    for s_index in range(len(words)):\n",
    "        s = words[s_index]\n",
    "        sentence = \" \".join(s)\n",
    "        s = [PRE_VALUE] * CTX_DIM + s + [POST_VALUE] * CTX_DIM\n",
    "\n",
    "        for w_index in range(len(s)):\n",
    "            if w_index < CTX_DIM or w_index >= len(s) - CTX_DIM:\n",
    "                continue\n",
    "\n",
    "            context = s[w_index - CTX_DIM:w_index] + s[w_index + 1:w_index  + CTX_DIM + 1]\n",
    "            context = \" \".join(context)\n",
    "            ctx.append(context)\n",
    "            w.append(words[s_index][w_index-CTX_DIM])\n",
    "            tag.append(tags[s_index][w_index-CTX_DIM])\n",
    "            lemma.append(lemmas[s_index][w_index-CTX_DIM])\n",
    "\n",
    "    return ctx, w, tag, lemma\n",
    "\n",
    "dev_ctx, dev_words, dev_tags, dev_lemmas = get_context(dev_words_s, dev_tags_s, dev_lemmas_s)\n",
    "test_ctx, test_words, test_tags, test_lemmas = get_context(test_words_s, test_tags_s, test_lemmas_s)\n",
    "\n",
    "dev_ctx, val_ctx, dev_words, val_words, dev_tags, val_tags, dev_lemmas, val_lemmas = train_test_split(dev_ctx, dev_words, dev_tags, dev_lemmas, test_size=0.01, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTX Dim: 13 \n",
      "\n",
      "Context:  con anni o mesi di anticipo , per gli asteroidi vaganti il rischio collisione inaspettata &egrave; molto pi&ugrave; grande e il fenomeno deve essere preso sul\n",
      "Word:  di\n",
      "Tag:  prep\n",
      "Lemma:  di\n",
      "\n",
      "Context:  <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> perfino negli usa che pure sono all' avanguardia e vantano una diffusione capillare del telefono (\n",
      "Word:  ,\n",
      "Tag:  p_oth\n",
      "Lemma:  ,\n",
      "\n",
      "Context:  <PRE> <PRE> con il risultato che i loro figli si trovano avvantaggiati , hanno pi&ugrave; tempo per crescere prima_che giunga la cattiva stagione e pi&ugrave; tempo\n",
      "Word:  in_quanto\n",
      "Tag:  conj_s\n",
      "Lemma:  in_quanto\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CTX Dim:\", CTX_DIM, \"\\n\")\n",
    "for i in range(3):\n",
    "    index = np.random.randint(0, len(dev_ctx))\n",
    "    print(\"Context: \", dev_ctx[index])\n",
    "    print(\"Word: \", dev_words[index])\n",
    "    print(\"Tag: \", dev_tags[index])\n",
    "    print(\"Lemma: \", dev_lemmas[index])\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Class Words\n",
    "The evaluation is done only on open-class words and not to functional words: only the tokens having a PoS-tag comprised in the set ADJ *, ADV, NN, V * had to be lemmatised, in all the other cases the token could be copied unchanged into the lemma column as they were not considered for the evaluation (the asterisk indicates all PoS-tag possibilities beginning with that prefix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_open_class_words(ctx, words, tags, lemmas):\n",
    "    open_class_words = []\n",
    "    open_class_ctx = []\n",
    "    open_class_tags = []\n",
    "    open_class_lemmas = []\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if \"adj_\" in tags[i] or \"adv\" in tags[i] or \"nn\" in tags[i] or \"v_\" in tags[i]:\n",
    "            open_class_words.append(words[i])\n",
    "            open_class_ctx.append(ctx[i])\n",
    "            open_class_tags.append(tags[i])\n",
    "            open_class_lemmas.append(lemmas[i])\n",
    "\n",
    "    return open_class_ctx, open_class_words, open_class_tags, open_class_lemmas\n",
    "\n",
    "\n",
    "test_ctx, test_words, test_tags, test_lemmas = get_open_class_words(test_ctx, test_words, test_tags, test_lemmas)\n",
    "dev_ctx, dev_words, dev_tags, dev_lemmas = get_open_class_words(dev_ctx, dev_words, dev_tags, dev_lemmas)\n",
    "val_ctx, val_words, val_tags, val_lemmas = get_open_class_words(val_ctx, val_words, val_tags, val_lemmas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  60\n",
      "Max word length:  26\n",
      "Context:  <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> ha una grossa con denti bianchi e forti , tranne_che per un canino che si  ->  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 26, 22, 1964, 23, 1757, 3474, 5, 1116, 3, 14624, 13, 11, 6239, 7, 17]\n",
      "Words:  bocca  ->  [35 48 36 36 34  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n",
      "Tag:  nn  ->  [1]\n",
      "Lemma:  bocca  ->  [35 48 36 36 34  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n"
     ]
    }
   ],
   "source": [
    "# word encoder\n",
    "word_tokenizer = Tokenizer(filters=\"\")\n",
    "word_tokenizer.fit_on_texts(dev_ctx + test_ctx + val_ctx)\n",
    "\n",
    "# tag encoder\n",
    "tag_tokenizer = Tokenizer(filters=\"\")\n",
    "tag_tokenizer.fit_on_texts(dev_tags_s + test_tags_s)\n",
    "\n",
    "# lemma encoder\n",
    "lemma_tokenizer = Tokenizer(filters=\"\")\n",
    "lemma_tokenizer.fit_on_texts(dev_lemmas_s + test_lemmas_s)\n",
    "\n",
    "dev_ctx_e = word_tokenizer.texts_to_sequences(dev_ctx)\n",
    "val_ctx_e = word_tokenizer.texts_to_sequences(val_ctx)\n",
    "test_ctx_e = word_tokenizer.texts_to_sequences(test_ctx)\n",
    "\n",
    "PRE_E = word_tokenizer.texts_to_sequences([PRE_VALUE])[0][0]\n",
    "POST_E = word_tokenizer.texts_to_sequences([POST_VALUE])[0][0]\n",
    "\n",
    "\n",
    "dev_tags_e = tag_tokenizer.texts_to_sequences(dev_tags)\n",
    "val_tags_e = tag_tokenizer.texts_to_sequences(val_tags)\n",
    "test_tags_e = tag_tokenizer.texts_to_sequences(test_tags)\n",
    "\n",
    "\n",
    "# get all unique letter in words\n",
    "characters = set()\n",
    "\n",
    "for lemma in df_dev[\"lemm\"].unique():\n",
    "    for letter in lemma:\n",
    "        characters.add(letter)\n",
    "\n",
    "for lemma in df_test[\"lemm\"].unique():\n",
    "    for letter in lemma:\n",
    "        characters.add(letter)\n",
    "\n",
    "# add padding and unknown to characters\n",
    "characters.add(\" \")\n",
    "\n",
    "# the length of the vocab for one-hot encoded char\n",
    "VOCAB_SIZE = len(characters)\n",
    "\n",
    "print (\"Vocab size: \", VOCAB_SIZE)\n",
    "# order characters\n",
    "characters = sorted(list(characters))\n",
    "\n",
    "\n",
    "char2idx = {char: idx for idx, char in enumerate(characters)}\n",
    "idx2char = {idx: char for idx, char in enumerate(characters)}\n",
    "\n",
    "MAX_WORD_LENGTH = 0\n",
    "for w in dev_words + test_words + dev_lemmas + test_lemmas:\n",
    "    MAX_WORD_LENGTH = max(MAX_WORD_LENGTH, len(w))\n",
    "MAX_WORD_LENGTH += 1\n",
    "print(\"Max word length: \", MAX_WORD_LENGTH)\n",
    "\n",
    "def encode_words(words):\n",
    "    encoded_words = []\n",
    "    for word in words:\n",
    "        word_e = []\n",
    "        for letter in word:\n",
    "            word_e.append(characters.index(letter))\n",
    "        encoded_words.append(word_e)\n",
    "    return encoded_words\n",
    "\n",
    "dev_words_e = encode_words(dev_words)\n",
    "test_words_e = encode_words(test_words)\n",
    "val_words_e = encode_words(val_words)\n",
    "\n",
    "dev_lemmas_e = encode_words(dev_lemmas)\n",
    "test_lemmas_e = encode_words(test_lemmas)\n",
    "val_lemmas_e = encode_words(val_lemmas)\n",
    "\n",
    "dev_words_e = tf.keras.preprocessing.sequence.pad_sequences(dev_words_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "test_words_e = tf.keras.preprocessing.sequence.pad_sequences(test_words_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "\n",
    "dev_lemmas_e = tf.keras.preprocessing.sequence.pad_sequences(dev_lemmas_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "test_lemmas_e = tf.keras.preprocessing.sequence.pad_sequences(test_lemmas_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "\n",
    "val_words_e = tf.keras.preprocessing.sequence.pad_sequences(val_words_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "val_lemmas_e = tf.keras.preprocessing.sequence.pad_sequences(val_lemmas_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "\n",
    "# show random data point\n",
    "index = np.random.randint(0, len(dev_ctx))\n",
    "print(\"Context: \", dev_ctx[index], \" -> \", dev_ctx_e[index])\n",
    "print(\"Words: \", dev_words[index], \" -> \", dev_words_e[index])\n",
    "print(\"Tag: \", dev_tags[index], \" -> \", dev_tags_e[index])\n",
    "print(\"Lemma: \", dev_lemmas[index], \" -> \", dev_lemmas_e[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the characters for the lemmas\n",
    "dev_lemmas_e = tf.one_hot(dev_lemmas_e, VOCAB_SIZE)\n",
    "test_lemmas_e = tf.one_hot(test_lemmas_e, VOCAB_SIZE)\n",
    "val_lemmas_e = tf.one_hot(val_lemmas_e, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context shape:  (8086, 26)\n",
      "Words shape:  (8086, 26)\n",
      "Tags shape:  (8086, 1)\n",
      "Lemmas shape:  (8086, 26, 60)\n"
     ]
    }
   ],
   "source": [
    "# trnasform to numpy array\n",
    "dev_ctx_e = np.array(dev_ctx_e)\n",
    "dev_words_e = np.array(dev_words_e)\n",
    "dev_tags_e = np.array(dev_tags_e)\n",
    "dev_lemmas_e = np.array(dev_lemmas_e)\n",
    "\n",
    "test_ctx_e = np.array(test_ctx_e)\n",
    "test_words_e = np.array(test_words_e)\n",
    "test_tags_e = np.array(test_tags_e)\n",
    "test_lemmas_e = np.array(test_lemmas_e)\n",
    "\n",
    "val_ctx_e = np.array(val_ctx_e)\n",
    "val_words_e = np.array(val_words_e)\n",
    "val_tags_e = np.array(val_tags_e)\n",
    "val_lemmas_e = np.array(val_lemmas_e)\n",
    "\n",
    "print(\"Context shape: \", dev_ctx_e.shape)\n",
    "print(\"Words shape: \", dev_words_e.shape)\n",
    "print(\"Tags shape: \", dev_tags_e.shape)\n",
    "print(\"Lemmas shape: \", dev_lemmas_e.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.argmax(y_true, axis=-1)\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    correct_predictions = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "    return accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 512\n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "def get_word2vec_weights(DIM):\n",
    "    # train word2vec model\n",
    "    word2vec = gensim.models.Word2Vec(dev_ctx, vector_size=DIM, window=10, min_count=1, workers=8)\n",
    "\n",
    "    # create an empty embedding matix\n",
    "    embedding_weights = np.zeros((VOCABULARY_SIZE, DIM))\n",
    "\n",
    "    # create a word to index dictionary mapping\n",
    "    word2id = word_tokenizer.word_index\n",
    "\n",
    "    # copy vectors from word2vec model to the words present in corpus\n",
    "    for word, index in word2id.items():\n",
    "        try:\n",
    "            embedding_weights[index, :] = word2vec.wv[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    return embedding_weights\n",
    "\n",
    "embedding_weights = get_word2vec_weights(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " context_input (InputLayer)     [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " tags_input (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " context_embedding (Embedding)  (None, 26, 512)      10096128    ['context_input[0][0]']          \n",
      "                                                                                                  \n",
      " repeat_vector_20 (RepeatVector  (None, 26, 1)       0           ['tags_input[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_19 (Bidirectiona  (None, 26, 512)     1574912     ['context_embedding[1][0]']      \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " tags_embedding_dense2 (Dense)  (None, 26, 512)      1024        ['repeat_vector_20[1][0]']       \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_19 (Attention)       (None, 26, 512)      1           ['bidirectional_19[1][0]',       \n",
      "                                                                  'tags_embedding_dense2[1][0]']  \n",
      "                                                                                                  \n",
      " words_embedding (Embedding)    (None, 26, 512)      30720       ['input_20[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 26, 1024)     0           ['attention_19[1][0]',           \n",
      "                                                                  'words_embedding[1][0]']        \n",
      "                                                                                                  \n",
      " dropout_60 (Dropout)           (None, 26, 1024)     0           ['concatenate_14[1][0]']         \n",
      "                                                                                                  \n",
      " lstm (Bidirectional)           (None, 26, 1024)     6295552     ['dropout_60[1][0]']             \n",
      "                                                                                                  \n",
      " lstm1 (Bidirectional)          (None, 26, 1024)     6295552     ['lstm[1][0]']                   \n",
      "                                                                                                  \n",
      " dense1 (Dense)                 (None, 26, 512)      524800      ['lstm1[1][0]']                  \n",
      "                                                                                                  \n",
      " dropout_61 (Dropout)           (None, 26, 512)      0           ['dense1[1][0]']                 \n",
      "                                                                                                  \n",
      " dense2 (Dense)                 (None, 26, 512)      262656      ['dropout_61[1][0]']             \n",
      "                                                                                                  \n",
      " dropout_62 (Dropout)           (None, 26, 512)      0           ['dense2[1][0]']                 \n",
      "                                                                                                  \n",
      " dense3 (Dense)                 (None, 26, 512)      262656      ['dropout_62[1][0]']             \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 26, 60)       30780       ['dense3[1][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25,374,781\n",
      "Trainable params: 15,278,653\n",
      "Non-trainable params: 10,096,128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Neural network model\n",
    "# inputs:\n",
    "#   - context: (batch_size, CTX_DIM * 2) \n",
    "#   - tags: encoded tags: (batch_size, 1)\n",
    "#   - words: encoded words: (batch_size, MAX_WORD_LENGTH)\n",
    "# outputs:\n",
    "#  - lemma: encoded lemma: (batch_size, MAX_WORD_LENGTH)\n",
    "\n",
    "def get_model():\n",
    "    # context\n",
    "    context_input = Input(shape=(CTX_DIM * 2,), name=\"context_input\")\n",
    "    context_embedding = Embedding(len(word_tokenizer.word_index) + 1, EMBEDDING_DIM, input_length=CTX_DIM * 2,name=\"context_embedding\", trainable=False, weights=[embedding_weights])(context_input)\n",
    "    context_embedding = Bidirectional(LSTM(int(EMBEDDING_DIM/2), return_sequences=True, name=\"context_lstm\"))(context_embedding)\n",
    "\n",
    "    # tags\n",
    "    tags_input = Input(shape=(1,), name=\"tags_input\")\n",
    "    tags_embedding = RepeatVector(MAX_WORD_LENGTH)(tags_input)\n",
    "    tags_embedding = Dense(EMBEDDING_DIM, activation=\"swish\",name=\"tags_embedding_dense2\")(tags_embedding)\n",
    "\n",
    "    # words\n",
    "    words_input = Input(shape=(MAX_WORD_LENGTH,), name=\"words_input\")\n",
    "    words_input = Masking(mask_value=0)(words_input)\n",
    "\n",
    "    words_embedding = Embedding(VOCAB_SIZE, int(EMBEDDING_DIM), input_length=MAX_WORD_LENGTH, name=\"words_embedding\", trainable=True)(words_input)\n",
    "\n",
    "    attention = tf.keras.layers.Attention(use_scale=True)([context_embedding, tags_embedding])\n",
    "\n",
    "    # combine\n",
    "    combine = Concatenate()([attention, words_embedding])\n",
    "\n",
    "    combine = Dropout(0.5)(combine)\n",
    "    lstm = Bidirectional(LSTM(EMBEDDING_DIM, return_sequences=True), name=\"lstm\")(combine)\n",
    "    lstm = Bidirectional(LSTM(EMBEDDING_DIM, return_sequences=True), name=\"lstm1\")(lstm)\n",
    "\n",
    "    # dense layers\n",
    "    dense1 = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"dense1\")(lstm)\n",
    "    dense1 = Dropout(0.5)(dense1)\n",
    "    dense2 = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"dense2\")(dense1)\n",
    "    dense2 = Dropout(0.5)(dense2)\n",
    "    dense3 = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"dense3\")(dense2)\n",
    "    dense4 = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"dense4\")(dense3)\n",
    "\n",
    "    # output\n",
    "    output = Dense(VOCAB_SIZE, activation=\"softmax\", name=\"output\")(dense3)\n",
    "\n",
    "    model = Model(inputs=[context_input, tags_input,\n",
    "                  words_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 32s 709ms/step - loss: 1.2267 - accuracy: 0.0000e+00 - val_loss: 0.7508 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 11s 345ms/step - loss: 0.7552 - accuracy: 0.0000e+00 - val_loss: 0.6992 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 11s 334ms/step - loss: 0.6506 - accuracy: 8.5449e-04 - val_loss: 0.5280 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 11s 355ms/step - loss: 0.4858 - accuracy: 0.0140 - val_loss: 0.3951 - val_accuracy: 0.0556\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 11s 337ms/step - loss: 0.3191 - accuracy: 0.1208 - val_loss: 0.2291 - val_accuracy: 0.4167\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 11s 335ms/step - loss: 0.1807 - accuracy: 0.3783 - val_loss: 0.1723 - val_accuracy: 0.5278\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 10s 326ms/step - loss: 0.1223 - accuracy: 0.5435 - val_loss: 0.1461 - val_accuracy: 0.6528\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 10s 328ms/step - loss: 0.0977 - accuracy: 0.6247 - val_loss: 0.1190 - val_accuracy: 0.7083\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 10s 326ms/step - loss: 0.0831 - accuracy: 0.6777 - val_loss: 0.1069 - val_accuracy: 0.6944\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 10s 321ms/step - loss: 0.0635 - accuracy: 0.7409 - val_loss: 0.1027 - val_accuracy: 0.7083\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 10s 321ms/step - loss: 0.0517 - accuracy: 0.7852 - val_loss: 0.0885 - val_accuracy: 0.7778\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 10s 325ms/step - loss: 0.0446 - accuracy: 0.8043 - val_loss: 0.0763 - val_accuracy: 0.7917\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 10s 326ms/step - loss: 0.0393 - accuracy: 0.8198 - val_loss: 0.0685 - val_accuracy: 0.7917\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 10s 323ms/step - loss: 0.0350 - accuracy: 0.8334 - val_loss: 0.0629 - val_accuracy: 0.8333\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 10s 320ms/step - loss: 0.0297 - accuracy: 0.8532 - val_loss: 0.0555 - val_accuracy: 0.8611\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 10s 320ms/step - loss: 0.0273 - accuracy: 0.8627 - val_loss: 0.0538 - val_accuracy: 0.8333\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 10s 321ms/step - loss: 0.0250 - accuracy: 0.8738 - val_loss: 0.0437 - val_accuracy: 0.8472\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 10s 319ms/step - loss: 0.0217 - accuracy: 0.8871 - val_loss: 0.0405 - val_accuracy: 0.8611\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 10s 320ms/step - loss: 0.0210 - accuracy: 0.8851 - val_loss: 0.0369 - val_accuracy: 0.8611\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 10s 321ms/step - loss: 0.0196 - accuracy: 0.8938 - val_loss: 0.0410 - val_accuracy: 0.8056\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 10s 318ms/step - loss: 0.0175 - accuracy: 0.9040 - val_loss: 0.0327 - val_accuracy: 0.8611\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 10s 322ms/step - loss: 0.0137 - accuracy: 0.9216 - val_loss: 0.0324 - val_accuracy: 0.8750\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 10s 319ms/step - loss: 0.0121 - accuracy: 0.9281 - val_loss: 0.0339 - val_accuracy: 0.8889\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 10s 317ms/step - loss: 0.0112 - accuracy: 0.9317 - val_loss: 0.0243 - val_accuracy: 0.9306\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 10s 315ms/step - loss: 0.0103 - accuracy: 0.9367 - val_loss: 0.0345 - val_accuracy: 0.8472\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 10s 319ms/step - loss: 0.0109 - accuracy: 0.9335 - val_loss: 0.0199 - val_accuracy: 0.9306\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 10s 314ms/step - loss: 0.0093 - accuracy: 0.9463 - val_loss: 0.0336 - val_accuracy: 0.9167\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 10s 317ms/step - loss: 0.0096 - accuracy: 0.9439 - val_loss: 0.0264 - val_accuracy: 0.9028\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 10s 315ms/step - loss: 0.0068 - accuracy: 0.9531 - val_loss: 0.0290 - val_accuracy: 0.9306\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 10s 315ms/step - loss: 0.0057 - accuracy: 0.9634 - val_loss: 0.0240 - val_accuracy: 0.8889\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 10s 317ms/step - loss: 0.0047 - accuracy: 0.9666 - val_loss: 0.0226 - val_accuracy: 0.9306\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 10s 315ms/step - loss: 0.0049 - accuracy: 0.9684 - val_loss: 0.0185 - val_accuracy: 0.9444\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 10s 318ms/step - loss: 0.0051 - accuracy: 0.9662 - val_loss: 0.0288 - val_accuracy: 0.9167\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 10s 317ms/step - loss: 0.0059 - accuracy: 0.9625 - val_loss: 0.0327 - val_accuracy: 0.8750\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 10s 318ms/step - loss: 0.0063 - accuracy: 0.9543 - val_loss: 0.0346 - val_accuracy: 0.9167\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 10s 317ms/step - loss: 0.0045 - accuracy: 0.9695 - val_loss: 0.0283 - val_accuracy: 0.9167\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 10s 319ms/step - loss: 0.0039 - accuracy: 0.9715 - val_loss: 0.0169 - val_accuracy: 0.9583\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 10s 316ms/step - loss: 0.0026 - accuracy: 0.9809 - val_loss: 0.0258 - val_accuracy: 0.9306\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 10s 317ms/step - loss: 0.0022 - accuracy: 0.9840 - val_loss: 0.0270 - val_accuracy: 0.9028\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 10s 317ms/step - loss: 0.0018 - accuracy: 0.9860 - val_loss: 0.0343 - val_accuracy: 0.9028\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 10s 317ms/step - loss: 0.0016 - accuracy: 0.9881 - val_loss: 0.0216 - val_accuracy: 0.9444\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 10s 316ms/step - loss: 0.0013 - accuracy: 0.9904 - val_loss: 0.0206 - val_accuracy: 0.9167\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 10s 316ms/step - loss: 9.3668e-04 - accuracy: 0.9931 - val_loss: 0.0209 - val_accuracy: 0.9306\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 10s 316ms/step - loss: 0.0013 - accuracy: 0.9898 - val_loss: 0.0287 - val_accuracy: 0.9306\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 10s 316ms/step - loss: 0.0014 - accuracy: 0.9889 - val_loss: 0.0330 - val_accuracy: 0.9306\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 10s 316ms/step - loss: 0.0012 - accuracy: 0.9911 - val_loss: 0.0200 - val_accuracy: 0.9306\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 10s 316ms/step - loss: 0.0029 - accuracy: 0.9852 - val_loss: 0.0541 - val_accuracy: 0.9444\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 10s 316ms/step - loss: 0.0058 - accuracy: 0.9658 - val_loss: 0.0215 - val_accuracy: 0.9306\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 10s 317ms/step - loss: 0.0030 - accuracy: 0.9779 - val_loss: 0.0171 - val_accuracy: 0.9167\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 10s 317ms/step - loss: 0.0021 - accuracy: 0.9844 - val_loss: 0.0189 - val_accuracy: 0.9306\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 10s 316ms/step - loss: 0.0016 - accuracy: 0.9869 - val_loss: 0.0189 - val_accuracy: 0.9306\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 10s 317ms/step - loss: 0.0011 - accuracy: 0.9919 - val_loss: 0.0194 - val_accuracy: 0.9444\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 10s 318ms/step - loss: 7.3511e-04 - accuracy: 0.9943 - val_loss: 0.0203 - val_accuracy: 0.9306\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 10s 317ms/step - loss: 7.1649e-04 - accuracy: 0.9943 - val_loss: 0.0088 - val_accuracy: 0.9444\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 10s 316ms/step - loss: 7.7480e-04 - accuracy: 0.9939 - val_loss: 0.0200 - val_accuracy: 0.9306\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 10s 315ms/step - loss: 8.8344e-04 - accuracy: 0.9928 - val_loss: 0.0218 - val_accuracy: 0.9306\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 10s 316ms/step - loss: 0.0318 - accuracy: 0.9408 - val_loss: 0.5764 - val_accuracy: 0.2500\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 10s 317ms/step - loss: 0.0972 - accuracy: 0.6548 - val_loss: 0.0605 - val_accuracy: 0.8611\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 10s 318ms/step - loss: 0.0132 - accuracy: 0.9261 - val_loss: 0.0240 - val_accuracy: 0.9306\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 10s 318ms/step - loss: 0.0051 - accuracy: 0.9680 - val_loss: 0.0193 - val_accuracy: 0.9306\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 10s 317ms/step - loss: 0.0029 - accuracy: 0.9798 - val_loss: 0.0211 - val_accuracy: 0.9306\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 10s 318ms/step - loss: 0.0015 - accuracy: 0.9903 - val_loss: 0.0210 - val_accuracy: 0.9306\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 10s 317ms/step - loss: 0.0136 - accuracy: 0.9598 - val_loss: 0.0194 - val_accuracy: 0.8889\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 10s 312ms/step - loss: 0.0031 - accuracy: 0.9801 - val_loss: 0.0259 - val_accuracy: 0.9444\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 10s 311ms/step - loss: 0.0013 - accuracy: 0.9917 - val_loss: 0.0145 - val_accuracy: 0.9306\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 10s 310ms/step - loss: 8.2352e-04 - accuracy: 0.9943 - val_loss: 0.0137 - val_accuracy: 0.9306\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 10s 312ms/step - loss: 5.8954e-04 - accuracy: 0.9965 - val_loss: 0.0099 - val_accuracy: 0.9444\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 10s 311ms/step - loss: 4.3140e-04 - accuracy: 0.9977 - val_loss: 0.0155 - val_accuracy: 0.9306\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 10s 311ms/step - loss: 3.5019e-04 - accuracy: 0.9983 - val_loss: 0.0120 - val_accuracy: 0.9444\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 10s 311ms/step - loss: 3.1479e-04 - accuracy: 0.9983 - val_loss: 0.0166 - val_accuracy: 0.9444\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 10s 310ms/step - loss: 3.0401e-04 - accuracy: 0.9988 - val_loss: 0.0176 - val_accuracy: 0.9306\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 10s 311ms/step - loss: 2.8074e-04 - accuracy: 0.9983 - val_loss: 0.0135 - val_accuracy: 0.9583\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 10s 312ms/step - loss: 2.2005e-04 - accuracy: 0.9988 - val_loss: 0.0079 - val_accuracy: 0.9444\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 10s 311ms/step - loss: 1.9299e-04 - accuracy: 0.9995 - val_loss: 0.0144 - val_accuracy: 0.9306\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 10s 310ms/step - loss: 2.1217e-04 - accuracy: 0.9989 - val_loss: 0.0176 - val_accuracy: 0.9167\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 10s 310ms/step - loss: 1.7462e-04 - accuracy: 0.9991 - val_loss: 0.0196 - val_accuracy: 0.9167\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 10s 310ms/step - loss: 1.3967e-04 - accuracy: 0.9995 - val_loss: 0.0169 - val_accuracy: 0.9306\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 10s 310ms/step - loss: 1.1579e-04 - accuracy: 0.9994 - val_loss: 0.0184 - val_accuracy: 0.9306\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 10s 310ms/step - loss: 1.7061e-04 - accuracy: 0.9988 - val_loss: 0.0190 - val_accuracy: 0.9444\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 10s 310ms/step - loss: 1.6583e-04 - accuracy: 0.9991 - val_loss: 0.0182 - val_accuracy: 0.9444\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 10s 310ms/step - loss: 1.2196e-04 - accuracy: 0.9994 - val_loss: 0.0228 - val_accuracy: 0.9444\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 10s 309ms/step - loss: 1.4061e-04 - accuracy: 0.9990 - val_loss: 0.0213 - val_accuracy: 0.9444\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 10s 309ms/step - loss: 2.1048e-04 - accuracy: 0.9988 - val_loss: 0.0211 - val_accuracy: 0.9444\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 10s 310ms/step - loss: 2.0691e-04 - accuracy: 0.9989 - val_loss: 0.0213 - val_accuracy: 0.9306\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 10s 310ms/step - loss: 2.5151e-04 - accuracy: 0.9980 - val_loss: 0.0241 - val_accuracy: 0.9167\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 10s 311ms/step - loss: 3.8835e-04 - accuracy: 0.9972 - val_loss: 0.0168 - val_accuracy: 0.9306\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 10s 309ms/step - loss: 2.4977e-04 - accuracy: 0.9979 - val_loss: 0.0190 - val_accuracy: 0.9167\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 10s 308ms/step - loss: 0.0013 - accuracy: 0.9915 - val_loss: 0.0270 - val_accuracy: 0.9167\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 10s 309ms/step - loss: 0.0019 - accuracy: 0.9871 - val_loss: 0.0140 - val_accuracy: 0.9306\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 10s 309ms/step - loss: 7.4455e-04 - accuracy: 0.9954 - val_loss: 0.0086 - val_accuracy: 0.9444\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 10s 309ms/step - loss: 7.0852e-04 - accuracy: 0.9962 - val_loss: 0.0129 - val_accuracy: 0.9306\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 10s 309ms/step - loss: 0.0011 - accuracy: 0.9934 - val_loss: 0.0160 - val_accuracy: 0.9444\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 10s 311ms/step - loss: 0.0018 - accuracy: 0.9882 - val_loss: 0.0108 - val_accuracy: 0.9306\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[accuracy])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True)\n",
    "\n",
    "# train model\n",
    "history = model.fit([dev_ctx_e, dev_tags_e, dev_words_e], dev_lemmas_e, epochs=100, batch_size=256, validation_data=([val_ctx_e, val_tags_e, val_words_e], val_lemmas_e), callbacks=[early_stopping])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1957/1957 [==============================] - 138s 70ms/step - loss: 0.0297 - accuracy: 0.9537\n",
      "Test loss:  0.02970941551029682\n",
      "Test accuracy:  0.9537237882614136\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "result = model.evaluate([test_ctx_e, test_tags_e, test_words_e], test_lemmas_e)\n",
    "print(\"Test loss: \", result[0])\n",
    "print(\"Test accuracy: \", result[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
