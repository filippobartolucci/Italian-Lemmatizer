{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian Word Lemmatizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Bidirectional, TimeDistributed, RepeatVector, Activation, Dot, Lambda, Dropout\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import gensim\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# set all random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  95\n",
      "Max sentence length:  107\n",
      "Number of sentences in dev set:  703\n",
      "Number of sentences in test set:  5596\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_path = \"./dev.csv\"\n",
    "df_dev = pd.read_csv(dataset_path, sep=\"\\t\", header=None,\n",
    "                     names=[\"word\", \"tag\", \"lemm\"])\n",
    "\n",
    "dataset_path = \"./test.csv\"\n",
    "df_test = pd.read_csv(dataset_path, sep=\"\\t\", header=None,\n",
    "                      names=[\"word\", \"tag\", \"lemm\"])\n",
    "\n",
    "df_dev[\"word\"] = df_dev[\"word\"].astype(str)\n",
    "df_dev[\"tag\"] = df_dev[\"tag\"].astype(str)\n",
    "df_dev[\"lemm\"] = df_dev[\"lemm\"].astype(str)\n",
    "\n",
    "df_test[\"word\"] = df_test[\"word\"].astype(str)\n",
    "df_test[\"tag\"] = df_test[\"tag\"].astype(str)\n",
    "df_test[\"lemm\"] = df_test[\"lemm\"].astype(str)\n",
    "\n",
    "# remove head\n",
    "df_dev = df_dev.iloc[1:]\n",
    "df_test = df_test.iloc[1:]\n",
    "\n",
    "# removing rows where tag is nan\n",
    "df_dev = df_dev.dropna(subset=[\"tag\"])\n",
    "df_dev = df_dev[df_dev[\"tag\"] != \"nan\"]\n",
    "df_test = df_test.dropna(subset=[\"tag\"])\n",
    "df_test = df_test[df_test[\"tag\"] != \"nan\"]\n",
    "\n",
    "# lower case all words\n",
    "df_test[\"word\"] = df_test[\"word\"].str.lower()\n",
    "df_dev[\"word\"] = df_dev[\"word\"].str.lower()\n",
    "\n",
    "\n",
    "def get_sentences(df):\n",
    "    words = []\n",
    "    tags = []\n",
    "    lemmas = []\n",
    "    sentence = []\n",
    "    max_s = 0\n",
    "    for index, row in df.iterrows():\n",
    "        word = row[\"word\"]\n",
    "        tag = row[\"tag\"]\n",
    "        lemm = row[\"lemm\"]\n",
    "        sentence.append([word, tag, lemm])\n",
    "\n",
    "        if row[\"word\"] in [\".\", \"?\", \"!\", \";\"]:\n",
    "            words.append([word for word, tag, lemm in sentence])\n",
    "            tags.append([tag for word, tag, lemm in sentence])\n",
    "            lemmas.append([lemm for word, tag, lemm in sentence])\n",
    "            max_s = max(max_s, len(sentence))\n",
    "            sentence = []\n",
    "\n",
    "    print(\"Max sentence length: \", max_s)\n",
    "    return words, tags, lemmas\n",
    "\n",
    "# _s is for string\n",
    "dev_words_s, dev_tags_s, dev_lemmas_s = get_sentences(df_dev)\n",
    "test_words_s, test_tags_s, test_lemmas_s = get_sentences(df_test)\n",
    "print(\"Number of sentences in dev set: \", len(dev_words_s))\n",
    "print(\"Number of sentences in test set: \", len(test_words_s))\n",
    "\n",
    "for i in range(len(dev_words_s)):\n",
    "    if len(dev_words_s[i]) != len(dev_tags_s[i]) or len(dev_words_s[i]) != len(dev_lemmas_s[i]):\n",
    "        print(\"Dimension mismatch in sentence: \", i)\n",
    "        print(\"Words: \", dev_words_s[i])\n",
    "        print(\"Tags: \", dev_tags_s[i])\n",
    "        print(\"Lemmas: \", dev_lemmas_s[i])\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTX_DIM = 13\n",
    "PRE_VALUE = \"<PRE>\"\n",
    "POST_VALUE = \"<POST>\"\n",
    "\n",
    "def get_context(words, tags, lemmas):\n",
    "    ctx = []\n",
    "    w = []\n",
    "    tag = []\n",
    "    lemma = []\n",
    "\n",
    "    for s_index in range(len(words)):\n",
    "        s = words[s_index]\n",
    "        sentence = \" \".join(s)\n",
    "        s = [PRE_VALUE] * CTX_DIM + s + [POST_VALUE] * CTX_DIM\n",
    "\n",
    "        for w_index in range(len(s)):\n",
    "            if w_index < CTX_DIM or w_index >= len(s) - CTX_DIM:\n",
    "                continue\n",
    "\n",
    "            context = s[w_index - CTX_DIM:w_index] + s[w_index + 1:w_index  + CTX_DIM + 1]\n",
    "            context = \" \".join(context)\n",
    "            ctx.append(context)\n",
    "            w.append(words[s_index][w_index-CTX_DIM])\n",
    "            tag.append(tags[s_index][w_index-CTX_DIM])\n",
    "            lemma.append(lemmas[s_index][w_index-CTX_DIM])\n",
    "\n",
    "    return ctx, w, tag, lemma\n",
    "\n",
    "dev_ctx, dev_words, dev_tags, dev_lemmas = get_context(dev_words_s, dev_tags_s, dev_lemmas_s)\n",
    "test_ctx, test_words, test_tags, test_lemmas = get_context(test_words_s, test_tags_s, test_lemmas_s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTX Dim: 13 \n",
      "\n",
      "Context:  , e le sorelle che non trovavano marito neanche a regalarle , e mamma la quale filava al buio per risparmiar l' olio della lucerna ,\n",
      "Word:  la\n",
      "Tag:  art\n",
      "Lemma:  la\n",
      "\n",
      "Context:  esiste una grave frattura tra gli stati uniti e altre grandi potenze locali internazionali , egli sfider&agrave; apertamente le risoluzioni onu provocando la reazione militare statunitense\n",
      "Word:  e\n",
      "Tag:  conj_c\n",
      "Lemma:  e\n",
      "\n",
      "Context:  <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> era una bandiera che tempo aveva cessato di sventolare . <POST> <POST> <POST> <POST> <POST> <POST> <POST>\n",
      "Word:  da\n",
      "Tag:  prep\n",
      "Lemma:  da\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CTX Dim:\", CTX_DIM, \"\\n\")\n",
    "for i in range(3):\n",
    "    index = np.random.randint(0, len(dev_ctx))\n",
    "    print(\"Context: \", dev_ctx[index])\n",
    "    print(\"Word: \", dev_words[index])\n",
    "    print(\"Tag: \", dev_tags[index])\n",
    "    print(\"Lemma: \", dev_lemmas[index])\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max word length:  26\n",
      "Context:  <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> sullo scalandrone invece il tempo della &egrave; assai pi&ugrave; disteso e il ritmo necessariamente pi&ugrave; lento , pi&ugrave; o  ->  [1, 1, 1, 1, 1, 1, 1, 1460, 4566, 214, 8, 111, 24, 14, 585, 29, 7720, 5, 8, 1867, 3149, 29, 1692, 3, 29, 35]\n",
      "Words:  danza  ->  [37 34 47 59 34  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n",
      "Tag:  nn  ->  [1]\n",
      "Lemma:  danza  ->  [37 34 47 59 34  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n"
     ]
    }
   ],
   "source": [
    "# word encoder\n",
    "word_tokenizer = Tokenizer(filters=\"\")\n",
    "word_tokenizer.fit_on_texts(dev_ctx + test_ctx)\n",
    "\n",
    "# tag encoder\n",
    "tag_tokenizer = Tokenizer(filters=\"\")\n",
    "tag_tokenizer.fit_on_texts(dev_tags_s + test_tags_s)\n",
    "\n",
    "# lemma encoder\n",
    "lemma_tokenizer = Tokenizer(filters=\"\")\n",
    "lemma_tokenizer.fit_on_texts(dev_lemmas_s + test_lemmas_s)\n",
    "\n",
    "dev_ctx_e = word_tokenizer.texts_to_sequences(dev_ctx)\n",
    "test_ctx_e = word_tokenizer.texts_to_sequences(test_ctx)\n",
    "\n",
    "dev_tags_e = tag_tokenizer.texts_to_sequences(dev_tags)\n",
    "test_tags_e = tag_tokenizer.texts_to_sequences(test_tags)\n",
    "\n",
    "# get all unique letter in words\n",
    "characters = set()\n",
    "\n",
    "for lemma in dev_lemmas + test_lemmas:\n",
    "    for letter in lemma:\n",
    "        characters.add(letter)\n",
    "\n",
    "# add padding and unknown to characters\n",
    "characters.add(\" \")\n",
    "\n",
    "# the length of the vocab for one-hot encoded char\n",
    "VOCAB_SIZE = len(characters)\n",
    "\n",
    "# order characters\n",
    "characters = sorted(list(characters))\n",
    "\n",
    "char2idx = {char: idx for idx, char in enumerate(characters)}\n",
    "idx2char = {idx: char for idx, char in enumerate(characters)}\n",
    "\n",
    "MAX_WORD_LENGTH = 0\n",
    "for w in dev_words + test_words + dev_lemmas + test_lemmas:\n",
    "    MAX_WORD_LENGTH = max(MAX_WORD_LENGTH, len(w))\n",
    "MAX_WORD_LENGTH += 1\n",
    "print(\"Max word length: \", MAX_WORD_LENGTH)\n",
    "\n",
    "def encode_words(words):\n",
    "    encoded_words = []\n",
    "    for word in words:\n",
    "        word_e = []\n",
    "        for letter in word:\n",
    "            word_e.append(characters.index(letter))\n",
    "        encoded_words.append(word_e)\n",
    "    return encoded_words\n",
    "\n",
    "dev_words_e = encode_words(dev_words)\n",
    "test_words_e = encode_words(test_words)\n",
    "dev_lemmas_e = encode_words(dev_lemmas)\n",
    "test_lemmas_e = encode_words(test_lemmas)\n",
    "\n",
    "dev_words_e = tf.keras.preprocessing.sequence.pad_sequences(dev_words_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "test_words_e = tf.keras.preprocessing.sequence.pad_sequences(test_words_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "\n",
    "dev_lemmas_e = tf.keras.preprocessing.sequence.pad_sequences(dev_lemmas_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "test_lemmas_e = tf.keras.preprocessing.sequence.pad_sequences(test_lemmas_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "\n",
    "\n",
    "# show random data point\n",
    "index = np.random.randint(0, len(dev_ctx))\n",
    "print(\"Context: \", dev_ctx[index], \" -> \", dev_ctx_e[index])\n",
    "print(\"Words: \", dev_words[index], \" -> \", dev_words_e[index])\n",
    "print(\"Tag: \", dev_tags[index], \" -> \", dev_tags_e[index])\n",
    "print(\"Lemma: \", dev_lemmas[index], \" -> \", dev_lemmas_e[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the characters for the lemmas\n",
    "dev_lemmas_e = tf.one_hot(dev_lemmas_e, VOCAB_SIZE)\n",
    "test_lemmas_e = tf.one_hot(test_lemmas_e, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context shape:  (17301, 26)\n",
      "Words shape:  (17301, 26)\n",
      "Tags shape:  (17301, 1)\n",
      "Lemmas shape:  (17301, 26, 60)\n"
     ]
    }
   ],
   "source": [
    "# trnasform to numpy array\n",
    "dev_ctx_e = np.array(dev_ctx_e)\n",
    "dev_words_e = np.array(dev_words_e)\n",
    "dev_tags_e = np.array(dev_tags_e)\n",
    "dev_lemmas_e = np.array(dev_lemmas_e)\n",
    "\n",
    "test_ctx_e = np.array(test_ctx_e)\n",
    "test_words_e = np.array(test_words_e)\n",
    "test_tags_e = np.array(test_tags_e)\n",
    "test_lemmas_e = np.array(test_lemmas_e)\n",
    "\n",
    "print(\"Context shape: \", dev_ctx_e.shape)\n",
    "print(\"Words shape: \", dev_words_e.shape)\n",
    "print(\"Tags shape: \", dev_tags_e.shape)\n",
    "print(\"Lemmas shape: \", dev_lemmas_e.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 256\n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "# train word2vec model\n",
    "word2vec = gensim.models.Word2Vec(dev_words_s + test_words_s, vector_size=EMBEDDING_SIZE, window=10, min_count=1, workers=8)\n",
    "\n",
    "# create an empty embedding matix\n",
    "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "\n",
    "# create a word to index dictionary mapping\n",
    "word2id = word_tokenizer.word_index\n",
    "\n",
    "# copy vectors from word2vec model to the words present in corpus\n",
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec.wv[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " context_input (InputLayer)     [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " tags_input (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " words_input (InputLayer)       [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " context_embedding (Embedding)  (None, 26, 256)      5051392     ['context_input[0][0]']          \n",
      "                                                                                                  \n",
      " repeat_vector_5 (RepeatVector)  (None, 26, 1)       0           ['tags_input[0][0]']             \n",
      "                                                                                                  \n",
      " words_embedding (Embedding)    (None, 26, 256)      15360       ['words_input[0][0]']            \n",
      "                                                                                                  \n",
      " context_embedding_dense (Dense  (None, 26, 256)     65792       ['context_embedding[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tags_embedding_dense (Dense)   (None, 26, 256)      512         ['repeat_vector_5[0][0]']        \n",
      "                                                                                                  \n",
      " words_embedding_dense (Dense)  (None, 26, 256)      65792       ['words_embedding[0][0]']        \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 26, 256)      0           ['context_embedding_dense[0][0]',\n",
      "                                                                  'tags_embedding_dense[0][0]',   \n",
      "                                                                  'words_embedding_dense[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 26, 256)      0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " lstm (Bidirectional)           (None, 26, 512)      1050624     ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " lstm2 (Bidirectional)          (None, 26, 512)      1574912     ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense1 (Dense)                 (None, 26, 256)      131328      ['lstm2[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 26, 256)      0           ['dense1[0][0]']                 \n",
      "                                                                                                  \n",
      " dense2 (Dense)                 (None, 26, 256)      65792       ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " dense3 (Dense)                 (None, 26, 256)      65792       ['dense2[0][0]']                 \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 26, 60)       15420       ['dense3[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,102,716\n",
      "Trainable params: 3,051,324\n",
      "Non-trainable params: 5,051,392\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Neural network model with attention mechanism\n",
    "# inputs:\n",
    "#   - context: (batch_size, CTX_DIM * 2) \n",
    "#   - tags: encoded tags: (batch_size, 1)\n",
    "#   - words: encoded words: (batch_size, MAX_WORD_LENGTH)\n",
    "# outputs:\n",
    "#  - lemma: encoded lemma: (batch_size, MAX_WORD_LENGTH)\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "\n",
    "def get_model():\n",
    "    # context\n",
    "    context_input = Input(shape=(CTX_DIM * 2,), name=\"context_input\")\n",
    "    context_embedding = Embedding(len(word_tokenizer.word_index) + 1, EMBEDDING_DIM, input_length=CTX_DIM * 2, name=\"context_embedding\", trainable=False, weights=[embedding_weights])(context_input)\n",
    "    context_embedding = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"context_embedding_dense\")(context_embedding)\n",
    "\n",
    "    # tags\n",
    "    tags_input = Input(shape=(1,), name=\"tags_input\")\n",
    "    tags_embedding = RepeatVector(MAX_WORD_LENGTH)(tags_input)\n",
    "    tags_embedding = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"tags_embedding_dense\")(tags_embedding)\n",
    "\n",
    "    # words\n",
    "    words_input = Input(shape=(MAX_WORD_LENGTH,), name=\"words_input\")\n",
    "    words_embedding = Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_WORD_LENGTH, name=\"words_embedding\", trainable=True)(words_input)\n",
    "    words_embedding = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"words_embedding_dense\")(words_embedding)\n",
    "\n",
    "    # combine context, tags and words without using concatenation\n",
    "    combine = tf.keras.layers.Add()([context_embedding, tags_embedding, words_embedding])\n",
    "\n",
    "    combine = Dropout(0.5)(combine)\n",
    "    lstm = Bidirectional(LSTM(EMBEDDING_DIM, return_sequences=True), name=\"lstm\")(combine)\n",
    "    lstm = Bidirectional(LSTM(EMBEDDING_DIM, return_sequences=True), name=\"lstm2\")(lstm)\n",
    "\n",
    "    # dense layers\n",
    "    dense1 = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"dense1\")(lstm)\n",
    "    dense1 = Dropout(0.5)(dense1)\n",
    "    dense2 = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"dense2\")(dense1)\n",
    "    dense3 = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"dense3\")(dense2)\n",
    "\n",
    "    # output\n",
    "    output = Dense(VOCAB_SIZE, activation=\"softmax\", name=\"output\")(dense3)\n",
    "\n",
    "    model = Model(inputs=[context_input, tags_input, words_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.argmax(y_true, axis=-1)\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    correct_predictions = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "129/129 [==============================] - 29s 181ms/step - loss: 0.7033 - accuracy: 0.0255 - val_loss: 0.3933 - val_accuracy: 0.0645\n",
      "Epoch 2/100\n",
      "129/129 [==============================] - 16s 127ms/step - loss: 0.2690 - accuracy: 0.2589 - val_loss: 0.1732 - val_accuracy: 0.4884\n",
      "Epoch 3/100\n",
      "129/129 [==============================] - 16s 126ms/step - loss: 0.1516 - accuracy: 0.4919 - val_loss: 0.1237 - val_accuracy: 0.5999\n",
      "Epoch 4/100\n",
      "129/129 [==============================] - 16s 121ms/step - loss: 0.1107 - accuracy: 0.5810 - val_loss: 0.0809 - val_accuracy: 0.7451\n",
      "Epoch 5/100\n",
      "129/129 [==============================] - 16s 121ms/step - loss: 0.0832 - accuracy: 0.6627 - val_loss: 0.0663 - val_accuracy: 0.7420\n",
      "Epoch 6/100\n",
      "129/129 [==============================] - 16s 121ms/step - loss: 0.0686 - accuracy: 0.7078 - val_loss: 0.0576 - val_accuracy: 0.7676\n",
      "Epoch 7/100\n",
      "129/129 [==============================] - 16s 121ms/step - loss: 0.0591 - accuracy: 0.7349 - val_loss: 0.0422 - val_accuracy: 0.8198\n",
      "Epoch 8/100\n",
      "129/129 [==============================] - 16s 123ms/step - loss: 0.0502 - accuracy: 0.7752 - val_loss: 0.0358 - val_accuracy: 0.8325\n",
      "Epoch 9/100\n",
      "129/129 [==============================] - 16s 122ms/step - loss: 0.0487 - accuracy: 0.7682 - val_loss: 0.0365 - val_accuracy: 0.8289\n",
      "Epoch 10/100\n",
      "129/129 [==============================] - 16s 122ms/step - loss: 0.0360 - accuracy: 0.8308 - val_loss: 0.0288 - val_accuracy: 0.8689\n",
      "Epoch 11/100\n",
      "129/129 [==============================] - 16s 121ms/step - loss: 0.0318 - accuracy: 0.8421 - val_loss: 0.0243 - val_accuracy: 0.8785\n",
      "Epoch 12/100\n",
      "129/129 [==============================] - 16s 123ms/step - loss: 0.0277 - accuracy: 0.8594 - val_loss: 0.0215 - val_accuracy: 0.9049\n",
      "Epoch 13/100\n",
      "129/129 [==============================] - 15s 120ms/step - loss: 0.0248 - accuracy: 0.8668 - val_loss: 0.0193 - val_accuracy: 0.9071\n",
      "Epoch 14/100\n",
      "129/129 [==============================] - 16s 123ms/step - loss: 0.0257 - accuracy: 0.8560 - val_loss: 0.0206 - val_accuracy: 0.8990\n",
      "Epoch 15/100\n",
      " 75/129 [================>.............] - ETA: 6s - loss: 0.0210 - accuracy: 0.8804"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[accuracy])\n",
    "\n",
    "# train model\n",
    "history = model.fit([dev_ctx_e, dev_tags_e, dev_words_e], dev_lemmas_e, epochs=100, batch_size=128, validation_split=0.05)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4177/4177 [==============================] - 217s 52ms/step - loss: 0.0193 - accuracy: 0.9699\n",
      "Test loss:  0.019336679950356483\n",
      "Test accuracy:  0.969872236251831\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "result = model.evaluate([test_ctx_e, test_tags_e, test_words_e], test_lemmas_e)\n",
    "print(\"Test loss: \", result[0])\n",
    "print(\"Test accuracy: \", result[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4177/4177 [==============================] - 180s 43ms/step\n",
      "Accuracy:  0.9698715406887574\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([test_ctx_e, test_tags_e, test_words_e])\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for i in range(len(test_lemmas)):\n",
    "    total += 1\n",
    "\n",
    "    y_pred = np.argmax(pred[i], axis=-1)\n",
    "    y_true = np.argmax(test_lemmas_e[i], axis=-1)\n",
    "\n",
    "    if np.array_equal(y_pred, y_true):\n",
    "        correct += 1\n",
    "\n",
    "print(\"Accuracy: \", correct / total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
