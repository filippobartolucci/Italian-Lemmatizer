{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian Word Lemmatizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Bidirectional, TimeDistributed, RepeatVector, Activation, Dot, Lambda, Dropout, Add, Multiply, Masking\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import gensim\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# set all random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  95\n",
      "Max sentence length:  107\n",
      "Number of sentences in dev set:  703\n",
      "Number of sentences in test set:  5596\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_path = \"./dev.csv\"\n",
    "df_dev = pd.read_csv(dataset_path, sep=\"\\t\", header=None,\n",
    "                     names=[\"word\", \"tag\", \"lemm\"])\n",
    "\n",
    "dataset_path = \"./test.csv\"\n",
    "df_test = pd.read_csv(dataset_path, sep=\"\\t\", header=None,\n",
    "                      names=[\"word\", \"tag\", \"lemm\"])\n",
    "\n",
    "df_dev[\"word\"] = df_dev[\"word\"].astype(str)\n",
    "df_dev[\"tag\"] = df_dev[\"tag\"].astype(str)\n",
    "df_dev[\"lemm\"] = df_dev[\"lemm\"].astype(str)\n",
    "\n",
    "df_test[\"word\"] = df_test[\"word\"].astype(str)\n",
    "df_test[\"tag\"] = df_test[\"tag\"].astype(str)\n",
    "df_test[\"lemm\"] = df_test[\"lemm\"].astype(str)\n",
    "\n",
    "# remove head\n",
    "df_dev = df_dev.iloc[1:]\n",
    "df_test = df_test.iloc[1:]\n",
    "\n",
    "# removing rows where tag is nan\n",
    "df_dev = df_dev.dropna(subset=[\"tag\"])\n",
    "df_dev = df_dev[df_dev[\"tag\"] != \"nan\"]\n",
    "df_test = df_test.dropna(subset=[\"tag\"])\n",
    "df_test = df_test[df_test[\"tag\"] != \"nan\"]\n",
    "\n",
    "# lower case all words\n",
    "df_test[\"word\"] = df_test[\"word\"].str.lower()\n",
    "df_dev[\"word\"] = df_dev[\"word\"].str.lower()\n",
    "\n",
    "\n",
    "def get_sentences(df):\n",
    "    words = []\n",
    "    tags = []\n",
    "    lemmas = []\n",
    "    sentence = []\n",
    "    max_s = 0\n",
    "    for index, row in df.iterrows():\n",
    "        word = row[\"word\"]\n",
    "        tag = row[\"tag\"]\n",
    "        lemm = row[\"lemm\"]\n",
    "        sentence.append([word, tag, lemm])\n",
    "\n",
    "        if row[\"word\"] in [\".\", \"?\", \"!\", \";\"]:\n",
    "            words.append([word for word, tag, lemm in sentence])\n",
    "            tags.append([tag for word, tag, lemm in sentence])\n",
    "            lemmas.append([lemm for word, tag, lemm in sentence])\n",
    "            max_s = max(max_s, len(sentence))\n",
    "            sentence = []\n",
    "\n",
    "    print(\"Max sentence length: \", max_s)\n",
    "    return words, tags, lemmas\n",
    "\n",
    "# _s is for string\n",
    "dev_words_s, dev_tags_s, dev_lemmas_s = get_sentences(df_dev)\n",
    "test_words_s, test_tags_s, test_lemmas_s = get_sentences(df_test)\n",
    "print(\"Number of sentences in dev set: \", len(dev_words_s))\n",
    "print(\"Number of sentences in test set: \", len(test_words_s))\n",
    "\n",
    "for i in range(len(dev_words_s)):\n",
    "    if len(dev_words_s[i]) != len(dev_tags_s[i]) or len(dev_words_s[i]) != len(dev_lemmas_s[i]):\n",
    "        print(\"Dimension mismatch in sentence: \", i)\n",
    "        print(\"Words: \", dev_words_s[i])\n",
    "        print(\"Tags: \", dev_tags_s[i])\n",
    "        print(\"Lemmas: \", dev_lemmas_s[i])\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTX_DIM = 13\n",
    "PRE_VALUE = \"<PRE>\"\n",
    "POST_VALUE = \"<POST>\"\n",
    "\n",
    "def get_context(words, tags, lemmas):\n",
    "    ctx = []\n",
    "    w = []\n",
    "    tag = []\n",
    "    lemma = []\n",
    "\n",
    "    for s_index in range(len(words)):\n",
    "        s = words[s_index]\n",
    "        sentence = \" \".join(s)\n",
    "        s = [PRE_VALUE] * CTX_DIM + s + [POST_VALUE] * CTX_DIM\n",
    "\n",
    "        for w_index in range(len(s)):\n",
    "            if w_index < CTX_DIM or w_index >= len(s) - CTX_DIM:\n",
    "                continue\n",
    "\n",
    "            context = s[w_index - CTX_DIM:w_index] + s[w_index + 1:w_index  + CTX_DIM + 1]\n",
    "            context = \" \".join(context)\n",
    "            ctx.append(context)\n",
    "            w.append(words[s_index][w_index-CTX_DIM])\n",
    "            tag.append(tags[s_index][w_index-CTX_DIM])\n",
    "            lemma.append(lemmas[s_index][w_index-CTX_DIM])\n",
    "\n",
    "    return ctx, w, tag, lemma\n",
    "\n",
    "dev_ctx, dev_words, dev_tags, dev_lemmas = get_context(dev_words_s, dev_tags_s, dev_lemmas_s)\n",
    "test_ctx, test_words, test_tags, test_lemmas = get_context(test_words_s, test_tags_s, test_lemmas_s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTX Dim: 13 \n",
      "\n",
      "Context:  , e le sorelle che non trovavano marito neanche a regalarle , e mamma la quale filava al buio per risparmiar l' olio della lucerna ,\n",
      "Word:  la\n",
      "Tag:  art\n",
      "Lemma:  la\n",
      "\n",
      "Context:  esiste una grave frattura tra gli stati uniti e altre grandi potenze locali internazionali , egli sfider&agrave; apertamente le risoluzioni onu provocando la reazione militare statunitense\n",
      "Word:  e\n",
      "Tag:  conj_c\n",
      "Lemma:  e\n",
      "\n",
      "Context:  <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> era una bandiera che tempo aveva cessato di sventolare . <POST> <POST> <POST> <POST> <POST> <POST> <POST>\n",
      "Word:  da\n",
      "Tag:  prep\n",
      "Lemma:  da\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CTX Dim:\", CTX_DIM, \"\\n\")\n",
    "for i in range(3):\n",
    "    index = np.random.randint(0, len(dev_ctx))\n",
    "    print(\"Context: \", dev_ctx[index])\n",
    "    print(\"Word: \", dev_words[index])\n",
    "    print(\"Tag: \", dev_tags[index])\n",
    "    print(\"Lemma: \", dev_lemmas[index])\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Class Words\n",
    "The evaluation is done only on open-class words and not to functional words: only the tokens having a PoS-tag comprised in the set ADJ *, ADV, NN, V * had to be lemmatised, in all the other cases the token could be copied unchanged into the lemma column as they were not considered for the evaluation (the asterisk indicates all PoS-tag possibilities beginning with that prefix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_ctx:  62597\n",
      "test_words:  62597\n",
      "test_tags:  62597\n",
      "test_lemmas:  62597\n"
     ]
    }
   ],
   "source": [
    "def get_open_class_words(ctx, words, tags, lemmas):\n",
    "    open_class_words = []\n",
    "    open_class_ctx = []\n",
    "    open_class_tags = []\n",
    "    open_class_lemmas = []\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if \"adj_\" in tags[i] or \"adv\" in tags[i] or \"nn\" in tags[i] or \"v_\" in tags[i]:\n",
    "            open_class_words.append(words[i])\n",
    "            open_class_ctx.append(ctx[i])\n",
    "            open_class_tags.append(tags[i])\n",
    "            open_class_lemmas.append(lemmas[i])\n",
    "\n",
    "    return open_class_ctx, open_class_words, open_class_tags, open_class_lemmas\n",
    "\n",
    "\n",
    "test_ctx, test_words, test_tags, test_lemmas = get_open_class_words(test_ctx, test_words, test_tags, test_lemmas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  60\n",
      "Max word length:  26\n",
      "Context:  leader delle confederazioni sindacali prima_di rientrare a varsavia , ieri ha completato i con i governanti italiani vedendo il vice presidente del consiglio claudio martelli .  ->  [609, 40, 7058, 4131, 338, 6304, 10, 4566, 3, 121, 26, 4567, 18, 23, 18, 6522, 292, 2304, 8, 3858, 157, 15, 635, 3590, 9561, 6]\n",
      "Words:  colloqui  ->  [36 48 45 45 48 50 54 42  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n",
      "Tag:  nn  ->  [1]\n",
      "Lemma:  colloquio  ->  [36 48 45 45 48 50 54 42 48  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n"
     ]
    }
   ],
   "source": [
    "# word encoder\n",
    "word_tokenizer = Tokenizer(filters=\"\")\n",
    "word_tokenizer.fit_on_texts(dev_ctx + test_ctx)\n",
    "\n",
    "# tag encoder\n",
    "tag_tokenizer = Tokenizer(filters=\"\")\n",
    "tag_tokenizer.fit_on_texts(dev_tags_s + test_tags_s)\n",
    "\n",
    "# lemma encoder\n",
    "lemma_tokenizer = Tokenizer(filters=\"\")\n",
    "lemma_tokenizer.fit_on_texts(dev_lemmas_s + test_lemmas_s)\n",
    "\n",
    "dev_ctx_e = word_tokenizer.texts_to_sequences(dev_ctx)\n",
    "test_ctx_e = word_tokenizer.texts_to_sequences(test_ctx)\n",
    "\n",
    "PRE_E = word_tokenizer.texts_to_sequences([PRE_VALUE])[0][0]\n",
    "POST_E = word_tokenizer.texts_to_sequences([POST_VALUE])[0][0]\n",
    "\n",
    "\n",
    "dev_tags_e = tag_tokenizer.texts_to_sequences(dev_tags)\n",
    "test_tags_e = tag_tokenizer.texts_to_sequences(test_tags)\n",
    "\n",
    "# get all unique letter in words\n",
    "characters = set()\n",
    "\n",
    "for lemma in df_dev[\"lemm\"].unique():\n",
    "    for letter in lemma:\n",
    "        characters.add(letter)\n",
    "\n",
    "for lemma in df_test[\"lemm\"].unique():\n",
    "    for letter in lemma:\n",
    "        characters.add(letter)\n",
    "\n",
    "# add padding and unknown to characters\n",
    "characters.add(\" \")\n",
    "\n",
    "# the length of the vocab for one-hot encoded char\n",
    "VOCAB_SIZE = len(characters)\n",
    "\n",
    "print (\"Vocab size: \", VOCAB_SIZE)\n",
    "# order characters\n",
    "characters = sorted(list(characters))\n",
    "\n",
    "\n",
    "char2idx = {char: idx for idx, char in enumerate(characters)}\n",
    "idx2char = {idx: char for idx, char in enumerate(characters)}\n",
    "\n",
    "MAX_WORD_LENGTH = 0\n",
    "for w in dev_words + test_words + dev_lemmas + test_lemmas:\n",
    "    MAX_WORD_LENGTH = max(MAX_WORD_LENGTH, len(w))\n",
    "MAX_WORD_LENGTH += 1\n",
    "print(\"Max word length: \", MAX_WORD_LENGTH)\n",
    "\n",
    "def encode_words(words):\n",
    "    encoded_words = []\n",
    "    for word in words:\n",
    "        word_e = []\n",
    "        for letter in word:\n",
    "            word_e.append(characters.index(letter))\n",
    "        encoded_words.append(word_e)\n",
    "    return encoded_words\n",
    "\n",
    "dev_words_e = encode_words(dev_words)\n",
    "test_words_e = encode_words(test_words)\n",
    "dev_lemmas_e = encode_words(dev_lemmas)\n",
    "test_lemmas_e = encode_words(test_lemmas)\n",
    "\n",
    "dev_words_e = tf.keras.preprocessing.sequence.pad_sequences(dev_words_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "test_words_e = tf.keras.preprocessing.sequence.pad_sequences(test_words_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "\n",
    "dev_lemmas_e = tf.keras.preprocessing.sequence.pad_sequences(dev_lemmas_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "test_lemmas_e = tf.keras.preprocessing.sequence.pad_sequences(test_lemmas_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "\n",
    "\n",
    "# show random data point\n",
    "index = np.random.randint(0, len(dev_ctx))\n",
    "print(\"Context: \", dev_ctx[index], \" -> \", dev_ctx_e[index])\n",
    "print(\"Words: \", dev_words[index], \" -> \", dev_words_e[index])\n",
    "print(\"Tag: \", dev_tags[index], \" -> \", dev_tags_e[index])\n",
    "print(\"Lemma: \", dev_lemmas[index], \" -> \", dev_lemmas_e[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the characters for the lemmas\n",
    "dev_lemmas_e = tf.one_hot(dev_lemmas_e, VOCAB_SIZE)\n",
    "test_lemmas_e = tf.one_hot(test_lemmas_e, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context shape:  (17301, 26)\n",
      "Words shape:  (17301, 26)\n",
      "Tags shape:  (17301, 1)\n",
      "Lemmas shape:  (17301, 26, 60)\n"
     ]
    }
   ],
   "source": [
    "# trnasform to numpy array\n",
    "dev_ctx_e = np.array(dev_ctx_e)\n",
    "dev_words_e = np.array(dev_words_e)\n",
    "dev_tags_e = np.array(dev_tags_e)\n",
    "dev_lemmas_e = np.array(dev_lemmas_e)\n",
    "\n",
    "test_ctx_e = np.array(test_ctx_e)\n",
    "test_words_e = np.array(test_words_e)\n",
    "test_tags_e = np.array(test_tags_e)\n",
    "test_lemmas_e = np.array(test_lemmas_e)\n",
    "\n",
    "print(\"Context shape: \", dev_ctx_e.shape)\n",
    "print(\"Words shape: \", dev_words_e.shape)\n",
    "print(\"Tags shape: \", dev_tags_e.shape)\n",
    "print(\"Lemmas shape: \", dev_lemmas_e.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 512\n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "# train word2vec model\n",
    "word2vec = gensim.models.Word2Vec(dev_ctx, vector_size=EMBEDDING_DIM, window=10, min_count=1, workers=8)\n",
    "\n",
    "# create an empty embedding matix\n",
    "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "# create a word to index dictionary mapping\n",
    "word2id = word_tokenizer.word_index\n",
    "\n",
    "# copy vectors from word2vec model to the words present in corpus\n",
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec.wv[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_25\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " tags_input (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " context_embedding (Embedding)  (None, 26, 384)      7577088     ['input_11[0][0]']               \n",
      "                                                                                                  \n",
      " repeat_vector_32 (RepeatVector  (None, 26, 1)       0           ['tags_input[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " words_embedding (Embedding)    (None, 26, 384)      23040       ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " time_distributed_33 (TimeDistr  (None, 26, 384)     147840      ['context_embedding[1][0]']      \n",
      " ibuted)                                                                                          \n",
      "                                                                                                  \n",
      " tags_embedding_dense2 (Dense)  (None, 26, 384)      768         ['repeat_vector_32[1][0]']       \n",
      "                                                                                                  \n",
      " words_embedding_dense (Dense)  (None, 26, 384)      147840      ['words_embedding[1][0]']        \n",
      "                                                                                                  \n",
      " add_37 (Add)                   (None, 26, 384)      0           ['time_distributed_33[1][0]',    \n",
      "                                                                  'tags_embedding_dense2[1][0]',  \n",
      "                                                                  'words_embedding_dense[1][0]']  \n",
      "                                                                                                  \n",
      " combine (Dense)                (None, 26, 384)      147840      ['add_37[1][0]']                 \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)           (None, 26, 384)      0           ['combine[1][0]']                \n",
      "                                                                                                  \n",
      " lstm (Bidirectional)           (None, 26, 768)      2362368     ['dropout_50[1][0]']             \n",
      "                                                                                                  \n",
      " lstm2 (Bidirectional)          (None, 26, 768)      3542016     ['lstm[1][0]']                   \n",
      "                                                                                                  \n",
      " lstm3 (Bidirectional)          (None, 26, 768)      3542016     ['lstm2[1][0]']                  \n",
      "                                                                                                  \n",
      " dense1 (Dense)                 (None, 26, 384)      295296      ['lstm3[1][0]']                  \n",
      "                                                                                                  \n",
      " dropout_51 (Dropout)           (None, 26, 384)      0           ['dense1[1][0]']                 \n",
      "                                                                                                  \n",
      " dense2 (Dense)                 (None, 26, 384)      147840      ['dropout_51[1][0]']             \n",
      "                                                                                                  \n",
      " dense3 (Dense)                 (None, 26, 384)      147840      ['dense2[1][0]']                 \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 26, 60)       23100       ['dense3[1][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 18,104,892\n",
      "Trainable params: 10,527,804\n",
      "Non-trainable params: 7,577,088\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Neural network model\n",
    "# inputs:\n",
    "#   - context: (batch_size, CTX_DIM * 2) \n",
    "#   - tags: encoded tags: (batch_size, 1)\n",
    "#   - words: encoded words: (batch_size, MAX_WORD_LENGTH)\n",
    "# outputs:\n",
    "#  - lemma: encoded lemma: (batch_size, MAX_WORD_LENGTH)\n",
    "\n",
    "def get_model():\n",
    "    # context\n",
    "    context_input = Input(shape=(CTX_DIM * 2,), name=\"context_input\")\n",
    "    \n",
    "    context_embedding = Embedding(len(word_tokenizer.word_index) + 1, EMBEDDING_DIM, input_length=CTX_DIM * 2, name=\"context_embedding\", trainable=False, weights=[embedding_weights])(context_input)\n",
    "    context_embedding = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"context_embedding_dense\")(context_embedding)\n",
    "\n",
    "    # tags\n",
    "    tags_input = Input(shape=(1,), name=\"tags_input\")\n",
    "    tags_embedding = RepeatVector(MAX_WORD_LENGTH)(tags_input)\n",
    "    tags_embedding = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"tags_embedding_dense2\")(tags_embedding)\n",
    "\n",
    "    # words\n",
    "    words_input = Input(shape=(MAX_WORD_LENGTH,), name=\"words_input\")\n",
    "    words_input = Masking(mask_value=0)(words_input)\n",
    "    \n",
    "    words_embedding = Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_WORD_LENGTH, name=\"words_embedding\", trainable=True)(words_input)\n",
    "    words_embedding = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"words_embedding_dense\")(words_embedding)\n",
    "\n",
    "    # combine context, tags and words without using concatenation\n",
    "    combine = Add()([context_embedding, tags_embedding, words_embedding])\n",
    "\n",
    "    combine = Dropout(0.5)(combine)\n",
    "    lstm = Bidirectional(LSTM(EMBEDDING_DIM, return_sequences=True), name=\"lstm\")(combine)\n",
    "    lstm = Bidirectional(LSTM(EMBEDDING_DIM, return_sequences=True), name=\"lstm2\")(lstm)\n",
    "\n",
    "    # dense layers\n",
    "    dense1 = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"dense1\")(lstm)\n",
    "    dense1 = Dropout(0.5)(dense1)\n",
    "    dense2 = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"dense2\")(dense1)\n",
    "    dense3 = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"dense3\")(dense2)\n",
    "\n",
    "    # output\n",
    "    output = Dense(VOCAB_SIZE, activation=\"softmax\", name=\"output\")(dense3)\n",
    "\n",
    "    model = Model(inputs=[context_input, tags_input, words_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.argmax(y_true, axis=-1)\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    correct_predictions = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "65/65 [==============================] - 86s 1s/step - loss: 0.9136 - accuracy: 0.0048 - val_loss: 0.6344 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "65/65 [==============================] - 30s 461ms/step - loss: 0.5239 - accuracy: 0.0245 - val_loss: 0.4217 - val_accuracy: 0.0385\n",
      "Epoch 3/100\n",
      "65/65 [==============================] - 29s 449ms/step - loss: 0.3211 - accuracy: 0.1782 - val_loss: 0.2371 - val_accuracy: 0.2886\n",
      "Epoch 4/100\n",
      "65/65 [==============================] - 26s 396ms/step - loss: 0.1970 - accuracy: 0.3602 - val_loss: 0.1666 - val_accuracy: 0.4778\n",
      "Epoch 5/100\n",
      "65/65 [==============================] - 25s 380ms/step - loss: 0.1383 - accuracy: 0.5108 - val_loss: 0.1225 - val_accuracy: 0.6290\n",
      "Epoch 6/100\n",
      "65/65 [==============================] - 24s 371ms/step - loss: 0.1140 - accuracy: 0.5715 - val_loss: 0.0909 - val_accuracy: 0.6744\n",
      "Epoch 7/100\n",
      "65/65 [==============================] - 24s 366ms/step - loss: 0.0985 - accuracy: 0.5964 - val_loss: 0.0848 - val_accuracy: 0.6888\n",
      "Epoch 8/100\n",
      "65/65 [==============================] - 26s 406ms/step - loss: 0.0798 - accuracy: 0.6669 - val_loss: 0.0716 - val_accuracy: 0.7105\n",
      "Epoch 9/100\n",
      "65/65 [==============================] - 23s 355ms/step - loss: 0.0637 - accuracy: 0.7334 - val_loss: 0.0584 - val_accuracy: 0.7707\n",
      "Epoch 10/100\n",
      "65/65 [==============================] - 25s 385ms/step - loss: 0.0554 - accuracy: 0.7491 - val_loss: 0.0529 - val_accuracy: 0.7600\n",
      "Epoch 11/100\n",
      "65/65 [==============================] - 23s 359ms/step - loss: 0.0693 - accuracy: 0.6979 - val_loss: 0.0476 - val_accuracy: 0.8327\n",
      "Epoch 12/100\n",
      "65/65 [==============================] - 23s 355ms/step - loss: 0.0404 - accuracy: 0.8169 - val_loss: 0.0346 - val_accuracy: 0.8743\n",
      "Epoch 13/100\n",
      "65/65 [==============================] - 23s 349ms/step - loss: 0.0354 - accuracy: 0.8369 - val_loss: 0.0290 - val_accuracy: 0.8792\n",
      "Epoch 14/100\n",
      "65/65 [==============================] - 23s 354ms/step - loss: 0.0329 - accuracy: 0.8419 - val_loss: 0.0316 - val_accuracy: 0.8530\n",
      "Epoch 15/100\n",
      "65/65 [==============================] - 23s 357ms/step - loss: 0.0287 - accuracy: 0.8508 - val_loss: 0.0290 - val_accuracy: 0.8612\n",
      "Epoch 16/100\n",
      "65/65 [==============================] - 23s 357ms/step - loss: 0.0714 - accuracy: 0.6878 - val_loss: 0.0420 - val_accuracy: 0.8282\n",
      "Epoch 17/100\n",
      "65/65 [==============================] - 24s 362ms/step - loss: 0.0286 - accuracy: 0.8553 - val_loss: 0.0281 - val_accuracy: 0.8798\n",
      "Epoch 18/100\n",
      "65/65 [==============================] - 23s 347ms/step - loss: 0.0214 - accuracy: 0.8849 - val_loss: 0.0223 - val_accuracy: 0.9107\n",
      "Epoch 19/100\n",
      "65/65 [==============================] - 25s 377ms/step - loss: 0.0195 - accuracy: 0.8946 - val_loss: 0.0193 - val_accuracy: 0.9214\n",
      "Epoch 20/100\n",
      "65/65 [==============================] - 23s 356ms/step - loss: 0.0184 - accuracy: 0.8986 - val_loss: 0.0196 - val_accuracy: 0.9142\n",
      "Epoch 21/100\n",
      "65/65 [==============================] - 23s 350ms/step - loss: 0.0199 - accuracy: 0.8924 - val_loss: 0.0183 - val_accuracy: 0.9101\n",
      "Epoch 22/100\n",
      "65/65 [==============================] - 23s 348ms/step - loss: 0.0169 - accuracy: 0.9027 - val_loss: 0.0195 - val_accuracy: 0.9072\n",
      "Epoch 23/100\n",
      "65/65 [==============================] - 23s 360ms/step - loss: 0.0154 - accuracy: 0.9096 - val_loss: 0.0193 - val_accuracy: 0.8976\n",
      "Epoch 24/100\n",
      "65/65 [==============================] - 24s 374ms/step - loss: 0.0140 - accuracy: 0.9149 - val_loss: 0.0176 - val_accuracy: 0.9142\n",
      "Epoch 25/100\n",
      "65/65 [==============================] - 24s 363ms/step - loss: 0.0137 - accuracy: 0.9178 - val_loss: 0.0176 - val_accuracy: 0.9136\n",
      "Epoch 26/100\n",
      "65/65 [==============================] - 23s 357ms/step - loss: 0.0130 - accuracy: 0.9203 - val_loss: 0.0214 - val_accuracy: 0.9111\n",
      "Epoch 27/100\n",
      "65/65 [==============================] - 24s 368ms/step - loss: 0.0133 - accuracy: 0.9201 - val_loss: 0.0147 - val_accuracy: 0.9238\n",
      "Epoch 28/100\n",
      "65/65 [==============================] - 25s 372ms/step - loss: 0.0801 - accuracy: 0.7174 - val_loss: 0.0376 - val_accuracy: 0.8505\n",
      "Epoch 29/100\n",
      "65/65 [==============================] - 25s 382ms/step - loss: 0.0179 - accuracy: 0.9006 - val_loss: 0.0170 - val_accuracy: 0.9214\n",
      "Epoch 30/100\n",
      "65/65 [==============================] - 25s 379ms/step - loss: 0.0110 - accuracy: 0.9378 - val_loss: 0.0147 - val_accuracy: 0.9195\n",
      "Epoch 31/100\n",
      "65/65 [==============================] - 24s 363ms/step - loss: 0.0097 - accuracy: 0.9422 - val_loss: 0.0150 - val_accuracy: 0.9431\n",
      "Epoch 32/100\n",
      "65/65 [==============================] - 23s 353ms/step - loss: 0.0097 - accuracy: 0.9430 - val_loss: 0.0129 - val_accuracy: 0.9412\n",
      "Epoch 33/100\n",
      "65/65 [==============================] - 23s 354ms/step - loss: 0.0093 - accuracy: 0.9406 - val_loss: 0.0145 - val_accuracy: 0.9347\n",
      "Epoch 34/100\n",
      "65/65 [==============================] - 23s 348ms/step - loss: 0.0077 - accuracy: 0.9496 - val_loss: 0.0148 - val_accuracy: 0.9383\n",
      "Epoch 35/100\n",
      "65/65 [==============================] - 24s 362ms/step - loss: 0.0068 - accuracy: 0.9556 - val_loss: 0.0141 - val_accuracy: 0.9516\n",
      "Epoch 36/100\n",
      "65/65 [==============================] - 23s 350ms/step - loss: 0.0069 - accuracy: 0.9545 - val_loss: 0.0155 - val_accuracy: 0.9416\n",
      "Epoch 37/100\n",
      "65/65 [==============================] - 22s 342ms/step - loss: 0.0086 - accuracy: 0.9474 - val_loss: 0.0153 - val_accuracy: 0.9361\n",
      "Epoch 38/100\n",
      "65/65 [==============================] - 22s 341ms/step - loss: 0.0062 - accuracy: 0.9575 - val_loss: 0.0138 - val_accuracy: 0.9461\n",
      "Epoch 39/100\n",
      "65/65 [==============================] - 22s 338ms/step - loss: 0.0052 - accuracy: 0.9632 - val_loss: 0.0142 - val_accuracy: 0.9455\n",
      "Epoch 40/100\n",
      "65/65 [==============================] - 24s 369ms/step - loss: 0.0054 - accuracy: 0.9608 - val_loss: 0.0134 - val_accuracy: 0.9533\n",
      "Epoch 41/100\n",
      "65/65 [==============================] - 26s 392ms/step - loss: 0.0043 - accuracy: 0.9687 - val_loss: 0.0154 - val_accuracy: 0.9441\n",
      "Epoch 42/100\n",
      "65/65 [==============================] - 24s 374ms/step - loss: 0.0042 - accuracy: 0.9696 - val_loss: 0.0150 - val_accuracy: 0.9529\n",
      "Epoch 43/100\n",
      "65/65 [==============================] - 23s 353ms/step - loss: 0.0045 - accuracy: 0.9679 - val_loss: 0.0171 - val_accuracy: 0.9380\n",
      "Epoch 44/100\n",
      "65/65 [==============================] - 23s 353ms/step - loss: 0.0072 - accuracy: 0.9492 - val_loss: 0.0147 - val_accuracy: 0.9461\n",
      "Epoch 45/100\n",
      "65/65 [==============================] - 23s 348ms/step - loss: 0.0056 - accuracy: 0.9610 - val_loss: 0.0176 - val_accuracy: 0.9400\n",
      "Epoch 46/100\n",
      "65/65 [==============================] - 23s 346ms/step - loss: 0.0059 - accuracy: 0.9552 - val_loss: 0.0182 - val_accuracy: 0.9455\n",
      "Epoch 47/100\n",
      "65/65 [==============================] - 22s 345ms/step - loss: 0.0074 - accuracy: 0.9515 - val_loss: 0.0175 - val_accuracy: 0.9425\n",
      "Epoch 48/100\n",
      "65/65 [==============================] - 23s 359ms/step - loss: 0.0274 - accuracy: 0.8606 - val_loss: 0.0338 - val_accuracy: 0.8667\n",
      "Epoch 49/100\n",
      "65/65 [==============================] - 24s 370ms/step - loss: 0.0137 - accuracy: 0.9129 - val_loss: 0.0162 - val_accuracy: 0.9357\n",
      "Epoch 50/100\n",
      "65/65 [==============================] - 23s 359ms/step - loss: 0.0041 - accuracy: 0.9724 - val_loss: 0.0123 - val_accuracy: 0.9707\n",
      "Epoch 51/100\n",
      "65/65 [==============================] - 23s 361ms/step - loss: 0.0025 - accuracy: 0.9835 - val_loss: 0.0135 - val_accuracy: 0.9717\n",
      "Epoch 52/100\n",
      "65/65 [==============================] - 23s 349ms/step - loss: 0.0031 - accuracy: 0.9789 - val_loss: 0.0157 - val_accuracy: 0.9597\n",
      "Epoch 53/100\n",
      "65/65 [==============================] - 22s 336ms/step - loss: 0.0045 - accuracy: 0.9744 - val_loss: 0.0148 - val_accuracy: 0.9574\n",
      "Epoch 54/100\n",
      "65/65 [==============================] - 22s 337ms/step - loss: 0.0027 - accuracy: 0.9800 - val_loss: 0.0139 - val_accuracy: 0.9594\n",
      "Epoch 55/100\n",
      "65/65 [==============================] - 22s 335ms/step - loss: 0.0030 - accuracy: 0.9796 - val_loss: 0.0167 - val_accuracy: 0.9568\n",
      "Epoch 56/100\n",
      "65/65 [==============================] - 22s 335ms/step - loss: 0.0100 - accuracy: 0.9468 - val_loss: 0.0239 - val_accuracy: 0.9244\n",
      "Epoch 57/100\n",
      "65/65 [==============================] - 22s 346ms/step - loss: 0.0067 - accuracy: 0.9552 - val_loss: 0.0139 - val_accuracy: 0.9474\n",
      "Epoch 58/100\n",
      "65/65 [==============================] - 22s 338ms/step - loss: 0.0037 - accuracy: 0.9740 - val_loss: 0.0177 - val_accuracy: 0.9519\n",
      "Epoch 59/100\n",
      "65/65 [==============================] - 22s 343ms/step - loss: 0.0037 - accuracy: 0.9709 - val_loss: 0.0155 - val_accuracy: 0.9506\n",
      "Epoch 60/100\n",
      "65/65 [==============================] - 23s 349ms/step - loss: 0.0024 - accuracy: 0.9829 - val_loss: 0.0169 - val_accuracy: 0.9480\n",
      "Epoch 61/100\n",
      "65/65 [==============================] - 22s 338ms/step - loss: 0.0029 - accuracy: 0.9778 - val_loss: 0.0153 - val_accuracy: 0.9525\n",
      "Epoch 62/100\n",
      "65/65 [==============================] - 22s 345ms/step - loss: 0.0034 - accuracy: 0.9776 - val_loss: 0.0154 - val_accuracy: 0.9519\n",
      "Epoch 63/100\n",
      "65/65 [==============================] - 22s 345ms/step - loss: 0.0031 - accuracy: 0.9758 - val_loss: 0.0166 - val_accuracy: 0.9600\n",
      "Epoch 64/100\n",
      "65/65 [==============================] - 22s 344ms/step - loss: 0.0031 - accuracy: 0.9789 - val_loss: 0.0154 - val_accuracy: 0.9639\n",
      "Epoch 65/100\n",
      "65/65 [==============================] - 22s 337ms/step - loss: 0.0022 - accuracy: 0.9822 - val_loss: 0.0196 - val_accuracy: 0.9461\n",
      "Epoch 66/100\n",
      "65/65 [==============================] - 22s 333ms/step - loss: 0.0051 - accuracy: 0.9650 - val_loss: 0.0176 - val_accuracy: 0.9384\n",
      "Epoch 67/100\n",
      "65/65 [==============================] - 22s 340ms/step - loss: 0.0036 - accuracy: 0.9745 - val_loss: 0.0162 - val_accuracy: 0.9473\n",
      "Epoch 68/100\n",
      "65/65 [==============================] - 22s 335ms/step - loss: 0.0037 - accuracy: 0.9738 - val_loss: 0.0175 - val_accuracy: 0.9498\n",
      "Epoch 69/100\n",
      "65/65 [==============================] - 23s 350ms/step - loss: 0.0042 - accuracy: 0.9693 - val_loss: 0.0169 - val_accuracy: 0.9574\n",
      "Epoch 70/100\n",
      "65/65 [==============================] - 23s 359ms/step - loss: 0.0030 - accuracy: 0.9767 - val_loss: 0.0173 - val_accuracy: 0.9455\n",
      "Epoch 71/100\n",
      "65/65 [==============================] - 23s 354ms/step - loss: 0.0571 - accuracy: 0.7989 - val_loss: 0.0195 - val_accuracy: 0.9328\n",
      "Epoch 72/100\n",
      "65/65 [==============================] - 23s 349ms/step - loss: 0.0044 - accuracy: 0.9724 - val_loss: 0.0133 - val_accuracy: 0.9549\n",
      "Epoch 73/100\n",
      "65/65 [==============================] - 22s 339ms/step - loss: 0.0042 - accuracy: 0.9757 - val_loss: 0.0129 - val_accuracy: 0.9510\n",
      "Epoch 74/100\n",
      "65/65 [==============================] - 22s 336ms/step - loss: 0.0027 - accuracy: 0.9854 - val_loss: 0.0123 - val_accuracy: 0.9578\n",
      "Epoch 75/100\n",
      "65/65 [==============================] - 22s 341ms/step - loss: 0.0016 - accuracy: 0.9900 - val_loss: 0.0132 - val_accuracy: 0.9642\n",
      "Epoch 76/100\n",
      "65/65 [==============================] - 23s 347ms/step - loss: 0.0014 - accuracy: 0.9909 - val_loss: 0.0122 - val_accuracy: 0.9648\n",
      "Epoch 77/100\n",
      "65/65 [==============================] - 23s 347ms/step - loss: 0.0017 - accuracy: 0.9892 - val_loss: 0.0141 - val_accuracy: 0.9617\n",
      "Epoch 78/100\n",
      "65/65 [==============================] - 22s 346ms/step - loss: 0.0014 - accuracy: 0.9907 - val_loss: 0.0130 - val_accuracy: 0.9676\n",
      "Epoch 79/100\n",
      "65/65 [==============================] - 22s 340ms/step - loss: 0.0010 - accuracy: 0.9924 - val_loss: 0.0136 - val_accuracy: 0.9588\n",
      "Epoch 80/100\n",
      "65/65 [==============================] - 22s 334ms/step - loss: 0.0013 - accuracy: 0.9907 - val_loss: 0.0151 - val_accuracy: 0.9633\n",
      "Epoch 81/100\n",
      "65/65 [==============================] - 22s 344ms/step - loss: 0.0020 - accuracy: 0.9858 - val_loss: 0.0163 - val_accuracy: 0.9527\n",
      "Epoch 82/100\n",
      "65/65 [==============================] - 24s 362ms/step - loss: 0.0011 - accuracy: 0.9915 - val_loss: 0.0148 - val_accuracy: 0.9594\n",
      "Epoch 83/100\n",
      "65/65 [==============================] - 21s 328ms/step - loss: 0.0022 - accuracy: 0.9880 - val_loss: 0.0163 - val_accuracy: 0.9484\n",
      "Epoch 84/100\n",
      "65/65 [==============================] - 21s 328ms/step - loss: 0.0017 - accuracy: 0.9876 - val_loss: 0.0142 - val_accuracy: 0.9627\n",
      "Epoch 85/100\n",
      "65/65 [==============================] - 21s 326ms/step - loss: 0.0022 - accuracy: 0.9837 - val_loss: 0.0174 - val_accuracy: 0.9578\n",
      "Epoch 86/100\n",
      "65/65 [==============================] - 21s 328ms/step - loss: 0.0034 - accuracy: 0.9791 - val_loss: 0.0151 - val_accuracy: 0.9545\n",
      "Epoch 87/100\n",
      "65/65 [==============================] - 21s 328ms/step - loss: 0.0030 - accuracy: 0.9799 - val_loss: 0.0146 - val_accuracy: 0.9591\n",
      "Epoch 88/100\n",
      "65/65 [==============================] - 24s 364ms/step - loss: 0.0023 - accuracy: 0.9846 - val_loss: 0.0179 - val_accuracy: 0.9546\n",
      "Epoch 89/100\n",
      "65/65 [==============================] - 23s 352ms/step - loss: 0.0019 - accuracy: 0.9855 - val_loss: 0.0171 - val_accuracy: 0.9568\n",
      "Epoch 90/100\n",
      "65/65 [==============================] - 23s 357ms/step - loss: 0.0025 - accuracy: 0.9834 - val_loss: 0.0175 - val_accuracy: 0.9568\n",
      "Epoch 91/100\n",
      "65/65 [==============================] - 24s 369ms/step - loss: 0.0021 - accuracy: 0.9865 - val_loss: 0.0217 - val_accuracy: 0.9322\n",
      "Epoch 92/100\n",
      "65/65 [==============================] - 23s 350ms/step - loss: 0.0033 - accuracy: 0.9755 - val_loss: 0.0155 - val_accuracy: 0.9623\n",
      "Epoch 93/100\n",
      "65/65 [==============================] - 22s 333ms/step - loss: 0.0022 - accuracy: 0.9832 - val_loss: 0.0172 - val_accuracy: 0.9594\n",
      "Epoch 94/100\n",
      "65/65 [==============================] - 22s 333ms/step - loss: 0.0032 - accuracy: 0.9804 - val_loss: 0.0165 - val_accuracy: 0.9494\n",
      "Epoch 95/100\n",
      "65/65 [==============================] - 23s 352ms/step - loss: 0.0032 - accuracy: 0.9772 - val_loss: 0.0174 - val_accuracy: 0.9439\n",
      "Epoch 96/100\n",
      "65/65 [==============================] - 24s 366ms/step - loss: 0.0022 - accuracy: 0.9849 - val_loss: 0.0152 - val_accuracy: 0.9584\n",
      "Epoch 97/100\n",
      "65/65 [==============================] - 24s 371ms/step - loss: 0.0022 - accuracy: 0.9841 - val_loss: 0.0166 - val_accuracy: 0.9525\n",
      "Epoch 98/100\n",
      "65/65 [==============================] - 23s 360ms/step - loss: 0.0021 - accuracy: 0.9865 - val_loss: 0.0190 - val_accuracy: 0.9500\n",
      "Epoch 99/100\n",
      "65/65 [==============================] - 24s 367ms/step - loss: 0.0039 - accuracy: 0.9724 - val_loss: 0.0174 - val_accuracy: 0.9431\n",
      "Epoch 100/100\n",
      "65/65 [==============================] - 24s 364ms/step - loss: 0.0038 - accuracy: 0.9722 - val_loss: 0.0171 - val_accuracy: 0.9416\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[accuracy])\n",
    "\n",
    "# train model\n",
    "history = model.fit([dev_ctx_e, dev_tags_e, dev_words_e], dev_lemmas_e, epochs=100, batch_size=256, validation_split=0.05)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1957/1957 [==============================] - 202s 103ms/step - loss: 0.0351 - accuracy: 0.9117\n",
      "Test loss:  0.03508284315466881\n",
      "Test accuracy:  0.911679208278656\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "result = model.evaluate([test_ctx_e, test_tags_e, test_words_e], test_lemmas_e)\n",
    "print(\"Test loss: \", result[0])\n",
    "print(\"Test accuracy: \", result[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
