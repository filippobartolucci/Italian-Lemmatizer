{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian Word Lemmatizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Bidirectional, TimeDistributed, RepeatVector, Activation, Dot, Lambda, Dropout, Add, Multiply, Masking, Attention\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import gensim\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# set all random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  320\n",
      "Max sentence length:  107\n",
      "Number of sentences in dev set:  2730\n",
      "Number of sentences in test set:  5596\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_path = \"./dev_new.csv\"\n",
    "df_dev = pd.read_csv(dataset_path, sep=\"\\t\", header=None,\n",
    "                     names=[\"word\", \"tag\", \"lemm\"])\n",
    "\n",
    "dataset_path = \"./test_new.csv\"\n",
    "df_test = pd.read_csv(dataset_path, sep=\"\\t\", header=None,\n",
    "                      names=[\"word\", \"tag\", \"lemm\"])\n",
    "\n",
    "df_dev[\"word\"] = df_dev[\"word\"].astype(str)\n",
    "df_dev[\"tag\"] = df_dev[\"tag\"].astype(str)\n",
    "df_dev[\"lemm\"] = df_dev[\"lemm\"].astype(str)\n",
    "\n",
    "df_test[\"word\"] = df_test[\"word\"].astype(str)\n",
    "df_test[\"tag\"] = df_test[\"tag\"].astype(str)\n",
    "df_test[\"lemm\"] = df_test[\"lemm\"].astype(str)\n",
    "\n",
    "# remove head\n",
    "df_dev = df_dev.iloc[1:]\n",
    "df_test = df_test.iloc[1:]\n",
    "\n",
    "# removing rows where tag is nan\n",
    "df_dev = df_dev.dropna(subset=[\"tag\"])\n",
    "df_dev = df_dev[df_dev[\"tag\"] != \"nan\"]\n",
    "df_test = df_test.dropna(subset=[\"tag\"])\n",
    "df_test = df_test[df_test[\"tag\"] != \"nan\"]\n",
    "\n",
    "# lower case all words\n",
    "df_test[\"word\"] = df_test[\"word\"].str.lower()\n",
    "df_dev[\"word\"] = df_dev[\"word\"].str.lower()\n",
    "\n",
    "\n",
    "def get_sentences(df):\n",
    "    words = []\n",
    "    tags = []\n",
    "    lemmas = []\n",
    "    sentence = []\n",
    "    max_s = 0\n",
    "    for index, row in df.iterrows():\n",
    "        word = row[\"word\"]\n",
    "        tag = row[\"tag\"]\n",
    "        lemm = row[\"lemm\"]\n",
    "        sentence.append([word, tag, lemm])\n",
    "\n",
    "        if row[\"word\"] in [\".\", \"?\", \"!\", \";\"]:\n",
    "            words.append([word for word, tag, lemm in sentence])\n",
    "            tags.append([tag for word, tag, lemm in sentence])\n",
    "            lemmas.append([lemm for word, tag, lemm in sentence])\n",
    "            max_s = max(max_s, len(sentence))\n",
    "            sentence = []\n",
    "\n",
    "    print(\"Max sentence length: \", max_s)\n",
    "    return words, tags, lemmas\n",
    "\n",
    "# _s is for string\n",
    "dev_words_s, dev_tags_s, dev_lemmas_s = get_sentences(df_dev)\n",
    "test_words_s, test_tags_s, test_lemmas_s = get_sentences(df_test)\n",
    "print(\"Number of sentences in dev set: \", len(dev_words_s))\n",
    "print(\"Number of sentences in test set: \", len(test_words_s))\n",
    "\n",
    "for i in range(len(dev_words_s)):\n",
    "    if len(dev_words_s[i]) != len(dev_tags_s[i]) or len(dev_words_s[i]) != len(dev_lemmas_s[i]):\n",
    "        print(\"Dimension mismatch in sentence: \", i)\n",
    "        print(\"Words: \", dev_words_s[i])\n",
    "        print(\"Tags: \", dev_tags_s[i])\n",
    "        print(\"Lemmas: \", dev_lemmas_s[i])\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTX_DIM = 11\n",
    "PRE_VALUE = \"<PRE>\"\n",
    "POST_VALUE = \"<POST>\"\n",
    "\n",
    "def get_context(words, tags, lemmas):\n",
    "    ctx = []\n",
    "    w = []\n",
    "    tag = []\n",
    "    lemma = []\n",
    "\n",
    "    for s_index in range(len(words)):\n",
    "        s = words[s_index]\n",
    "        sentence = \" \".join(s)\n",
    "        s = [PRE_VALUE] * CTX_DIM + s + [POST_VALUE] * CTX_DIM\n",
    "\n",
    "        for w_index in range(len(s)):\n",
    "            if w_index < CTX_DIM or w_index >= len(s) - CTX_DIM:\n",
    "                continue\n",
    "\n",
    "            context = s[w_index - CTX_DIM:w_index] + s[w_index + 1:w_index  + CTX_DIM + 1]\n",
    "            context = \" \".join(context)\n",
    "            ctx.append(context)\n",
    "            w.append(words[s_index][w_index-CTX_DIM])\n",
    "            tag.append(tags[s_index][w_index-CTX_DIM])\n",
    "            lemma.append(lemmas[s_index][w_index-CTX_DIM])\n",
    "\n",
    "    return ctx, w, tag, lemma\n",
    "\n",
    "dev_ctx, dev_words, dev_tags, dev_lemmas = get_context(dev_words_s, dev_tags_s, dev_lemmas_s)\n",
    "test_ctx, test_words, test_tags, test_lemmas = get_context(test_words_s, test_tags_s, test_lemmas_s)\n",
    "\n",
    "dev_ctx, val_ctx, dev_words, val_words, dev_tags, val_tags, dev_lemmas, val_lemmas = train_test_split(dev_ctx, dev_words, dev_tags, dev_lemmas, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTX Dim: 11 \n",
      "\n",
      "Context:  visti come l' ancora di salvezza dell' di l' economia mondiale il motore di una crescita destinata ad affermarsi affermar si mentre\n",
      "Word:  ,\n",
      "Tag:  punct\n",
      "Lemma:  ,\n",
      "\n",
      "Context:  le linee direttrici non possono essere lasciate alla a la personale di un qualsiasi funzionario della di la commissione o dell' di\n",
      "Word:  interpretazione\n",
      "Tag:  noun\n",
      "Lemma:  interpretazione\n",
      "\n",
      "Context:  <PRE> teneva quel foglio con tutte e due le mani sul della banchina abbagliato dalla pozza di luce di una fotocellula ,\n",
      "Word:  ciglio\n",
      "Tag:  noun\n",
      "Lemma:  ciglio\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CTX Dim:\", CTX_DIM, \"\\n\")\n",
    "for i in range(3):\n",
    "    index = np.random.randint(0, len(dev_ctx))\n",
    "    print(\"Context: \", dev_ctx[index])\n",
    "    print(\"Word: \", dev_words[index])\n",
    "    print(\"Tag: \", dev_tags[index])\n",
    "    print(\"Lemma: \", dev_lemmas[index])\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Class Words\n",
    "The evaluation is done only on open-class words and not to functional words: only the tokens having a PoS-tag comprised in the set ADJ *, ADV, NN, V * had to be lemmatised, in all the other cases the token could be copied unchanged into the lemma column as they were not considered for the evaluation (the asterisk indicates all PoS-tag possibilities beginning with that prefix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_open_class_words(ctx, words, tags, lemmas):\n",
    "    open_class_words = []\n",
    "    open_class_ctx = []\n",
    "    open_class_tags = []\n",
    "    open_class_lemmas = []\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if \"adj_\" in tags[i] or \"adv\" in tags[i] or \"nn\" in tags[i] or \"v_\" in tags[i]:\n",
    "            open_class_words.append(words[i])\n",
    "            open_class_ctx.append(ctx[i])\n",
    "            open_class_tags.append(tags[i])\n",
    "            open_class_lemmas.append(lemmas[i])\n",
    "\n",
    "    return open_class_ctx, open_class_words, open_class_tags, open_class_lemmas\n",
    "\n",
    "\n",
    "test_ctx, test_words, test_tags, test_lemmas = get_open_class_words(test_ctx, test_words, test_tags, test_lemmas)\n",
    "dev_ctx, dev_words, dev_tags, dev_lemmas = get_open_class_words(dev_ctx, dev_words, dev_tags, dev_lemmas)\n",
    "val_ctx, val_words, val_tags, val_lemmas = get_open_class_words(val_ctx, val_words, val_tags, val_lemmas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  76\n",
      "Max word length:  22\n",
      "Context:  <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> <PRE> il terzo punto è già stato toccato . <POST> <POST> <POST>  ->  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 509, 143, 47, 755, 54, 852, 6, 2, 2, 2]\n",
      "Words:  anche  ->  [34 47 36 41 38  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Tag:  adv  ->  [7]\n",
      "Lemma:  anche  ->  [34 47 36 41 38  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# word encoder\n",
    "word_tokenizer = Tokenizer(filters=\"\")\n",
    "word_tokenizer.fit_on_texts(dev_ctx + test_ctx + val_ctx)\n",
    "\n",
    "# tag encoder\n",
    "tag_tokenizer = Tokenizer(filters=\"\")\n",
    "tag_tokenizer.fit_on_texts(dev_tags_s + test_tags_s)\n",
    "\n",
    "# lemma encoder\n",
    "lemma_tokenizer = Tokenizer(filters=\"\")\n",
    "lemma_tokenizer.fit_on_texts(dev_lemmas_s + test_lemmas_s)\n",
    "\n",
    "dev_ctx_e = word_tokenizer.texts_to_sequences(dev_ctx)\n",
    "val_ctx_e = word_tokenizer.texts_to_sequences(val_ctx)\n",
    "test_ctx_e = word_tokenizer.texts_to_sequences(test_ctx)\n",
    "\n",
    "PRE_E = word_tokenizer.texts_to_sequences([PRE_VALUE])[0][0]\n",
    "POST_E = word_tokenizer.texts_to_sequences([POST_VALUE])[0][0]\n",
    "\n",
    "\n",
    "dev_tags_e = tag_tokenizer.texts_to_sequences(dev_tags)\n",
    "val_tags_e = tag_tokenizer.texts_to_sequences(val_tags)\n",
    "test_tags_e = tag_tokenizer.texts_to_sequences(test_tags)\n",
    "\n",
    "\n",
    "# get all unique letter in words\n",
    "characters = set()\n",
    "\n",
    "for lemma in df_dev[\"lemm\"].unique():\n",
    "    for letter in lemma:\n",
    "        characters.add(letter)\n",
    "\n",
    "for lemma in df_test[\"lemm\"].unique():\n",
    "    for letter in lemma:\n",
    "        characters.add(letter)\n",
    "\n",
    "# add padding and unknown to characters\n",
    "characters.add(\" \")\n",
    "\n",
    "# the length of the vocab for one-hot encoded char\n",
    "VOCAB_SIZE = len(characters)\n",
    "\n",
    "print (\"Vocab size: \", VOCAB_SIZE)\n",
    "# order characters\n",
    "characters = sorted(list(characters))\n",
    "\n",
    "\n",
    "char2idx = {char: idx for idx, char in enumerate(characters)}\n",
    "idx2char = {idx: char for idx, char in enumerate(characters)}\n",
    "\n",
    "MAX_WORD_LENGTH = 0\n",
    "for w in dev_words + test_words + dev_lemmas + test_lemmas:\n",
    "    MAX_WORD_LENGTH = max(MAX_WORD_LENGTH, len(w))\n",
    "#MAX_WORD_LENGTH += 1\n",
    "print(\"Max word length: \", MAX_WORD_LENGTH)\n",
    "\n",
    "def encode_words(words):\n",
    "    encoded_words = []\n",
    "    for word in words:\n",
    "        word_e = []\n",
    "        for letter in word:\n",
    "            word_e.append(characters.index(letter))\n",
    "        encoded_words.append(word_e)\n",
    "    return encoded_words\n",
    "\n",
    "dev_words_e = encode_words(dev_words)\n",
    "test_words_e = encode_words(test_words)\n",
    "val_words_e = encode_words(val_words)\n",
    "\n",
    "dev_lemmas_e = encode_words(dev_lemmas)\n",
    "test_lemmas_e = encode_words(test_lemmas)\n",
    "val_lemmas_e = encode_words(val_lemmas)\n",
    "\n",
    "dev_words_e = tf.keras.preprocessing.sequence.pad_sequences(dev_words_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "test_words_e = tf.keras.preprocessing.sequence.pad_sequences(test_words_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "\n",
    "dev_lemmas_e = tf.keras.preprocessing.sequence.pad_sequences(dev_lemmas_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "test_lemmas_e = tf.keras.preprocessing.sequence.pad_sequences(test_lemmas_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "\n",
    "val_words_e = tf.keras.preprocessing.sequence.pad_sequences(val_words_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "val_lemmas_e = tf.keras.preprocessing.sequence.pad_sequences(val_lemmas_e, maxlen=MAX_WORD_LENGTH, padding=\"post\")\n",
    "\n",
    "# show random data point\n",
    "index = np.random.randint(0, len(dev_ctx))\n",
    "print(\"Context: \", dev_ctx[index], \" -> \", dev_ctx_e[index])\n",
    "print(\"Words: \", dev_words[index], \" -> \", dev_words_e[index])\n",
    "print(\"Tag: \", dev_tags[index], \" -> \", dev_tags_e[index])\n",
    "print(\"Lemma: \", dev_lemmas[index], \" -> \", dev_lemmas_e[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one hot encode the characters for the lemmas\n",
    "dev_lemmas_e = tf.one_hot(dev_lemmas_e, VOCAB_SIZE)\n",
    "test_lemmas_e = tf.one_hot(test_lemmas_e, VOCAB_SIZE)\n",
    "val_lemmas_e = tf.one_hot(val_lemmas_e, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context shape:  (2245, 22)\n",
      "Words shape:  (2245, 22)\n",
      "Tags shape:  (2245, 1)\n",
      "Lemmas shape:  (2245, 22, 76)\n"
     ]
    }
   ],
   "source": [
    "# trnasform to numpy array\n",
    "dev_ctx_e = np.array(dev_ctx_e)\n",
    "dev_words_e = np.array(dev_words_e)\n",
    "dev_tags_e = np.array(dev_tags_e)\n",
    "dev_lemmas_e = np.array(dev_lemmas_e)\n",
    "\n",
    "test_ctx_e = np.array(test_ctx_e)\n",
    "test_words_e = np.array(test_words_e)\n",
    "test_tags_e = np.array(test_tags_e)\n",
    "test_lemmas_e = np.array(test_lemmas_e)\n",
    "\n",
    "val_ctx_e = np.array(val_ctx_e)\n",
    "val_words_e = np.array(val_words_e)\n",
    "val_tags_e = np.array(val_tags_e)\n",
    "val_lemmas_e = np.array(val_lemmas_e)\n",
    "\n",
    "print(\"Context shape: \", dev_ctx_e.shape)\n",
    "print(\"Words shape: \", dev_words_e.shape)\n",
    "print(\"Tags shape: \", dev_tags_e.shape)\n",
    "print(\"Lemmas shape: \", dev_lemmas_e.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.argmax(y_true, axis=-1)\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    correct_predictions = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "    return accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 512\n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "def get_word2vec_weights(DIM):\n",
    "    # train word2vec model\n",
    "    word2vec = gensim.models.Word2Vec(dev_ctx, vector_size=DIM, window=10, min_count=1, workers=8)\n",
    "\n",
    "    # create an empty embedding matix\n",
    "    embedding_weights = np.zeros((VOCABULARY_SIZE, DIM))\n",
    "\n",
    "    # create a word to index dictionary mapping\n",
    "    word2id = word_tokenizer.word_index\n",
    "\n",
    "    # copy vectors from word2vec model to the words present in corpus\n",
    "    for word, index in word2id.items():\n",
    "        try:\n",
    "            embedding_weights[index, :] = word2vec.wv[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    return embedding_weights\n",
    "\n",
    "embedding_weights = get_word2vec_weights(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " context_input (InputLayer)     [(None, 22)]         0           []                               \n",
      "                                                                                                  \n",
      " tags_input (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " context_embedding (Embedding)  (None, 22, 512)      7968768     ['context_input[0][0]']          \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVector)   (None, 22, 1)        0           ['tags_input[0][0]']             \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 22, 512)      1574912     ['context_embedding[1][0]']      \n",
      "                                                                                                  \n",
      " tags_embedding_dense2 (Dense)  (None, 22, 512)      1024        ['repeat_vector[1][0]']          \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 22)]         0           []                               \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 22, 512)      1           ['bidirectional[1][0]',          \n",
      "                                                                  'tags_embedding_dense2[1][0]']  \n",
      "                                                                                                  \n",
      " words_embedding (Embedding)    (None, 22, 512)      38912       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 22, 1024)     0           ['attention[1][0]',              \n",
      "                                                                  'words_embedding[1][0]']        \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 22, 1024)     0           ['concatenate[1][0]']            \n",
      "                                                                                                  \n",
      " lstm (Bidirectional)           (None, 22, 1024)     6295552     ['dropout[1][0]']                \n",
      "                                                                                                  \n",
      " lstm1 (Bidirectional)          (None, 22, 1024)     6295552     ['lstm[1][0]']                   \n",
      "                                                                                                  \n",
      " dense1 (Dense)                 (None, 22, 512)      524800      ['lstm1[1][0]']                  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 22, 512)      0           ['dense1[1][0]']                 \n",
      "                                                                                                  \n",
      " dense2 (Dense)                 (None, 22, 512)      262656      ['dropout_1[1][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 22, 512)      0           ['dense2[1][0]']                 \n",
      "                                                                                                  \n",
      " dense3 (Dense)                 (None, 22, 512)      262656      ['dropout_2[1][0]']              \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 22, 76)       38988       ['dense3[1][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,263,821\n",
      "Trainable params: 15,295,053\n",
      "Non-trainable params: 7,968,768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Neural network model\n",
    "# inputs:\n",
    "#   - context: (batch_size, CTX_DIM * 2) \n",
    "#   - tags: encoded tags: (batch_size, 1)\n",
    "#   - words: encoded words: (batch_size, MAX_WORD_LENGTH)\n",
    "# outputs:\n",
    "#  - lemma: encoded lemma: (batch_size, MAX_WORD_LENGTH)\n",
    "\n",
    "def get_model():\n",
    "    # context\n",
    "    context_input = Input(shape=(CTX_DIM * 2,), name=\"context_input\")\n",
    "    context_embedding = Embedding(len(word_tokenizer.word_index) + 1, EMBEDDING_DIM, input_length=CTX_DIM * 2,name=\"context_embedding\", trainable=False, weights=[embedding_weights])(context_input)\n",
    "    context_embedding = Bidirectional(LSTM(int(EMBEDDING_DIM/2), return_sequences=True, name=\"context_lstm\"))(context_embedding)\n",
    "\n",
    "    # tags\n",
    "    tags_input = Input(shape=(1,), name=\"tags_input\")\n",
    "    tags_embedding = RepeatVector(MAX_WORD_LENGTH)(tags_input)\n",
    "    tags_embedding = Dense(EMBEDDING_DIM, activation=\"swish\",name=\"tags_embedding_dense2\")(tags_embedding)\n",
    "\n",
    "    # words\n",
    "    words_input = Input(shape=(MAX_WORD_LENGTH,), name=\"words_input\")\n",
    "    words_input = Masking(mask_value=0)(words_input)\n",
    "\n",
    "    words_embedding = Embedding(VOCAB_SIZE, int(EMBEDDING_DIM), input_length=MAX_WORD_LENGTH, name=\"words_embedding\", trainable=True)(words_input)\n",
    "\n",
    "    attention = tf.keras.layers.Attention(use_scale=True)([context_embedding, tags_embedding])\n",
    "\n",
    "    # combine\n",
    "    combine = Concatenate()([attention, words_embedding])\n",
    "\n",
    "    combine = Dropout(0.5)(combine)\n",
    "    lstm = Bidirectional(LSTM(EMBEDDING_DIM, return_sequences=True), name=\"lstm\")(combine)\n",
    "    lstm = Bidirectional(LSTM(EMBEDDING_DIM, return_sequences=True), name=\"lstm1\")(lstm)\n",
    "\n",
    "    # dense layers\n",
    "    dense1 = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"dense1\")(lstm)\n",
    "    dense1 = Dropout(0.5)(dense1)\n",
    "    dense2 = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"dense2\")(dense1)\n",
    "    dense2 = Dropout(0.5)(dense2)\n",
    "    dense3 = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"dense3\")(dense2)\n",
    "    dense4 = Dense(EMBEDDING_DIM, activation=\"swish\", name=\"dense4\")(dense3)\n",
    "\n",
    "    # output\n",
    "    output = Dense(VOCAB_SIZE, activation=\"softmax\", name=\"output\")(dense3)\n",
    "\n",
    "    model = Model(inputs=[context_input, tags_input,\n",
    "                  words_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 13s 922ms/step - loss: 2.0320 - accuracy: 5.6402e-04 - val_loss: 1.0780 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 6s 656ms/step - loss: 1.1358 - accuracy: 0.0020 - val_loss: 0.9875 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 6s 665ms/step - loss: 0.9377 - accuracy: 0.0148 - val_loss: 0.8007 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 6s 659ms/step - loss: 0.8127 - accuracy: 0.0024 - val_loss: 0.7389 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 6s 640ms/step - loss: 0.7556 - accuracy: 0.0171 - val_loss: 0.6991 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 6s 661ms/step - loss: 0.7085 - accuracy: 0.0338 - val_loss: 0.6599 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 6s 678ms/step - loss: 0.6496 - accuracy: 0.0204 - val_loss: 0.5757 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 6s 653ms/step - loss: 0.5391 - accuracy: 0.0453 - val_loss: 0.4289 - val_accuracy: 0.1743\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 6s 661ms/step - loss: 0.4511 - accuracy: 0.1229 - val_loss: 0.3504 - val_accuracy: 0.1860\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 6s 714ms/step - loss: 0.3629 - accuracy: 0.1835 - val_loss: 0.3128 - val_accuracy: 0.2699\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 6s 696ms/step - loss: 0.3003 - accuracy: 0.2148 - val_loss: 0.2387 - val_accuracy: 0.2519\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 6s 704ms/step - loss: 0.2414 - accuracy: 0.2879 - val_loss: 0.1855 - val_accuracy: 0.3724\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 6s 692ms/step - loss: 0.1985 - accuracy: 0.3403 - val_loss: 0.1529 - val_accuracy: 0.5000\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 6s 687ms/step - loss: 0.1693 - accuracy: 0.3836 - val_loss: 0.1328 - val_accuracy: 0.5184\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 6s 723ms/step - loss: 0.1478 - accuracy: 0.4393 - val_loss: 0.1067 - val_accuracy: 0.6191\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 6s 715ms/step - loss: 0.1199 - accuracy: 0.5177 - val_loss: 0.0969 - val_accuracy: 0.6321\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 6s 722ms/step - loss: 0.1033 - accuracy: 0.5664 - val_loss: 0.0769 - val_accuracy: 0.7493\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 6s 708ms/step - loss: 0.0841 - accuracy: 0.6369 - val_loss: 0.0649 - val_accuracy: 0.7517\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 7s 733ms/step - loss: 0.0610 - accuracy: 0.7149 - val_loss: 0.0422 - val_accuracy: 0.8859\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 7s 809ms/step - loss: 0.0407 - accuracy: 0.8225 - val_loss: 0.0338 - val_accuracy: 0.9133\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 7s 777ms/step - loss: 0.0321 - accuracy: 0.8677 - val_loss: 0.0285 - val_accuracy: 0.9514\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 6s 706ms/step - loss: 0.0244 - accuracy: 0.9101 - val_loss: 0.0259 - val_accuracy: 0.9592\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 6s 715ms/step - loss: 0.0205 - accuracy: 0.9290 - val_loss: 0.0261 - val_accuracy: 0.9579\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 6s 697ms/step - loss: 0.0171 - accuracy: 0.9451 - val_loss: 0.0261 - val_accuracy: 0.9738\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 7s 746ms/step - loss: 0.0148 - accuracy: 0.9563 - val_loss: 0.0240 - val_accuracy: 0.9697\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 7s 720ms/step - loss: 0.0124 - accuracy: 0.9560 - val_loss: 0.0228 - val_accuracy: 0.9738\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 6s 696ms/step - loss: 0.0111 - accuracy: 0.9691 - val_loss: 0.0220 - val_accuracy: 0.9697\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 6s 708ms/step - loss: 0.0105 - accuracy: 0.9662 - val_loss: 0.0219 - val_accuracy: 0.9736\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 6s 718ms/step - loss: 0.0090 - accuracy: 0.9736 - val_loss: 0.0223 - val_accuracy: 0.9738\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 6s 704ms/step - loss: 0.0089 - accuracy: 0.9713 - val_loss: 0.0228 - val_accuracy: 0.9738\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 6s 699ms/step - loss: 0.0091 - accuracy: 0.9677 - val_loss: 0.0228 - val_accuracy: 0.9803\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 6s 700ms/step - loss: 0.0072 - accuracy: 0.9792 - val_loss: 0.0214 - val_accuracy: 0.9723\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 6s 705ms/step - loss: 0.0067 - accuracy: 0.9782 - val_loss: 0.0232 - val_accuracy: 0.9803\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 6s 694ms/step - loss: 0.0057 - accuracy: 0.9849 - val_loss: 0.0240 - val_accuracy: 0.9763\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 6s 692ms/step - loss: 0.0054 - accuracy: 0.9834 - val_loss: 0.0234 - val_accuracy: 0.9763\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 6s 696ms/step - loss: 0.0046 - accuracy: 0.9859 - val_loss: 0.0223 - val_accuracy: 0.9816\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 6s 708ms/step - loss: 0.0039 - accuracy: 0.9895 - val_loss: 0.0226 - val_accuracy: 0.9816\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 6s 697ms/step - loss: 0.0040 - accuracy: 0.9822 - val_loss: 0.0250 - val_accuracy: 0.9922\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 6s 699ms/step - loss: 0.0036 - accuracy: 0.9882 - val_loss: 0.0248 - val_accuracy: 0.9855\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 6s 699ms/step - loss: 0.0031 - accuracy: 0.9875 - val_loss: 0.0248 - val_accuracy: 0.9749\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 6s 695ms/step - loss: 0.0030 - accuracy: 0.9869 - val_loss: 0.0243 - val_accuracy: 0.9855\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 6s 710ms/step - loss: 0.0025 - accuracy: 0.9895 - val_loss: 0.0254 - val_accuracy: 0.9829\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 6s 703ms/step - loss: 0.0023 - accuracy: 0.9876 - val_loss: 0.0258 - val_accuracy: 0.9855\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 6s 705ms/step - loss: 0.0024 - accuracy: 0.9888 - val_loss: 0.0257 - val_accuracy: 0.9855\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 6s 707ms/step - loss: 0.0017 - accuracy: 0.9908 - val_loss: 0.0269 - val_accuracy: 0.9855\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 6s 701ms/step - loss: 0.0015 - accuracy: 0.9925 - val_loss: 0.0260 - val_accuracy: 0.9935\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 6s 711ms/step - loss: 0.0016 - accuracy: 0.9925 - val_loss: 0.0270 - val_accuracy: 0.9935\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 6s 704ms/step - loss: 0.0011 - accuracy: 0.9944 - val_loss: 0.0274 - val_accuracy: 0.9789\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 6s 709ms/step - loss: 0.0013 - accuracy: 0.9944 - val_loss: 0.0275 - val_accuracy: 0.9868\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 6s 698ms/step - loss: 9.2407e-04 - accuracy: 0.9952 - val_loss: 0.0266 - val_accuracy: 0.9868\n",
      "Epoch 51/100\n",
      "9/9 [==============================] - 7s 732ms/step - loss: 0.0011 - accuracy: 0.9950 - val_loss: 0.0279 - val_accuracy: 0.9935\n",
      "Epoch 52/100\n",
      "9/9 [==============================] - 6s 719ms/step - loss: 9.2383e-04 - accuracy: 0.9957 - val_loss: 0.0291 - val_accuracy: 0.9922\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[accuracy])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True)\n",
    "\n",
    "# train model\n",
    "history = model.fit([dev_ctx_e, dev_tags_e, dev_words_e], dev_lemmas_e, epochs=100, batch_size=256, validation_data=([val_ctx_e, val_tags_e, val_words_e], val_lemmas_e), callbacks=[early_stopping])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - 16s 72ms/step - loss: 0.0137 - accuracy: 0.9761\n",
      "Test loss:  0.013708939775824547\n",
      "Test accuracy:  0.9761284589767456\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "result = model.evaluate([test_ctx_e, test_tags_e, test_words_e], test_lemmas_e)\n",
    "print(\"Test loss: \", result[0])\n",
    "print(\"Test accuracy: \", result[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4976e0179d97dd6d59b1329a76e601e17b789c2571b41c8b57f5fd69821c0dd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
