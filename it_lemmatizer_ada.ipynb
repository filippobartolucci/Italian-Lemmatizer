{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlXK_WPhCXve"
      },
      "source": [
        "# Italian Word Lemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfFQpeq7CXvj"
      },
      "source": [
        "### Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1m3hfyemCXvj"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time \n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEeIQWgECXvk"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcBN-hJ3CXvl",
        "outputId": "125649a0-be13-4111-9b48-a982c1f92d4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max sentence length:  95\n",
            "Max sentence length:  107\n",
            "Number of sentences in dev set:  703\n",
            "Number of sentences in test set:  5596\n",
            "Number of unique tags:  32\n"
          ]
        }
      ],
      "source": [
        "dataset_path = \"./dev.csv\"\n",
        "df_dev = pd.read_csv(dataset_path, sep=\"\\t\", header=None,\n",
        "                     names=[\"word\", \"tag\", \"lemm\"])\n",
        "\n",
        "dataset_path = \"./test.csv\"\n",
        "df_test = pd.read_csv(dataset_path, sep=\"\\t\", header=None,\n",
        "                      names=[\"word\", \"tag\", \"lemm\"])\n",
        "\n",
        "df_dev[\"word\"] = df_dev[\"word\"].astype(str)\n",
        "df_dev[\"tag\"] = df_dev[\"tag\"].astype(str)\n",
        "df_dev[\"lemm\"] = df_dev[\"lemm\"].astype(str)\n",
        "\n",
        "df_test[\"word\"] = df_test[\"word\"].astype(str)\n",
        "df_test[\"tag\"] = df_test[\"tag\"].astype(str)\n",
        "df_test[\"lemm\"] = df_test[\"lemm\"].astype(str)\n",
        "\n",
        "# remove head\n",
        "df_dev = df_dev.iloc[1:]\n",
        "df_test = df_test.iloc[1:]\n",
        "\n",
        "# lower case all words\n",
        "df_test[\"word\"] = df_test[\"word\"].str.lower()\n",
        "df_dev[\"word\"] = df_dev[\"word\"].str.lower()\n",
        "\n",
        "\n",
        "def get_sentences(df):\n",
        "    words = []\n",
        "    tags = []\n",
        "    lemmas = []\n",
        "    sentence = []\n",
        "    max_s = 0\n",
        "    for _, row in df.iterrows():\n",
        "        word = row[\"word\"]\n",
        "        tag = row[\"tag\"]\n",
        "        lemm = row[\"lemm\"]\n",
        "        sentence.append([word, tag, lemm])\n",
        "\n",
        "        if row[\"word\"] in [\".\", \"?\", \"!\", \";\"]:\n",
        "            words.append([word for word, tag, lemm in sentence])\n",
        "            tags.append([tag for word, tag, lemm in sentence])\n",
        "            lemmas.append([lemm for word, tag, lemm in sentence])\n",
        "            max_s = max(max_s, len(sentence))\n",
        "            sentence = []\n",
        "\n",
        "    print(\"Max sentence length: \", max_s)\n",
        "    return words, tags, lemmas\n",
        "\n",
        "# _s is for string\n",
        "dev_words_s, dev_tags_s, dev_lemmas_s = get_sentences(df_dev)\n",
        "test_words_s, test_tags_s, test_lemmas_s = get_sentences(df_test)\n",
        "print(\"Number of sentences in dev set: \", len(dev_words_s))\n",
        "print(\"Number of sentences in test set: \", len(test_words_s))\n",
        "\n",
        "# print number of unique tags\n",
        "print(\"Number of unique tags: \", len(df_dev[\"tag\"].unique()))\n",
        "\n",
        "for i in range(len(dev_words_s)):\n",
        "    if len(dev_words_s[i]) != len(dev_tags_s[i]) or len(dev_words_s[i]) != len(dev_lemmas_s[i]):\n",
        "        print(\"Dimension mismatch in sentence: \", i)\n",
        "        print(\"Words: \", dev_words_s[i])\n",
        "        print(\"Tags: \", dev_tags_s[i])\n",
        "        print(\"Lemmas: \", dev_lemmas_s[i])\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QieUyx1CCXvo"
      },
      "source": [
        "## Word Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9dkBA-zCXvo",
        "outputId": "fb764d08-bdbd-429e-92a0-8caf2ab1af38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sample in dev set:  17313\n",
            "Number of sample in test set:  133756\n"
          ]
        }
      ],
      "source": [
        "# Code reused from it_lemmatizer.ipynb\n",
        "\n",
        "CTX_DIM = 12            # context dimension, 12 words on each side\n",
        "PRE_VALUE = \"<PRE>\"     # value for padding pre \n",
        "POST_VALUE = \"<POST>\"   # value for padding post\n",
        "NONE_TAG = \"<NONE>\"     # value for padding tags\n",
        "\n",
        "def get_context(words, tags, lemmas):\n",
        "    ctx = []    # context list\n",
        "    w = []      # word list\n",
        "    tag = []    # context tags list\n",
        "    t = []      # word tags list\n",
        "    lemma = []  # lemma list\n",
        "\n",
        "    for s_index in range(len(words)):\n",
        "        s = words[s_index]\n",
        "        s_tags = tags[s_index]\n",
        "        \n",
        "        s = [PRE_VALUE] * CTX_DIM + s + [POST_VALUE] * CTX_DIM\n",
        "        s_tags = [NONE_TAG] * CTX_DIM + s_tags + [NONE_TAG] * CTX_DIM\n",
        "\n",
        "        for w_index in range(len(s)):\n",
        "            if w_index < CTX_DIM or w_index >= len(s) - CTX_DIM:\n",
        "                continue\n",
        "\n",
        "            context = s[w_index - CTX_DIM:w_index] + [s[w_index]] +s[w_index + 1:w_index  + CTX_DIM + 1]\n",
        "            context = \" \".join(context)\n",
        "            ctx.append(context)\n",
        "            w.append(words[s_index][w_index-CTX_DIM])\n",
        "\n",
        "            ctx_tags = s_tags[w_index - CTX_DIM:w_index] + [s_tags[w_index]] + s_tags[w_index + 1:w_index  + CTX_DIM + 1]\n",
        "            tag.append(ctx_tags)\n",
        "            t.append(tags[s_index][w_index-CTX_DIM])\n",
        "\n",
        "            lemma.append(lemmas[s_index][w_index-CTX_DIM])\n",
        "    return ctx, w, tag, t, lemma\n",
        "\n",
        "dev_ctx, dev_words, dev_tags, dev_tag,dev_lemmas = get_context(dev_words_s, dev_tags_s, dev_lemmas_s)\n",
        "test_ctx, test_words, test_tags, test_tag,test_lemmas = get_context(test_words_s, test_tags_s, test_lemmas_s)\n",
        "\n",
        "print(\"Number of sample in dev set: \", len(dev_lemmas))\n",
        "print(\"Number of sample in test set: \", len(test_lemmas))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sample in dev set:  17313\n",
            "Number of sample in test set:  133756\n"
          ]
        }
      ],
      "source": [
        "# Removing padding tokens and NONE_TAG\n",
        "# Not useful for GPT-3\n",
        "\n",
        "for i in range(len(test_ctx)):\n",
        "    # remove PRE_VALUE and POST_VALUE\n",
        "    c = test_ctx[i]\n",
        "    c = c.replace(PRE_VALUE, \"\")\n",
        "    c = c.replace(POST_VALUE, \"\")\n",
        "\n",
        "    # remove multiple spaces\n",
        "    c = \" \".join(c.split())\n",
        "    test_ctx[i] = c\n",
        "\n",
        "    # remove NONE_TAG from tags\n",
        "    t = test_tags[i]\n",
        "    t = [x for x in t if x != NONE_TAG]\n",
        "    test_tags[i] = t\n",
        "\n",
        "for i in range(len(dev_ctx)):\n",
        "    # remove PRE_VALUE and POST_VALUE\n",
        "    c = dev_ctx[i]\n",
        "    c = c.replace(PRE_VALUE, \"\")\n",
        "    c = c.replace(POST_VALUE, \"\")\n",
        "\n",
        "    # remove multiple spaces\n",
        "    c = \" \".join(c.split())\n",
        "    dev_ctx[i] = c\n",
        "\n",
        "    # remove NONE_TAG from tags\n",
        "    t = dev_tags[i]\n",
        "    t = [x for x in t if x != NONE_TAG]\n",
        "    dev_tags[i] = t\n",
        "\n",
        "print(\"Number of sample in dev set: \", len(dev_lemmas))\n",
        "print(\"Number of sample in test set: \", len(test_lemmas))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfMwZrgcCXvo"
      },
      "source": [
        "### Example of context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVoxwHO4CXvp",
        "outputId": "69cd962e-0dbd-4b79-b707-74e5ddff8761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CTX Dim: 12 \n",
            "\n",
            "CTX:  dei rinogradi era comunque l' abnorme sviluppo del nasario , una struttura che pu&ograve; essere grossolanamente assimilata al naso ma che nella realt&agrave; aveva un'\n",
            "CTX Tags:  ['prep_a', 'nn_p', 'v_essere', 'adv', 'art', 'adj', 'nn', 'prep_a', 'nn', 'p_oth', 'art', 'nn', 'pron_rel', 'v_mod', 'v_essere', 'adv', 'v_pp', 'prep_a', 'nn', 'conj_c', 'pron_rel', 'prep_a', 'nn', 'v_avere', 'art']\n",
            "Word:  che\n",
            "Tag:  pron_rel\n",
            "Lemma:  che\n",
            "\n",
            "CTX:  regolamenti locali , il vicino pu&ograve; chiedere la comunione del muro soltanto allo_scopo_di fabbricare contro il muro stesso , pagando , oltre il valore della\n",
            "CTX Tags:  ['nn', 'adj', 'p_oth', 'art', 'nn', 'v_mod', 'v_gvrb', 'art', 'nn', 'prep_a', 'nn', 'adv', 'conj_s', 'v_gvrb', 'prep', 'art', 'nn', 'adj_dim', 'p_oth', 'v_gvrb', 'p_oth', 'prep', 'art', 'nn', 'prep_a']\n",
            "Word:  allo_scopo_di\n",
            "Tag:  conj_s\n",
            "Lemma:  allo_scopo_di\n",
            "\n",
            "CTX:  \" cosa sono le banane , padre ?\n",
            "CTX Tags:  ['p_oth', 'pron_ies', 'v_essere', 'art', 'nn', 'p_oth', 'nn', 'p_eos']\n",
            "Word:  padre\n",
            "Tag:  nn\n",
            "Lemma:  padre\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"CTX Dim:\", CTX_DIM, \"\\n\")\n",
        "for i in range(3):\n",
        "    index = np.random.randint(0, len(dev_ctx))\n",
        "    print(\"CTX: \", dev_ctx[index])\n",
        "    print(\"CTX Tags: \", dev_tags[index])\n",
        "    print(\"Word: \", dev_words[index])\n",
        "    print(\"Tag: \", dev_tag[index])\n",
        "    print(\"Lemma: \", dev_lemmas[index])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDN8khxaCXvp"
      },
      "source": [
        "## Open Class Words\n",
        "The evaluation is done only on open-class words and not to functional words: only the tokens having a PoS-tag comprised in the set ADJ *, ADV, NN, V * had to be lemmatised, in all the other cases the token could be copied unchanged into the lemma column as they were not considered for the evaluation (the asterisk indicates all PoS-tag possibilities beginning with that prefix)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Su-C7kFTCXvp",
        "outputId": "278266f6-b622-470c-a019-a3afd224a3e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of open class words in test set:  65210\n"
          ]
        }
      ],
      "source": [
        "def get_open_class_words(ctx, words, tags, tag, lemmas):\n",
        "    open_class_words = []   # open class words\n",
        "    open_class_ctx = []     # open class context \n",
        "    open_class_tags = []    # open class tags\n",
        "    open_class_tag = []     # open class tag\n",
        "    open_class_lemmas = []  # open class lemmas\n",
        "\n",
        "    open_classes = [\"nn\", \"v_gvrb\", \"v_essere\", \"v_avere\", \"v_pp\", \"v_mod\", \"v_clit\", \"adv\", \"adj_ind\", \"adj_num\", \"adj\", \"adj_pos\", \"adj_dim\", \"adj_ies\"]\n",
        "\n",
        "    for i in range(len(words)):\n",
        "        t = tag[i]\n",
        "        if t in open_classes:\n",
        "            open_class_words.append(words[i])\n",
        "            open_class_ctx.append(ctx[i])\n",
        "            open_class_tags.append(tags[i])\n",
        "            open_class_tag.append(t)\n",
        "            open_class_lemmas.append(lemmas[i])\n",
        "\n",
        "    return open_class_ctx, open_class_words, open_class_tags, open_class_tag, open_class_lemmas\n",
        "\n",
        "\n",
        "test_ctx, test_words, test_tags, test_tag, test_lemmas = get_open_class_words(test_ctx, test_words, test_tags, test_tag, test_lemmas)\n",
        "\n",
        "print(\"Number of open class words in test set: \", len(test_words))  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompt creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai \n",
        "\n",
        "# Generate a prompt for GPT-3 model \n",
        "# from a sample of the dataset\n",
        "def create_prompt_data(ctx, tags, word, lemmas):\n",
        "    prompt = \"Context: \" + ctx + \"\\n\"\n",
        "    prompt += \"Tags: \" + \" \".join(tags) + \"\\n\"\n",
        "    prompt += \"Word: \" + word+ \"\\n\"\n",
        "    lemma  = \" \" + lemmas + \"\\n\"\n",
        "    return prompt, lemma\n",
        "\n",
        "train_prompt = []\n",
        "train_lemma = []\n",
        "\n",
        "test_prompt = []\n",
        "test_lemma = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples:  17313\n",
            "Number of test samples:  65210\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(dev_words)):\n",
        "    prompt, lemma = create_prompt_data(dev_ctx[i], dev_tags[i], dev_words[i], dev_lemmas[i])\n",
        "    train_prompt.append(prompt)\n",
        "    train_lemma.append(lemma)\n",
        "\n",
        "print(\"Number of training samples: \", len(train_prompt))\n",
        "\n",
        "for i in range(len(test_words)):\n",
        "    prompt, lemma = create_prompt_data(test_ctx[i], test_tags[i], test_words[i], test_lemmas[i])\n",
        "    test_prompt.append(prompt)\n",
        "    test_lemma.append(lemma)\n",
        "\n",
        "print(\"Number of test samples: \", len(test_prompt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = pd.DataFrame(list(zip(train_prompt, train_lemma)), columns =['prompt', 'completion'])\n",
        "train_df.to_json(\"dev_prompt.jsonl\", orient='records', lines=True)\n",
        "\n",
        "test_df = pd.DataFrame(list(zip(test_prompt, test_lemma)), columns =['prompt', 'completion'])\n",
        "test_df.to_json(\"test_prompt.jsonl\", orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>completion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Context: mi riferisco al lavoro dove non c' &amp;e...</td>\n",
              "      <td>mi\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Context: mi riferisco al lavoro dove non c' &amp;e...</td>\n",
              "      <td>riferire\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Context: mi riferisco al lavoro dove non c' &amp;e...</td>\n",
              "      <td>al\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Context: mi riferisco al lavoro dove non c' &amp;e...</td>\n",
              "      <td>lavoro\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Context: mi riferisco al lavoro dove non c' &amp;e...</td>\n",
              "      <td>dove\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              prompt   completion\n",
              "0  Context: mi riferisco al lavoro dove non c' &e...         mi\\n\n",
              "1  Context: mi riferisco al lavoro dove non c' &e...   riferire\\n\n",
              "2  Context: mi riferisco al lavoro dove non c' &e...         al\\n\n",
              "3  Context: mi riferisco al lavoro dove non c' &e...     lavoro\\n\n",
              "4  Context: mi riferisco al lavoro dove non c' &e...       dove\\n"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing...\n",
            "\n",
            "- Your file contains 17313 prompt-completion pairs\n",
            "- Based on your data it seems like you're trying to fine-tune a model for classification\n",
            "- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n",
            "- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n",
            "- There are 77 duplicated prompt-completion sets. These are rows: [12, 222, 580, 594, 956, 1216, 1793, 1909, 2222, 2471, 2525, 2675, 3032, 3050, 3073, 3373, 3439, 3517, 3558, 4116, 4121, 4436, 4619, 4626, 4630, 5146, 5325, 6487, 6532, 6776, 6804, 6808, 6843, 6907, 6960, 6964, 6965, 7108, 7171, 7835, 7836, 8262, 8458, 8504, 9102, 9765, 9817, 9933, 10270, 10866, 11948, 12059, 12214, 12224, 12238, 12598, 12600, 12602, 12711, 12746, 12747, 13521, 13722, 13827, 13906, 14561, 15200, 15201, 15202, 15203, 16143, 16954, 16960, 17082, 17083, 17309, 17311]\n",
            "- All prompts end with suffix `\\n`\n",
            "  WARNING: Some of your prompts contain the suffix `\n",
            "` more than once. We strongly suggest that you review your prompts and add a unique suffix\n",
            "- All prompts start with prefix `Context: `\n",
            "\n",
            "Based on the analysis we will perform the following actions:\n",
            "- [Recommended] Remove 77 duplicate rows [Y/n]: Y\n",
            "- [Recommended] Would you like to split into training and validation set? [Y/n]: Y\n",
            "\n",
            "\n",
            "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
            "\n",
            "Wrote modified files to `dev_prompt_prepared_train (1).jsonl` and `dev_prompt_prepared_valid (1).jsonl`\n",
            "Feel free to take a look!\n",
            "\n",
            "Now use that file when fine-tuning:\n",
            "> openai api fine_tunes.create -t \"dev_prompt_prepared_train (1).jsonl\" -v \"dev_prompt_prepared_valid (1).jsonl\" --compute_classification_metrics --classification_n_classes 3979\n",
            "\n",
            "After youâ€™ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\n` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\"\\n\"]` so that the generated texts ends at the expected place.\n",
            "Once your model starts training, it'll approximately take 6.93 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
          ]
        }
      ],
      "source": [
        "# Dataset preparation with openai tools\n",
        "!openai tools fine_tunes.prepare_data -f dev_prompt.jsonl -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# API key for openai\n",
        "import json\n",
        "with open('openai_api_key.json') as f:\n",
        "    data = json.load(f)\n",
        "    api_key = data['api_key']\n",
        "\n",
        "openai.api_key = api_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created fine-tune: ft-TJyXJ9Vs1vWWG7QkDDtod6Rx\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2023-06-16 22:01:56] Created fine-tune: ft-TJyXJ9Vs1vWWG7QkDDtod6Rx\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Fine tuning of ADAGPT-3 model \n",
        "!openai api fine_tunes.create -t file-WFxP8GC9Fl3jHmfYxpYt4FxB -v file-dsbAKSdqDFncrnqrSGqMR7gO -m \"ada:ft-personal-2023-06-16-15-25-19\" --n_epochs 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<FineTune fine-tune id=ft-TJyXJ9Vs1vWWG7QkDDtod6Rx at 0x2b240fc50> JSON: {\n",
              "  \"object\": \"fine-tune\",\n",
              "  \"id\": \"ft-TJyXJ9Vs1vWWG7QkDDtod6Rx\",\n",
              "  \"hyperparams\": {\n",
              "    \"n_epochs\": 6,\n",
              "    \"batch_size\": 32,\n",
              "    \"prompt_loss_weight\": 0.01,\n",
              "    \"learning_rate_multiplier\": 0.1\n",
              "  },\n",
              "  \"organization_id\": \"org-Vo2aGTMZQOXwFWCVzZKQZ9HU\",\n",
              "  \"model\": \"ada:ft-personal-2023-06-16-15-25-19\",\n",
              "  \"training_files\": [\n",
              "    {\n",
              "      \"object\": \"file\",\n",
              "      \"id\": \"file-WFxP8GC9Fl3jHmfYxpYt4FxB\",\n",
              "      \"purpose\": \"fine-tune\",\n",
              "      \"filename\": \"dev_prompt_prepared_train.jsonl\",\n",
              "      \"bytes\": 4544763,\n",
              "      \"created_at\": 1686926701,\n",
              "      \"status\": \"processed\",\n",
              "      \"status_details\": null\n",
              "    }\n",
              "  ],\n",
              "  \"validation_files\": [\n",
              "    {\n",
              "      \"object\": \"file\",\n",
              "      \"id\": \"file-dsbAKSdqDFncrnqrSGqMR7gO\",\n",
              "      \"purpose\": \"fine-tune\",\n",
              "      \"filename\": \"dev_prompt_prepared_valid.jsonl\",\n",
              "      \"bytes\": 279695,\n",
              "      \"created_at\": 1686926704,\n",
              "      \"status\": \"processed\",\n",
              "      \"status_details\": null\n",
              "    }\n",
              "  ],\n",
              "  \"result_files\": [\n",
              "    {\n",
              "      \"object\": \"file\",\n",
              "      \"id\": \"file-KMlJnhegR4poLXN9uYbb7OoC\",\n",
              "      \"purpose\": \"fine-tune-results\",\n",
              "      \"filename\": \"compiled_results.csv\",\n",
              "      \"bytes\": 172670,\n",
              "      \"created_at\": 1686947675,\n",
              "      \"status\": \"processed\",\n",
              "      \"status_details\": null\n",
              "    }\n",
              "  ],\n",
              "  \"created_at\": 1686945716,\n",
              "  \"updated_at\": 1686947675,\n",
              "  \"status\": \"succeeded\",\n",
              "  \"fine_tuned_model\": \"ada:ft-personal-2023-06-16-20-34-34\",\n",
              "  \"events\": [\n",
              "    {\n",
              "      \"object\": \"fine-tune-event\",\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Created fine-tune: ft-TJyXJ9Vs1vWWG7QkDDtod6Rx\",\n",
              "      \"created_at\": 1686945716\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine-tune-event\",\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Fine-tune costs $3.86\",\n",
              "      \"created_at\": 1686946140\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine-tune-event\",\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Fine-tune enqueued. Queue number: 0\",\n",
              "      \"created_at\": 1686946141\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine-tune-event\",\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Fine-tune started\",\n",
              "      \"created_at\": 1686946144\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine-tune-event\",\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Completed epoch 1/6\",\n",
              "      \"created_at\": 1686946412\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine-tune-event\",\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Completed epoch 2/6\",\n",
              "      \"created_at\": 1686946660\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine-tune-event\",\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Completed epoch 4/6\",\n",
              "      \"created_at\": 1686947155\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine-tune-event\",\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Completed epoch 5/6\",\n",
              "      \"created_at\": 1686947403\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine-tune-event\",\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Uploaded model: ada:ft-personal-2023-06-16-20-34-34\",\n",
              "      \"created_at\": 1686947674\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine-tune-event\",\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Uploaded result file: file-KMlJnhegR4poLXN9uYbb7OoC\",\n",
              "      \"created_at\": 1686947675\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine-tune-event\",\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Fine-tune succeeded\",\n",
              "      \"created_at\": 1686947675\n",
              "    }\n",
              "  ]\n",
              "}"
            ]
          },
          "execution_count": 323,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Status of fine tuning\n",
        "openai.FineTune.retrieve(\"ft-TJyXJ9Vs1vWWG7QkDDtod6Rx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model ID\n",
        "model_id = \"ada:ft-personal-2023-06-16-20-34-34\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation\n",
        "\n",
        "API Limits:\n",
        "* Pay-as-you-go users (first 48 hours) - 60 RPM \n",
        "* Pay-as-you-go users (after 48 hours) - 3,500 RPM \n",
        "* ada 250,000 tokens per minute\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of prompts:  65210\n"
          ]
        }
      ],
      "source": [
        "prompts = test_df['prompt'].tolist()\n",
        "print(\"Number of prompts: \", len(prompts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed batch  3261 / 3260\r"
          ]
        }
      ],
      "source": [
        "predictions = []\n",
        "batch_size = 20 # Max number of prompts per request\n",
        "total_batches = len(prompts) // batch_size\n",
        "\n",
        "for i in range(0, len(prompts), batch_size):\n",
        "    # sleep for 1 second to avoid rate limit\n",
        "    time.sleep(1)\n",
        "\n",
        "    # Get batch of prompts\n",
        "    prompt = prompts[i:i+batch_size]\n",
        "\n",
        "    # Get predictions\n",
        "    response = openai.Completion.create(\n",
        "        model=model_id,\n",
        "        prompt=prompt,\n",
        "        max_tokens=20, \n",
        "    )\n",
        "    results = response.choices\n",
        "\n",
        "    # Store predictions\n",
        "    for j in range(len(results)):\n",
        "        predictions.append(results[j].text)\n",
        "\n",
        "    print(\"Completed batch \", i//batch_size + 1, \"/\", total_batches, end=\"\\r\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "predicted_lemmas = []\n",
        "\n",
        "# A single prediction contains multiple possiple lemmas\n",
        "# for each prompt. We take the most frequent lemma as the\n",
        "# predicted lemma\n",
        "\n",
        "for p in predictions:\n",
        "    p = p.replace(\" \", \"\")\n",
        "    words = p.split(\"\\n\")\n",
        "    freq = Counter(words)\n",
        "    lemma = max(freq, key=freq.get)\n",
        "    predicted_lemmas.append(lemma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save predictions to file\n",
        "with open(\"predictions.txt\", \"w\") as f:\n",
        "    for p in predicted_lemmas:\n",
        "        f.write(p + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load predictions from file\n",
        "with open(\"predictions.txt\", \"r\") as f:\n",
        "    predicted_lemmas = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.9753565404079129\n"
          ]
        }
      ],
      "source": [
        "# Evaluate predictions on test set\n",
        "\n",
        "tot_error = 0\n",
        "error_per_tag = {}\n",
        "tag_count = {}\n",
        "correct_lemmas = 0\n",
        "\n",
        "for index, row in test_df.iterrows():\n",
        "    completion = predicted_lemmas[index].strip()\n",
        "    expected_completion = row['completion'].strip()\n",
        "\n",
        "    tag = test_tag[index].split(\"_\")[0]\n",
        "\n",
        "    if tag not in tag_count:\n",
        "            tag_count[tag] = 1\n",
        "    else:\n",
        "        tag_count[tag] += 1\n",
        "\n",
        "    if completion != expected_completion:\n",
        "        tot_error += 1\n",
        "        if tag not in error_per_tag:\n",
        "            error_per_tag[tag] = 1\n",
        "        else:\n",
        "            error_per_tag[tag] += 1\n",
        "    else:\n",
        "        correct_lemmas += 1\n",
        "\n",
        "print(\"Accuracy: \", correct_lemmas/len(test_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Absolute error with respect to the PoS tag: \n",
            "v 0.313\n",
            "adj 0.262\n",
            "nn 0.398\n",
            "adv 0.027\n",
            "\n",
            "Relative error with respect to the PoS tag: \n",
            "v 0.029\n",
            "adj 0.037\n",
            "nn 0.026\n",
            "adv 0.007\n"
          ]
        }
      ],
      "source": [
        "print(\"Absolute error with respect to the PoS tag: \")\n",
        "for tag in error_per_tag:\n",
        "    print(tag, round(error_per_tag[tag] / tot_error,3))\n",
        "\n",
        "print(\"\\nRelative error with respect to the PoS tag: \")\n",
        "for tag in error_per_tag:\n",
        "    print(tag, round(error_per_tag[tag] / tag_count[tag], 3))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
