{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "from conllu import parse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import open\n",
    "from conllu import parse_incr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_dev = open(\"it_partut-ud-dev.conllu\", \"r\", encoding=\"utf-8\")\n",
    "data_file_train = open(\"it_partut-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "data_file_test = open(\"it_partut-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "sentences = []\n",
    "words_lemmas = []\n",
    "\n",
    "for tokenlist in parse_incr(data_file_dev):\n",
    "    w_l = []\n",
    "    for token in tokenlist:\n",
    "        w_l = [token['form'], token['upos'], token['lemma']]\n",
    "        words_lemmas.append(w_l)\n",
    "        w_l = []\n",
    "    sentences.append(tokenlist.metadata['text'])\n",
    "    \n",
    "\n",
    "for tokenlist in parse_incr(data_file_train):\n",
    "    w_l = []\n",
    "    for token in tokenlist:\n",
    "        w_l = [token['form'], token['upos'], token['lemma']]\n",
    "        words_lemmas.append(w_l)\n",
    "        w_l = []\n",
    "    sentences.append(tokenlist.metadata['text'])\n",
    "\n",
    "\n",
    "    \n",
    "for tokenlist in parse_incr(data_file_test):\n",
    "    w_l = []\n",
    "    for token in tokenlist:\n",
    "        w_l = [token['form'],token['upos'],token['lemma']]\n",
    "        words_lemmas.append(w_l)\n",
    "        w_l = []\n",
    "    sentences.append(tokenlist.metadata['text'])\n",
    "    \n",
    "# all lower words_lemmas\n",
    "words_lemmas = [[x.lower() for x in y] for y in words_lemmas]\n",
    "\n",
    "df = pd.DataFrame(words_lemmas, columns = ['word', 'tag', 'lem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            word       tag       lem\n",
      "0             mi  pron_per        mi\n",
      "1      riferisco    v_gvrb  riferire\n",
      "2             al    prep_a        al\n",
      "3         lavoro        nn    lavoro\n",
      "4           dove    conj_s      dove\n",
      "...          ...       ...       ...\n",
      "17308   &egrave;  v_essere    essere\n",
      "17309         la       art        la\n",
      "17310        mia  pron_pos       mia\n",
      "17311          '     p_apo         '\n",
      "17312          .     p_eos         .\n",
      "\n",
      "[17301 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "dev = pd.read_csv('./dev.csv', sep='\\t', header=0)\n",
    "test = pd.read_csv('./test.csv', sep='\\t', header=0)\n",
    "\n",
    "dev['word'] = dev['word'].str.lower()\n",
    "dev['tag'] = dev['tag'].str.lower()\n",
    "dev['lem'] = dev['lem'].str.lower()\n",
    "\n",
    "test['word'] = test['word'].str.lower()\n",
    "test['tag'] = test['tag'].str.lower()\n",
    "test['lem'] = test['lem'].str.lower()\n",
    "\n",
    "dev = dev.dropna()\n",
    "test = test.dropna()\n",
    "\n",
    "print(dev)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all possible pos-tags\n",
    "pos_tags = []\n",
    "for i in words_lemmas:\n",
    "    pos_tags.append(i[2])\n",
    "    \n",
    "pos_tags = list(set(pos_tags))\n",
    "\n",
    "dev_pos = dev['tag'].tolist()\n",
    "test_pos = test['tag'].tolist()\n",
    "\n",
    "# create dict where the key is old pos-tag and the value is new pos-tag\n",
    "dict_pos = {}\n",
    "for i in range(len(dev_pos)):\n",
    "    if 'v_' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = 'verb'\n",
    "    elif 'p_' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = 'punct'\n",
    "    elif dev_pos[i] == 'nn':\n",
    "        dict_pos[dev_pos[i]] = 'noun'\n",
    "    elif 'adj' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = 'adj'\n",
    "    elif 'nn_p' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = 'propn'\n",
    "    elif 'prep_a' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = '_'\n",
    "    elif 'conj_s' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = 'sconj'\n",
    "    elif 'adv' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = 'adv'\n",
    "    elif 'conj_c' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = 'cconj'\n",
    "    elif 'pron_rel' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = 'pron'\n",
    "    elif  dev_pos[i] == 'prep':\n",
    "        dict_pos[dev_pos[i]] = 'adp'\n",
    "    elif 'c_num' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = 'num'\n",
    "    elif 'art' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = 'det'\n",
    "    elif 'pron_ind' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = 'pron'\n",
    "    elif 'pron_per' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = 'pron'\n",
    "    elif 'pron_dim' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = 'pron'\n",
    "    elif 'pron_ies' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = 'pron'\n",
    "    elif 'pron_pos' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = 'pron'\n",
    "    elif 'int' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = 'X'\n",
    "    elif 'p_apo' in dev_pos[i]:\n",
    "        dict_pos[dev_pos[i]] = 'punct'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing old pos-tags with new pos-tags\n",
    "for index, row in dev.iterrows():\n",
    "    row['tag'] = dict_pos[row['tag']]\n",
    "    \n",
    "for index, row in test.iterrows():\n",
    "    row['tag'] = dict_pos[row['tag']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write on csv\n",
    "test.to_csv('./test_new.csv', sep=\"\\t\",escapechar='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate\n",
    "new = pd.concat([dev, df])\n",
    "\n",
    "# delete where tag is x\n",
    "new = new[new.tag != 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on csv\n",
    "new.to_csv('./dev_new.csv', sep=\"\\t\",escapechar='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4976e0179d97dd6d59b1329a76e601e17b789c2571b41c8b57f5fd69821c0dd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
