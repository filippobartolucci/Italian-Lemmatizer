{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "from conllu import parse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import open\n",
    "from conllu import parse_incr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_dev = open(\"it_partut-ud-dev.conllu\", \"r\", encoding=\"utf-8\")\n",
    "data_file_train = open(\"it_partut-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "data_file_test = open(\"it_partut-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "sentences = []\n",
    "words_lemmas = []\n",
    "\n",
    "for tokenlist in parse_incr(data_file_dev):\n",
    "    w_l = []\n",
    "    for token in tokenlist:\n",
    "        w_l = [token['form'], token['lemma'], token['upos']]\n",
    "        words_lemmas.append(w_l)\n",
    "        w_l = []\n",
    "    sentences.append(tokenlist.metadata['text'])\n",
    "    \n",
    "\n",
    "for tokenlist in parse_incr(data_file_train):\n",
    "    w_l = []\n",
    "    for token in tokenlist:\n",
    "        w_l = [token['form'], token['lemma'], token['upos']]\n",
    "        words_lemmas.append(w_l)\n",
    "        w_l = []\n",
    "    sentences.append(tokenlist.metadata['text'])\n",
    "\n",
    "\n",
    "    \n",
    "for tokenlist in parse_incr(data_file_test):\n",
    "    w_l = []\n",
    "    for token in tokenlist:\n",
    "        w_l = [token['form'], token['lemma'], token['upos']]\n",
    "        words_lemmas.append(w_l)\n",
    "        w_l = []\n",
    "    sentences.append(tokenlist.metadata['text'])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set([w_l[0] for w_l in words_lemmas])\n",
    "lemmas = set([w_l[1] for w_l in words_lemmas])\n",
    "pos_tags = set([w_l[2] for w_l in words_lemmas])\n",
    "\n",
    "# lower all\n",
    "words = set([w.lower() for w in words])\n",
    "lemmas = set([l.lower() for l in lemmas])\n",
    "sentences = [s.lower() for s in sentences]\n",
    "pos_tags = set([pt.lower() for pt in pos_tags])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with the words as keys and the lemmas as values\n",
    "words_lemmas_dict = {}\n",
    "words_tag_dict = {}\n",
    "for w_l in words_lemmas:\n",
    "    words_lemmas_dict[w_l[0]] = w_l[1]\n",
    "\n",
    "for w_l in words_lemmas:\n",
    "    words_tag_dict[w_l[0]] = w_l[2]\n",
    "    \n",
    "\n",
    "# create dataframe from the two dictionaries\n",
    "df = pd.DataFrame(list(words_tag_dict.items()), columns=['word', 'tag'])\n",
    "df2 = pd.DataFrame(list(words_lemmas_dict.items()), columns=['word', 'lem'])\n",
    "\n",
    "# merge the two dataframes\n",
    "df = pd.merge(df, df2, on='word')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write df content to csv\n",
    "df = df.drop_duplicates()\n",
    "df = df.sort_values(by=['word'])\n",
    "df['word'] = df['word'].str.lower()\n",
    "df['lem'] = df['lem'].str.lower()\n",
    "df['tag'] = df['tag'].str.lower()\n",
    "\n",
    "df.to_csv('./words_lemmas_tags_partut.csv', sep=\"\\t\", escapechar=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dev.csv in a list\n",
    "dev = pd.read_csv('./dev.csv', sep='\\t', header=0)\n",
    "\n",
    "test = pd.read_csv('./test.csv', sep='\\t', header=0)\n",
    "\n",
    "# load words_lemmas_tags_partut.csv in a list\n",
    "new = pd.read_csv('./words_lemmas_tags_partut.csv', sep='\\t', header=0)\n",
    "\n",
    "#delete the first row\n",
    "new = new.drop([0])\n",
    "dev = dev.drop([0])\n",
    "test = test.drop([0])\n",
    "\n",
    "# transform to lower all\n",
    "new['word'] = new['word'].str.lower()\n",
    "new['tag'] = new['tag'].str.lower()\n",
    "new['lem'] = new['lem'].str.lower()\n",
    "dev['word'] = dev['word'].str.lower()\n",
    "dev['tag'] = dev['tag'].str.lower()\n",
    "dev['lem'] = dev['lem'].str.lower()\n",
    "test['word'] = test['word'].str.lower()\n",
    "test['tag'] = test['tag'].str.lower()\n",
    "test['lem'] = test['lem'].str.lower()\n",
    "\n",
    "#delete nan from dev\n",
    "dev = dev.dropna()\n",
    "test = test.dropna()\n",
    "new = new.dropna()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dev.csv and take all pos tag\n",
    "dev_pos = dev['tag'].tolist()\n",
    "test_pos = test['tag'].tolist()\n",
    "\n",
    "# remove duplicates\n",
    "dev_pos = list(dict.fromkeys(dev_pos))\n",
    "test_pos = list(dict.fromkeys(test_pos))\n",
    "\n",
    "# delete nan\n",
    "dev_pos = [x for x in dev_pos if str(x) != 'nan']\n",
    "test_pos = [x for x in test_pos if str(x) != 'nan']\n",
    "\n",
    "# create dict where the key is dev_pos and the value is pos_tags\n",
    "dict_pos = {}\n",
    "for i in range(len(dev_pos)):\n",
    "    for j in range(len(pos_tags)):\n",
    "        #if dev_pos[i] contain a substring\n",
    "        if 'v_' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = 'verb'\n",
    "        if 'p_' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = 'punct'\n",
    "        if dev_pos[i] == 'nn':\n",
    "            dict_pos[dev_pos[i]] = 'noun'\n",
    "        if 'adj' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = 'adj'\n",
    "        if 'nn_p' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = 'propn'\n",
    "        if 'prep_a' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = '_'\n",
    "        if 'conj_s' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = 'sconj'\n",
    "        if 'adv' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = 'adv'\n",
    "        if 'conj_c' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = 'cconj'\n",
    "        if 'pron_rel' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = 'pron'\n",
    "        if  dev_pos[i] == 'prep':\n",
    "            dict_pos[dev_pos[i]] = 'adp'\n",
    "        if 'c_num' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = 'num'\n",
    "        if 'art' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = 'det'\n",
    "        if 'pron_ind' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = 'pron'\n",
    "        if 'pron_per' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = 'pron'\n",
    "        if 'pron_dim' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = 'pron'\n",
    "        if 'pron_ies' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = 'pron'\n",
    "        if 'pron_pos' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = 'pron'\n",
    "        if 'int' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = 'X'\n",
    "        if 'p_apo' in dev_pos[i]:\n",
    "            dict_pos[dev_pos[i]] = 'punct'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the pos tag in dev_new with the new pos tag\n",
    "for index, row in dev.iterrows():\n",
    "    row['tag'] = dict_pos[row['tag']]\n",
    "    \n",
    "for index, row in test.iterrows():\n",
    "    row['tag'] = dict_pos[row['tag']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write on csv\n",
    "dev.to_csv('./dev_new.csv', sep=\"\\t\",escapechar='\\t', index=False)\n",
    "test.to_csv('./test_new.csv', sep=\"\\t\",escapechar='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dev_new.csv with words_lemmas_tags_partut.csv without duplicates\n",
    "dev_new = pd.read_csv('./dev_new.csv', sep='\\t', header=0)\n",
    "partut = pd.read_csv('./words_lemmas_tags_partut.csv', sep='\\t', header=0)\n",
    "\n",
    "# delete nan\n",
    "dev_new = dev_new.dropna()\n",
    "partut = partut.dropna()\n",
    "\n",
    "# delete duplicates\n",
    "dev_new = dev_new.drop_duplicates()\n",
    "partut = partut.drop_duplicates()\n",
    "\n",
    "# concatenate\n",
    "new = pd.concat([dev_new, partut])\n",
    "\n",
    "# delete where tag is x\n",
    "new = new[new.tag != 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on csv\n",
    "new.to_csv('./dev_partut.csv', sep=\"\\t\",escapechar='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4976e0179d97dd6d59b1329a76e601e17b789c2571b41c8b57f5fd69821c0dd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
