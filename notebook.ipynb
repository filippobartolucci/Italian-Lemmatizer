{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DF shape: (133756, 3)\n",
      "\n",
      "### NaN values:\n",
      "word    0\n",
      "tag     0\n",
      "lemm    0\n",
      "dtype: int64\n",
      "\n",
      "### DF shape after removing rows where tag is nan: (133756, 3)\n",
      "\n",
      "### Unique values:\n",
      "word    18506\n",
      "tag        32\n",
      "lemm    12202\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = \"./out.csv\"\n",
    "\n",
    "df = pd.read_csv(dataset_path, sep=\"\\t\", header=None, names=[\"word\", \"tag\", \"lemm\"])\n",
    "\n",
    "df[\"word\"] = df[\"word\"].astype(str) \n",
    "df[\"tag\"] = df[\"tag\"].astype(str)\n",
    "df[\"lemm\"] = df[\"lemm\"].astype(str)\n",
    "\n",
    "# remove head\n",
    "df = df.iloc[1:]\n",
    "\n",
    "# remove tag P_OTH\n",
    "df = df[df[\"tag\"] != \"P_OTH\"]\n",
    "\n",
    "print(\"### DF shape:\" ,df.shape)\n",
    "print(\"\\n### NaN values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# removing rows where tag is nan\n",
    "df = df.dropna(subset=[\"tag\"])\n",
    "print(\"\\n### DF shape after removing rows where tag is nan:\" ,df.shape)\n",
    "\n",
    "# print number of unique values for each column\n",
    "print(\"\\n### Unique values:\")\n",
    "print(df.nunique())\n",
    "\n",
    "# lower case all words\n",
    "df[\"word\"] = df[\"word\"].str.lower()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Number of unique characters: 60\n",
      "\n",
      "### Max word length: 25\n"
     ]
    }
   ],
   "source": [
    "# get all unique letter in words\n",
    "characters = set()\n",
    "for word in df[\"lemm\"]:\n",
    "    for letter in word:\n",
    "        characters.add(letter)\n",
    "\n",
    "# add padding and unknown to characters\n",
    "characters.add(\" \")\n",
    "\n",
    "\n",
    "print(\"\\n### Number of unique characters:\", len(characters))\n",
    "\n",
    "# create a dictionary that maps characters to integers\n",
    "char2int = {c: i for i, c in enumerate(sorted(characters))}\n",
    "\n",
    "# create a dictionary that maps integers to characters\n",
    "int2char = {i: c for i, c in enumerate(sorted(characters))}\n",
    "\n",
    "\n",
    "max_word_length = df[\"lemm\"].str.len().max()\n",
    "print(\"\\n### Max word length:\", max_word_length)\n",
    "\n",
    "\n",
    "# create a function that will pad a word\n",
    "def pad_word(word, max_word_length):\n",
    "    return word + \" \" * (max_word_length - len(word) + 1 )\n",
    "\n",
    "df[\"word\"] = df[\"word\"].apply(lambda x: pad_word(x, max_word_length))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Character: a, \n",
      "Index: 34, \n",
      "Vector: [[-0.00634288 -0.0281008   0.02099048  0.00537093 -0.04169939  0.04311992\n",
      "   0.03632576  0.03021384]]\n",
      "\n",
      "Character: b, \n",
      "Index: 35, \n",
      "Vector: [[ 0.00803609 -0.00380163 -0.03261193 -0.02743235  0.02289846 -0.02173448\n",
      "   0.02918745 -0.04527965]]\n",
      "\n",
      "Character: c, \n",
      "Index: 36, \n",
      "Vector: [[-0.04879359  0.04591043 -0.03960212 -0.04415265 -0.0101123  -0.00556413\n",
      "  -0.02741838  0.01529712]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([TensorShape([26, 8])], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# Create an embedding layer\n",
    "vocab_size = len(characters)\n",
    "embedding_dim = 8\n",
    "embedding_layer = Embedding(vocab_size, embedding_dim, input_length=1)\n",
    "\n",
    "# Convert characters to dense vectors using the embedding layer\n",
    "word = \"abc\"\n",
    "\n",
    "dense_vectors = []\n",
    "for char in word:\n",
    "    char_index = char2int[char]\n",
    "    dense_vector = embedding_layer(np.array([char_index]))\n",
    "    dense_vectors.append(dense_vector)\n",
    "    print(\"Character: {}, Index: {}, Vector: {}\".format(char, char_index, dense_vector))\n",
    "\n",
    "# Concatenate dense vectors into a single tensor\n",
    "dense_tensor = np.concatenate(dense_vectors, axis=0)\n",
    "\n",
    "# apply the embedding layer to the all the words in the dataset\n",
    "df[\"word_e\"] = df[\"word\"].apply(lambda x: embedding_layer(np.array([char2int[char] for char in x])))\n",
    "\n",
    "df.head()\n",
    "\n",
    "# check if the size of word_e is always the same\n",
    "df[\"word_e\"].apply(lambda x: x.shape).unique()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cf3a2657c4530458a1d5b90a9ba637718c74089d900d5938397f33b4197fc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
